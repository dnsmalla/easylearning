{
  "category": "Computer Vision",
  "categoryId": "vision",
  "version": "1.0.0",
  "description": "Image processing, object detection, and visual understanding",
  "icon": "eye",
  "color": "#F59E0B",
  "topics": [
    {
      "id": "image_classification",
      "title": "Image Classification",
      "symbol": "ðŸ–¼ï¸",
      "level": "beginner",
      "definition": {
        "text": "Image classification assigns a label to an entire image from a predefined set of categories. It's the fundamental task in computer vision. CNNs revolutionized this field, achieving human-level performance on ImageNet. Modern approaches use transfer learning with pretrained models like ResNet, EfficientNet, or Vision Transformers.",
        "keyTerms": ["Classification", "ImageNet", "Top-1 Accuracy", "Top-5 Accuracy", "Softmax", "Pretrained Model", "Fine-tuning"]
      },
      "keyFormulas": [
        {
          "id": "softmax_class",
          "name": "Softmax Probability",
          "formula": "P(class_i) = exp(záµ¢) / Î£exp(zâ±¼)",
          "latex": "P(\\text{class}_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}",
          "meaning": "Convert logits to probabilities"
        }
      ],
      "examples": [
        {
          "id": "imgcls_ex1",
          "question": "Why use Top-5 accuracy in ImageNet?",
          "steps": [
            {"step": 1, "action": "ImageNet scale", "result": "1000 fine-grained classes", "explanation": "Many similar categories"},
            {"step": 2, "action": "Ambiguity", "result": "Multiple valid labels possible", "explanation": "Husky vs Malamute"},
            {"step": 3, "action": "Top-5 metric", "result": "Correct if true label in top 5 predictions", "explanation": "More forgiving"}
          ],
          "finalAnswer": "Top-5 accounts for ambiguity in fine-grained categories",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Medical Diagnosis", "description": "Detecting diseases from X-rays, MRIs"},
        {"title": "Content Moderation", "description": "Flagging inappropriate images"},
        {"title": "Species Identification", "description": "Plant and animal recognition apps"}
      ],
      "codeExample": {
        "python": "import torch\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom PIL import Image\n\n# Load pretrained ResNet-50\nweights = ResNet50_Weights.DEFAULT\nmodel = resnet50(weights=weights)\nmodel.eval()\n\n# Preprocessing\npreprocess = weights.transforms()\n\n# Load and classify image\nimg = Image.open('cat.jpg')\nimg_tensor = preprocess(img).unsqueeze(0)\n\nwith torch.no_grad():\n    output = model(img_tensor)\n    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n\n# Get top 5 predictions\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\nfor i in range(5):\n    category = weights.meta['categories'][top5_catid[i]]\n    print(f\"{category}: {top5_prob[i].item():.2%}\")"
      },
      "tips": [
        "Always normalize images with ImageNet mean/std for pretrained models",
        "Data augmentation (flip, crop, color jitter) improves generalization",
        "EfficientNet gives best accuracy/compute tradeoff"
      ]
    },
    {
      "id": "object_detection",
      "title": "Object Detection",
      "symbol": "ðŸŽ¯",
      "level": "intermediate",
      "definition": {
        "text": "Object detection locates and classifies multiple objects in an image. Unlike classification, it outputs bounding boxes (x, y, width, height) and class labels for each detected object. Two-stage detectors (R-CNN family) are accurate but slow; one-stage detectors (YOLO, SSD) are fast and suitable for real-time applications.",
        "keyTerms": ["Bounding Box", "IoU", "NMS", "Anchor Box", "Feature Pyramid", "mAP", "One-stage vs Two-stage"]
      },
      "keyFormulas": [
        {
          "id": "iou_formula",
          "name": "Intersection over Union",
          "formula": "IoU = Area(A âˆ© B) / Area(A âˆª B)",
          "latex": "\\text{IoU} = \\frac{|A \\cap B|}{|A \\cup B|}",
          "meaning": "Measures bounding box overlap"
        }
      ],
      "examples": [
        {
          "id": "det_ex1",
          "question": "What is Non-Maximum Suppression (NMS)?",
          "steps": [
            {"step": 1, "action": "Problem", "result": "Multiple overlapping detections", "explanation": "Same object detected multiple times"},
            {"step": 2, "action": "NMS process", "result": "Keep highest confidence, remove overlapping", "explanation": "IoU > threshold removed"},
            {"step": 3, "action": "Result", "result": "One box per object", "explanation": "Clean detections"}
          ],
          "finalAnswer": "NMS removes duplicate detections by keeping only the highest confidence box",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Autonomous Vehicles", "description": "Detecting pedestrians, cars, traffic signs"},
        {"title": "Retail Analytics", "description": "Counting products on shelves"},
        {"title": "Security", "description": "Person detection and tracking"}
      ],
      "codeExample": {
        "python": "from ultralytics import YOLO\nimport cv2\n\n# Load YOLOv8 model\nmodel = YOLO('yolov8n.pt')  # nano model for speed\n\n# Run inference\nresults = model('image.jpg')\n\n# Process results\nfor result in results:\n    boxes = result.boxes\n    for box in boxes:\n        # Bounding box coordinates\n        x1, y1, x2, y2 = box.xyxy[0].tolist()\n        \n        # Confidence and class\n        confidence = box.conf[0].item()\n        class_id = int(box.cls[0].item())\n        class_name = model.names[class_id]\n        \n        print(f\"{class_name}: {confidence:.2%} at [{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]\")\n\n# Save annotated image\nresults[0].save('output.jpg')"
      },
      "tips": [
        "YOLOv8 is current SOTA for real-time detection",
        "IoU threshold 0.5 is standard for mAP calculation",
        "Use smaller models (nano, small) for edge deployment"
      ]
    },
    {
      "id": "semantic_segmentation",
      "title": "Semantic Segmentation",
      "symbol": "ðŸ§©",
      "level": "advanced",
      "definition": {
        "text": "Semantic segmentation classifies every pixel in an image into predefined categories. Unlike object detection, it doesn't distinguish between instances of the same class. Key architectures include FCN (Fully Convolutional Networks), U-Net (encoder-decoder with skip connections), and DeepLab (atrous convolution + CRF).",
        "keyTerms": ["Pixel-wise Classification", "FCN", "U-Net", "Encoder-Decoder", "Skip Connection", "Atrous Convolution", "mIoU"]
      },
      "keyFormulas": [
        {
          "id": "miou",
          "name": "Mean IoU",
          "formula": "mIoU = (1/K) Î£ IoU_k",
          "latex": "\\text{mIoU} = \\frac{1}{K} \\sum_{k=1}^{K} \\frac{TP_k}{TP_k + FP_k + FN_k}",
          "meaning": "Average IoU across all classes"
        }
      ],
      "examples": [
        {
          "id": "seg_ex1",
          "question": "Why use skip connections in U-Net?",
          "steps": [
            {"step": 1, "action": "Encoder", "result": "Downsamples, loses spatial detail", "explanation": "Captures 'what'"},
            {"step": 2, "action": "Decoder", "result": "Upsamples, needs spatial info", "explanation": "Needs 'where'"},
            {"step": 3, "action": "Skip connections", "result": "Pass spatial info from encoder to decoder", "explanation": "Precise boundaries"}
          ],
          "finalAnswer": "Skip connections preserve spatial details lost during downsampling",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Medical Imaging", "description": "Organ and tumor segmentation"},
        {"title": "Autonomous Driving", "description": "Road, lane, obstacle segmentation"},
        {"title": "Satellite Imagery", "description": "Land use classification"}
      ],
      "codeExample": {
        "python": "import torch\nimport torch.nn as nn\n\nclass UNetBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.conv(x)\n\nclass SimpleUNet(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        # Encoder\n        self.enc1 = UNetBlock(3, 64)\n        self.enc2 = UNetBlock(64, 128)\n        self.pool = nn.MaxPool2d(2)\n        \n        # Decoder\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear')\n        self.dec1 = UNetBlock(128 + 64, 64)  # Skip connection\n        self.final = nn.Conv2d(64, n_classes, 1)\n    \n    def forward(self, x):\n        # Encoder\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool(e1))\n        \n        # Decoder with skip connection\n        d1 = self.up(e2)\n        d1 = torch.cat([d1, e1], dim=1)  # Skip!\n        d1 = self.dec1(d1)\n        \n        return self.final(d1)"
      },
      "tips": [
        "U-Net is excellent for biomedical images with limited training data",
        "Instance segmentation (Mask R-CNN) distinguishes individual objects",
        "SAM (Segment Anything Model) can segment any object from a prompt"
      ]
    },
    {
      "id": "yolo",
      "title": "YOLO",
      "symbol": "âš¡",
      "level": "advanced",
      "definition": {
        "text": "YOLO (You Only Look Once) is a real-time object detection system that frames detection as a single regression problem. Unlike two-stage detectors, YOLO processes the entire image once, predicting bounding boxes and class probabilities directly. YOLOv8 is the latest iteration, offering state-of-the-art speed and accuracy.",
        "keyTerms": ["One-stage Detector", "Grid Cell", "Anchor-free", "CSPNet", "PANet", "Real-time Detection"]
      },
      "keyFormulas": [
        {
          "id": "yolo_loss",
          "name": "YOLO Loss Components",
          "formula": "L = Î»_coord L_box + Î»_obj L_obj + Î»_cls L_cls",
          "latex": "\\mathcal{L} = \\lambda_{coord}\\mathcal{L}_{box} + \\lambda_{obj}\\mathcal{L}_{obj} + \\lambda_{cls}\\mathcal{L}_{cls}",
          "meaning": "Box + objectness + classification losses"
        }
      ],
      "examples": [
        {
          "id": "yolo_ex1",
          "question": "Why is YOLO faster than R-CNN?",
          "steps": [
            {"step": 1, "action": "R-CNN", "result": "Region proposal â†’ classify each", "explanation": "Two-stage, ~2000 crops"},
            {"step": 2, "action": "YOLO", "result": "Single forward pass for all detections", "explanation": "One-stage"},
            {"step": 3, "action": "Speed", "result": "YOLO: 30+ FPS, R-CNN: <1 FPS", "explanation": "Real-time capable"}
          ],
          "finalAnswer": "YOLO uses a single network pass instead of region-by-region classification",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Drone Detection", "description": "Real-time obstacle avoidance"},
        {"title": "Sports Analytics", "description": "Player and ball tracking"},
        {"title": "Manufacturing QC", "description": "Defect detection on assembly lines"}
      ],
      "codeExample": {
        "python": "from ultralytics import YOLO\n\n# Model sizes: n (nano), s (small), m (medium), l (large), x (xlarge)\nmodel = YOLO('yolov8m.pt')\n\n# Train on custom dataset\nmodel.train(\n    data='my_dataset.yaml',  # Dataset config\n    epochs=100,\n    imgsz=640,\n    batch=16,\n    device='cuda'\n)\n\n# Validate\nmetrics = model.val()\nprint(f\"mAP50: {metrics.box.map50:.3f}\")\nprint(f\"mAP50-95: {metrics.box.map:.3f}\")\n\n# Export for deployment\nmodel.export(format='onnx')  # or 'tflite', 'coreml'\n\n# Real-time video detection\nresults = model.predict(\n    source=0,  # Webcam\n    show=True,\n    conf=0.5\n)"
      },
      "tips": [
        "YOLOv8n runs at 100+ FPS on modern GPUs",
        "Use imgsz=640 for balance of speed and accuracy",
        "Export to ONNX or CoreML for edge deployment"
      ]
    },
    {
      "id": "transfer_learning_cv",
      "title": "Transfer Learning (CV)",
      "symbol": "ðŸ”€",
      "level": "intermediate",
      "definition": {
        "text": "Transfer learning uses a model pretrained on a large dataset (e.g., ImageNet) as a starting point for a new task. The pretrained model has learned general visual features (edges, textures, shapes) that transfer to new domains. Fine-tuning adjusts these weights for the specific task, requiring less data and training time.",
        "keyTerms": ["Pretrained Model", "Feature Extraction", "Fine-tuning", "Frozen Layers", "Learning Rate Schedule", "Domain Adaptation"]
      },
      "keyFormulas": [
        {
          "id": "finetune_lr",
          "name": "Discriminative Learning Rates",
          "formula": "lr_layer = base_lr Ã— decay^(N - layer)",
          "latex": "lr_{layer} = lr_{base} \\times \\gamma^{N - layer}",
          "meaning": "Earlier layers learn slower"
        }
      ],
      "examples": [
        {
          "id": "tl_ex1",
          "question": "When to freeze vs fine-tune layers?",
          "steps": [
            {"step": 1, "action": "Small dataset, similar domain", "result": "Freeze most, train classifier", "explanation": "Avoid overfitting"},
            {"step": 2, "action": "Large dataset, similar domain", "result": "Fine-tune all with low lr", "explanation": "Adapt to specifics"},
            {"step": 3, "action": "Different domain", "result": "Fine-tune later layers more", "explanation": "Earlier layers are general"}
          ],
          "finalAnswer": "Freeze for small data; fine-tune all for large data or different domains",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Medical Imaging", "description": "Few labeled X-rays, pretrain on ImageNet"},
        {"title": "Custom Classification", "description": "Train product classifier with few examples"},
        {"title": "Edge Deployment", "description": "Start with efficient pretrained model"}
      ],
      "codeExample": {
        "python": "import torch\nimport torch.nn as nn\nfrom torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n\n# Load pretrained model\nweights = EfficientNet_B0_Weights.DEFAULT\nmodel = efficientnet_b0(weights=weights)\n\n# Freeze early layers\nfor name, param in model.named_parameters():\n    if 'features.0' in name or 'features.1' in name:\n        param.requires_grad = False\n\n# Replace classifier for new task\nnum_classes = 10\nmodel.classifier = nn.Sequential(\n    nn.Dropout(0.2),\n    nn.Linear(1280, num_classes)\n)\n\n# Different learning rates\noptimizer = torch.optim.AdamW([\n    {'params': model.features.parameters(), 'lr': 1e-5},  # Backbone\n    {'params': model.classifier.parameters(), 'lr': 1e-3}  # Head\n])\n\n# Training loop with gradual unfreezing\nfor epoch in range(10):\n    if epoch == 5:  # Unfreeze all after 5 epochs\n        for param in model.parameters():\n            param.requires_grad = True"
      },
      "tips": [
        "Always use pretrained model when data < 10k images",
        "timm library has 700+ pretrained vision models",
        "Gradual unfreezing: train head first, then unfreeze backbone"
      ]
    }
  ]
}

