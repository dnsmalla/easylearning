{
  "category": "Neural Networks",
  "categoryId": "neural_networks",
  "version": "1.0.0",
  "description": "Artificial neural networks, architectures, and training techniques",
  "icon": "circle.grid.cross",
  "color": "#8B5CF6",
  "topics": [
    {
      "id": "perceptron",
      "title": "Perceptron",
      "symbol": "‚≠ï",
      "level": "beginner",
      "definition": {
        "text": "The Perceptron is the simplest form of a neural network - a single artificial neuron. It takes multiple inputs, multiplies each by a weight, sums them, adds a bias, and passes through an activation function. While limited to linearly separable problems, it's the building block for all neural networks.",
        "keyTerms": ["Weights", "Bias", "Linear Combination", "Threshold", "Binary Classifier", "Linearly Separable", "Frank Rosenblatt"]
      },
      "keyFormulas": [
        {
          "id": "perceptron_output",
          "name": "Perceptron Output",
          "formula": "y = f(Œ£w·µ¢x·µ¢ + b)",
          "latex": "y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)",
          "meaning": "Weighted sum through activation function"
        },
        {
          "id": "perceptron_update",
          "name": "Weight Update Rule",
          "formula": "w·µ¢ = w·µ¢ + Œ±(y - ≈∑)x·µ¢",
          "latex": "w_i := w_i + \\alpha(y - \\hat{y})x_i",
          "meaning": "Œ± = learning rate, error-driven update"
        }
      ],
      "examples": [
        {
          "id": "perceptron_ex1",
          "question": "Given inputs x=[1, 2], weights w=[0.5, -0.3], bias b=0.1, calculate the weighted sum",
          "steps": [
            {"step": 1, "action": "Multiply inputs by weights", "result": "1√ó0.5 + 2√ó(-0.3) = 0.5 - 0.6 = -0.1", "explanation": "Weighted inputs"},
            {"step": 2, "action": "Add bias", "result": "-0.1 + 0.1 = 0", "explanation": "Add bias term"},
            {"step": 3, "action": "Apply step function", "result": "step(0) = 1 (or 0 depending on threshold)", "explanation": "Activation"}
          ],
          "finalAnswer": "Weighted sum = 0 (output depends on activation)",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Historical Significance", "description": "First learning algorithm (1958)"},
        {"title": "AND/OR Gates", "description": "Can learn simple logical functions"},
        {"title": "Foundation", "description": "Building block for neural networks"}
      ],
      "codeExample": {
        "python": "import numpy as np\n\nclass Perceptron:\n    def __init__(self, n_features, learning_rate=0.1):\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        self.lr = learning_rate\n    \n    def predict(self, x):\n        linear = np.dot(x, self.weights) + self.bias\n        return 1 if linear >= 0 else 0\n    \n    def train(self, X, y, epochs=100):\n        for epoch in range(epochs):\n            errors = 0\n            for xi, yi in zip(X, y):\n                prediction = self.predict(xi)\n                error = yi - prediction\n                # Update weights and bias\n                self.weights += self.lr * error * xi\n                self.bias += self.lr * error\n                errors += abs(error)\n            if errors == 0:\n                print(f'Converged at epoch {epoch}')\n                break\n\n# Example: Learning AND gate\nX = np.array([[0,0], [0,1], [1,0], [1,1]])\ny = np.array([0, 0, 0, 1])  # AND\n\nperceptron = Perceptron(n_features=2)\nperceptron.train(X, y)\nprint(f'Weights: {perceptron.weights}, Bias: {perceptron.bias}')"
      },
      "tips": [
        "Perceptrons can only solve linearly separable problems",
        "XOR problem showed perceptron limitations ‚Üí led to multi-layer networks",
        "Modern 'neurons' use differentiable activation functions"
      ]
    },
    {
      "id": "activation_functions",
      "title": "Activation Functions",
      "symbol": "üìê",
      "level": "beginner",
      "definition": {
        "text": "Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns. Without activation functions, a network would just be a linear model. Common choices include ReLU (most popular for hidden layers), Sigmoid (probabilities), Tanh (centered output), and Softmax (multi-class probabilities).",
        "keyTerms": ["ReLU", "Sigmoid", "Tanh", "Softmax", "Leaky ReLU", "ELU", "GELU", "Non-linearity", "Saturation"]
      },
      "keyFormulas": [
        {
          "id": "relu",
          "name": "ReLU",
          "formula": "f(x) = max(0, x)",
          "latex": "f(x) = \\max(0, x)",
          "meaning": "Zero for negative, identity for positive"
        },
        {
          "id": "sigmoid",
          "name": "Sigmoid",
          "formula": "œÉ(x) = 1 / (1 + e‚ÅªÀ£)",
          "latex": "\\sigma(x) = \\frac{1}{1 + e^{-x}}",
          "meaning": "Squashes to (0, 1)"
        },
        {
          "id": "tanh",
          "name": "Tanh",
          "formula": "tanh(x) = (eÀ£ - e‚ÅªÀ£) / (eÀ£ + e‚ÅªÀ£)",
          "latex": "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}",
          "meaning": "Squashes to (-1, 1), zero-centered"
        },
        {
          "id": "softmax",
          "name": "Softmax",
          "formula": "softmax(x·µ¢) = eÀ£‚Å± / Œ£eÀ£ ≤",
          "latex": "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}",
          "meaning": "Probability distribution over classes"
        }
      ],
      "examples": [
        {
          "id": "act_ex1",
          "question": "Calculate ReLU and Sigmoid for x = -2 and x = 3",
          "steps": [
            {"step": 1, "action": "ReLU for x=-2", "result": "max(0, -2) = 0", "explanation": "Negative ‚Üí 0"},
            {"step": 2, "action": "ReLU for x=3", "result": "max(0, 3) = 3", "explanation": "Positive ‚Üí identity"},
            {"step": 3, "action": "Sigmoid for x=-2", "result": "1/(1+e¬≤) ‚âà 0.12", "explanation": "Close to 0"},
            {"step": 4, "action": "Sigmoid for x=3", "result": "1/(1+e‚Åª¬≥) ‚âà 0.95", "explanation": "Close to 1"}
          ],
          "finalAnswer": "ReLU: [0, 3], Sigmoid: [0.12, 0.95]",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Hidden Layers", "description": "ReLU is default for most deep networks"},
        {"title": "Binary Classification", "description": "Sigmoid for output layer"},
        {"title": "Multi-class", "description": "Softmax for mutually exclusive classes"}
      ],
      "codeExample": {
        "python": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Common activation functions\ndef relu(x):\n    return np.maximum(0, x)\n\ndef leaky_relu(x, alpha=0.01):\n    return np.where(x > 0, x, alpha * x)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))  # Numerical stability\n    return exp_x / exp_x.sum()\n\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(\n        np.sqrt(2/np.pi) * (x + 0.044715 * x**3)\n    ))\n\n# Example\nx = np.linspace(-5, 5, 100)\nplt.figure(figsize=(12, 4))\nfor i, (name, func) in enumerate([\n    ('ReLU', relu), ('Sigmoid', sigmoid), ('Tanh', tanh)\n]):\n    plt.subplot(1, 3, i+1)\n    plt.plot(x, func(x))\n    plt.title(name)\n    plt.grid(True)\nplt.tight_layout()\nplt.show()"
      },
      "tips": [
        "ReLU: Fast, effective, but can 'die' (Leaky ReLU helps)",
        "Sigmoid/Tanh: Saturate at extremes ‚Üí vanishing gradients",
        "GELU: Used in Transformers (GPT, BERT)"
      ]
    },
    {
      "id": "backpropagation",
      "title": "Backpropagation",
      "symbol": "‚¨ÖÔ∏è",
      "level": "intermediate",
      "definition": {
        "text": "Backpropagation is the algorithm for computing gradients in neural networks by applying the chain rule of calculus. It propagates the error backward from the output layer to update all weights. Forward pass computes predictions, backward pass computes gradients, then gradient descent updates weights. It's the foundation of modern deep learning.",
        "keyTerms": ["Chain Rule", "Gradient", "Forward Pass", "Backward Pass", "Computational Graph", "Automatic Differentiation"]
      },
      "keyFormulas": [
        {
          "id": "chain_rule",
          "name": "Chain Rule",
          "formula": "‚àÇL/‚àÇw = (‚àÇL/‚àÇy) √ó (‚àÇy/‚àÇz) √ó (‚àÇz/‚àÇw)",
          "latex": "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}",
          "meaning": "Gradient flows backward through layers"
        },
        {
          "id": "weight_gradient",
          "name": "Weight Gradient",
          "formula": "‚àÇL/‚àÇW = Œ¥ √ó a·µÄ",
          "latex": "\\frac{\\partial L}{\\partial W} = \\delta \\cdot a^T",
          "meaning": "Œ¥ = error signal, a = previous layer activation"
        }
      ],
      "examples": [
        {
          "id": "bp_ex1",
          "question": "For z = wx, y = œÉ(z), L = (y-t)¬≤, find ‚àÇL/‚àÇw",
          "steps": [
            {"step": 1, "action": "‚àÇL/‚àÇy", "result": "2(y - t)", "explanation": "Derivative of MSE"},
            {"step": 2, "action": "‚àÇy/‚àÇz", "result": "œÉ(z)(1 - œÉ(z))", "explanation": "Sigmoid derivative"},
            {"step": 3, "action": "‚àÇz/‚àÇw", "result": "x", "explanation": "z = wx ‚Üí derivative is x"},
            {"step": 4, "action": "Chain rule", "result": "‚àÇL/‚àÇw = 2(y-t) √ó œÉ(z)(1-œÉ(z)) √ó x", "explanation": "Multiply all terms"}
          ],
          "finalAnswer": "‚àÇL/‚àÇw = 2(y-t) √ó œÉ(z)(1-œÉ(z)) √ó x",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Deep Learning Training", "description": "All neural networks use backprop"},
        {"title": "Automatic Differentiation", "description": "PyTorch, TensorFlow autograd"},
        {"title": "LLM Training", "description": "GPT models trained with backprop"}
      ],
      "codeExample": {
        "python": "import numpy as np\n\nclass SimpleNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        # Initialize weights\n        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n        self.b1 = np.zeros((1, hidden_size))\n        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n        self.b2 = np.zeros((1, output_size))\n    \n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n    \n    def forward(self, X):\n        # Forward pass\n        self.z1 = X @ self.W1 + self.b1\n        self.a1 = self.sigmoid(self.z1)\n        self.z2 = self.a1 @ self.W2 + self.b2\n        self.a2 = self.sigmoid(self.z2)\n        return self.a2\n    \n    def backward(self, X, y, lr=0.1):\n        m = X.shape[0]\n        \n        # Backward pass (backpropagation)\n        dz2 = self.a2 - y\n        dW2 = (1/m) * self.a1.T @ dz2\n        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n        \n        da1 = dz2 @ self.W2.T\n        dz1 = da1 * self.a1 * (1 - self.a1)  # sigmoid gradient\n        dW1 = (1/m) * X.T @ dz1\n        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n        \n        # Update weights\n        self.W2 -= lr * dW2\n        self.b2 -= lr * db2\n        self.W1 -= lr * dW1\n        self.b1 -= lr * db1\n\n# XOR example\nX = np.array([[0,0], [0,1], [1,0], [1,1]])\ny = np.array([[0], [1], [1], [0]])\n\nnn = SimpleNN(2, 4, 1)\nfor epoch in range(10000):\n    output = nn.forward(X)\n    nn.backward(X, y)\n\nprint('Predictions:', nn.forward(X).round(2).flatten())"
      },
      "tips": [
        "PyTorch/TensorFlow handle backprop automatically (autograd)",
        "loss.backward() computes all gradients",
        "Understanding backprop helps debug training issues"
      ]
    },
    {
      "id": "optimizers",
      "title": "Optimizers",
      "symbol": "üöÄ",
      "level": "intermediate",
      "definition": {
        "text": "Optimizers determine how to update neural network weights based on computed gradients. SGD is the simplest, updating with learning rate √ó gradient. Momentum adds velocity to escape local minima. Adam combines momentum with adaptive learning rates per parameter and is the most popular choice. Learning rate scheduling helps fine-tune training.",
        "keyTerms": ["SGD", "Momentum", "Adam", "RMSprop", "AdaGrad", "Learning Rate", "Weight Decay", "Adaptive Learning Rate"]
      },
      "keyFormulas": [
        {
          "id": "sgd",
          "name": "SGD",
          "formula": "Œ∏ = Œ∏ - Œ±‚àáL",
          "latex": "\\theta := \\theta - \\alpha \\nabla L",
          "meaning": "Simple gradient descent step"
        },
        {
          "id": "momentum",
          "name": "Momentum",
          "formula": "v = Œ≤v + ‚àáL, Œ∏ = Œ∏ - Œ±v",
          "latex": "v := \\beta v + \\nabla L, \\quad \\theta := \\theta - \\alpha v",
          "meaning": "Œ≤ = momentum coefficient (usually 0.9)"
        },
        {
          "id": "adam",
          "name": "Adam",
          "formula": "Œ∏ = Œ∏ - Œ± √ó mÃÇ / (‚àövÃÇ + Œµ)",
          "latex": "\\theta := \\theta - \\alpha \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon}",
          "meaning": "Adaptive moment estimation"
        }
      ],
      "examples": [
        {
          "id": "opt_ex1",
          "question": "Why does Adam often converge faster than vanilla SGD?",
          "steps": [
            {"step": 1, "action": "Momentum component", "result": "Maintains moving average of gradients", "explanation": "Smooths updates, avoids oscillation"},
            {"step": 2, "action": "Adaptive learning rates", "result": "Different rate per parameter", "explanation": "Infrequent features get larger updates"},
            {"step": 3, "action": "Bias correction", "result": "Corrects initialization bias", "explanation": "Better early training"}
          ],
          "finalAnswer": "Adam combines momentum + adaptive LR + bias correction",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Deep Learning", "description": "Adam is default for most projects"},
        {"title": "NLP/Transformers", "description": "AdamW with weight decay for BERT, GPT"},
        {"title": "Fine-tuning", "description": "Lower learning rates with Adam"}
      ],
      "codeExample": {
        "python": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Simple network\nmodel = nn.Sequential(\n    nn.Linear(10, 50),\n    nn.ReLU(),\n    nn.Linear(50, 1)\n)\n\n# Different optimizers\noptimizers = {\n    'SGD': optim.SGD(model.parameters(), lr=0.01),\n    'SGD+Momentum': optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n    'Adam': optim.Adam(model.parameters(), lr=0.001),\n    'AdamW': optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01),\n    'RMSprop': optim.RMSprop(model.parameters(), lr=0.001)\n}\n\n# Training loop example\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\nfor epoch in range(100):\n    X = torch.randn(32, 10)  # batch of 32\n    y = torch.randn(32, 1)\n    \n    optimizer.zero_grad()  # Clear gradients\n    output = model(X)\n    loss = criterion(output, y)\n    loss.backward()        # Compute gradients\n    optimizer.step()       # Update weights\n\nprint('Training complete')"
      },
      "tips": [
        "Start with Adam, lr=0.001 for most problems",
        "Use AdamW (Adam + weight decay) for transformers",
        "Reduce LR when loss plateaus (ReduceLROnPlateau)"
      ]
    },
    {
      "id": "batch_norm",
      "title": "Batch Normalization",
      "symbol": "üìä",
      "level": "intermediate",
      "definition": {
        "text": "Batch Normalization normalizes layer inputs by subtracting the batch mean and dividing by batch standard deviation, then scaling and shifting with learnable parameters. It reduces internal covariate shift, allows higher learning rates, acts as regularization, and significantly speeds up training. It's used in most modern architectures.",
        "keyTerms": ["Normalization", "Internal Covariate Shift", "Running Mean", "Running Variance", "Gamma", "Beta", "Layer Normalization"]
      },
      "keyFormulas": [
        {
          "id": "bn",
          "name": "Batch Normalization",
          "formula": "≈∑ = Œ≥ √ó (x - Œº)/œÉ + Œ≤",
          "latex": "\\hat{y} = \\gamma \\cdot \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\beta",
          "meaning": "Œº, œÉ = batch stats; Œ≥, Œ≤ = learnable"
        }
      ],
      "examples": [
        {
          "id": "bn_ex1",
          "question": "Why use batch norm after a linear layer but before activation?",
          "steps": [
            {"step": 1, "action": "Consider internal covariate shift", "result": "Layer inputs change during training", "explanation": "Makes optimization harder"},
            {"step": 2, "action": "Normalization before activation", "result": "Activations in optimal range", "explanation": "Avoids saturation in sigmoid/tanh"},
            {"step": 3, "action": "Learnable Œ≥, Œ≤", "result": "Network can undo normalization if needed", "explanation": "Flexibility"}
          ],
          "finalAnswer": "BN stabilizes inputs to activations, speeding up training",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Computer Vision", "description": "Essential in CNNs (ResNet, etc.)"},
        {"title": "Faster Training", "description": "Higher learning rates possible"},
        {"title": "Regularization", "description": "Slight noise from batch statistics"}
      ],
      "codeExample": {
        "python": "import torch\nimport torch.nn as nn\n\n# CNN with Batch Normalization\nclass CNNWithBN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)  # After conv, before activation\n        \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        \n        self.fc = nn.Linear(64 * 8 * 8, 10)\n        \n    def forward(self, x):\n        # Conv -> BatchNorm -> ReLU -> Pool\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.max_pool2d(x, 2)\n        \n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.max_pool2d(x, 2)\n        \n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.fc(x)\n        return x\n\n# MLP with BatchNorm\nmlp = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.BatchNorm1d(256),  # 1d for fully connected\n    nn.ReLU(),\n    nn.Linear(256, 10)\n)\n\nprint('Model with BatchNorm ready')"
      },
      "tips": [
        "Use BatchNorm1d for FC layers, BatchNorm2d for Conv layers",
        "During inference, use running statistics (model.eval())",
        "Layer Normalization is preferred for Transformers/NLP"
      ]
    },
    {
      "id": "dropout",
      "title": "Dropout",
      "symbol": "‚ùå",
      "level": "intermediate",
      "definition": {
        "text": "Dropout is a regularization technique that randomly 'drops' (sets to zero) a fraction of neurons during training. This prevents co-adaptation of neurons, forces the network to learn redundant representations, and acts as an ensemble method. At inference time, all neurons are used but outputs are scaled. Typical dropout rates are 0.2-0.5.",
        "keyTerms": ["Regularization", "Co-adaptation", "Dropout Rate", "Inverted Dropout", "Monte Carlo Dropout", "DropConnect"]
      },
      "keyFormulas": [
        {
          "id": "dropout_train",
          "name": "Training (Inverted)",
          "formula": "y = x √ó mask / (1 - p)",
          "latex": "y = \\frac{x \\cdot \\text{mask}}{1 - p}",
          "meaning": "mask ‚àà {0,1}, p = dropout probability"
        },
        {
          "id": "dropout_test",
          "name": "Inference",
          "formula": "y = x",
          "latex": "y = x",
          "meaning": "No dropout at test time (already scaled)"
        }
      ],
      "examples": [
        {
          "id": "dropout_ex1",
          "question": "With dropout p=0.3, what happens to a neuron with output 10 during training?",
          "steps": [
            {"step": 1, "action": "Generate random mask", "result": "30% chance of being dropped", "explanation": "Randomly selected"},
            {"step": 2, "action": "If kept (70% chance)", "result": "Output = 10 / (1 - 0.3) = 10 / 0.7 ‚âà 14.3", "explanation": "Inverted dropout scaling"},
            {"step": 3, "action": "If dropped (30% chance)", "result": "Output = 0", "explanation": "Zeroed out"}
          ],
          "finalAnswer": "Either 0 (dropped) or ~14.3 (kept and scaled)",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Preventing Overfitting", "description": "Essential for deep networks"},
        {"title": "Uncertainty Estimation", "description": "MC Dropout for Bayesian approximation"},
        {"title": "NLP Models", "description": "Used in Transformers for regularization"}
      ],
      "codeExample": {
        "python": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ModelWithDropout(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n        self.fc3 = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout1(x)  # Dropout after activation\n        x = F.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = self.fc3(x)\n        return x\n\n# Usage\nmodel = ModelWithDropout(784, 256, 10, dropout_rate=0.5)\n\n# Training: dropout is active\nmodel.train()\noutput_train = model(torch.randn(32, 784))\n\n# Inference: dropout is disabled\nmodel.eval()\nwith torch.no_grad():\n    output_test = model(torch.randn(32, 784))\n\nprint('Remember: model.train() vs model.eval()')"
      },
      "tips": [
        "Higher dropout (0.5) for fully connected, lower (0.1-0.3) for conv",
        "Always use model.train() and model.eval() correctly",
        "Don't use dropout in the final layer"
      ]
    },
    {
      "id": "embeddings",
      "title": "Embedding Layers",
      "symbol": "üî¢",
      "level": "intermediate",
      "definition": {
        "text": "Embedding layers convert discrete tokens (words, categories) into dense, learnable vectors. Instead of one-hot encoding with sparse vectors, embeddings create compact representations where similar items are close in vector space. They're essential for NLP (word embeddings) and recommendation systems (user/item embeddings).",
        "keyTerms": ["Lookup Table", "Embedding Dimension", "Pretrained Embeddings", "Word2Vec", "GloVe", "Positional Embedding", "Token Embedding"]
      },
      "keyFormulas": [
        {
          "id": "embed_lookup",
          "name": "Embedding Lookup",
          "formula": "e = E[token_id]",
          "latex": "\\mathbf{e} = \\mathbf{E}[\\text{token\\_id}]",
          "meaning": "E is (vocab_size √ó embed_dim) matrix"
        }
      ],
      "examples": [
        {
          "id": "embed_ex1",
          "question": "How many parameters does an embedding layer with vocab_size=10000, embed_dim=256 have?",
          "steps": [
            {"step": 1, "action": "Identify dimensions", "result": "10000 √ó 256", "explanation": "One vector per token"},
            {"step": 2, "action": "Calculate parameters", "result": "10000 √ó 256 = 2,560,000", "explanation": "Total learnable weights"},
            {"step": 3, "action": "Convert to millions", "result": "2.56M parameters", "explanation": "Embeddings are parameter-heavy"}
          ],
          "finalAnswer": "2.56 million parameters",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "LLMs", "description": "Token embeddings in GPT, BERT"},
        {"title": "Recommendation", "description": "User and item embeddings"},
        {"title": "Semantic Search", "description": "Sentence embeddings for retrieval"}
      ],
      "codeExample": {
        "python": "import torch\nimport torch.nn as nn\n\n# Basic embedding layer\nvocab_size = 10000\nembed_dim = 256\n\nembedding = nn.Embedding(vocab_size, embed_dim)\n\n# Token IDs to embeddings\ntoken_ids = torch.tensor([1, 42, 1000, 5])\nembedded = embedding(token_ids)\nprint(f'Input shape: {token_ids.shape}')      # [4]\nprint(f'Output shape: {embedded.shape}')       # [4, 256]\n\n# With padding token\nembedding_padded = nn.Embedding(\n    vocab_size, embed_dim,\n    padding_idx=0  # Token 0 stays at zero\n)\n\n# Initialize with pretrained (e.g., GloVe)\npretrained_weights = torch.randn(vocab_size, embed_dim)\nembedding.weight = nn.Parameter(pretrained_weights)\nembedding.weight.requires_grad = False  # Freeze if desired"
      },
      "tips": [
        "Embedding dimension is a hyperparameter (64-1024 typical)",
        "Use padding_idx to handle variable-length sequences",
        "Pretrained embeddings (GloVe, Word2Vec) can boost performance"
      ]
    },
    {
      "id": "layer_norm",
      "title": "Layer Normalization",
      "symbol": "üìè",
      "level": "intermediate",
      "definition": {
        "text": "Layer Normalization normalizes activations across the feature dimension for each sample independently, unlike Batch Norm which normalizes across the batch. This makes it suitable for RNNs and Transformers where batch statistics are problematic. It stabilizes training and enables faster convergence.",
        "keyTerms": ["Feature Normalization", "Batch Independence", "Learnable Parameters", "Gamma", "Beta", "RMSNorm", "Pre-LN vs Post-LN"]
      },
      "keyFormulas": [
        {
          "id": "layer_norm_formula",
          "name": "Layer Normalization",
          "formula": "y = Œ≥ √ó (x - Œº) / œÉ + Œ≤",
          "latex": "y = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta",
          "meaning": "Œº, œÉ computed per sample across features"
        }
      ],
      "examples": [
        {
          "id": "ln_ex1",
          "question": "Why use LayerNorm instead of BatchNorm in Transformers?",
          "steps": [
            {"step": 1, "action": "BatchNorm dependency", "result": "Needs batch statistics", "explanation": "Problematic for variable sequences"},
            {"step": 2, "action": "Inference issue", "result": "Running mean/var from training", "explanation": "May not match test distribution"},
            {"step": 3, "action": "LayerNorm advantage", "result": "Sample-independent", "explanation": "Works with any batch size, including 1"}
          ],
          "finalAnswer": "LayerNorm is batch-independent, making it ideal for sequence models",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Transformers", "description": "Standard normalization in GPT, BERT"},
        {"title": "RNNs/LSTMs", "description": "Works better than BatchNorm for sequences"},
        {"title": "LLMs", "description": "RMSNorm (simplified LayerNorm) in LLaMA"}
      ],
      "codeExample": {
        "python": "import torch\nimport torch.nn as nn\n\n# LayerNorm normalizes across last dimension(s)\nbatch_size = 32\nseq_len = 100\nhidden_dim = 512\n\nx = torch.randn(batch_size, seq_len, hidden_dim)\n\n# LayerNorm (normalizes across hidden_dim for each position)\nlayer_norm = nn.LayerNorm(hidden_dim)\noutput = layer_norm(x)\n\nprint(f'Input shape: {x.shape}')      # [32, 100, 512]\nprint(f'Output shape: {output.shape}')  # [32, 100, 512]\n\n# Verify normalization (per sample, per position)\nsample_position = output[0, 0]  # First sample, first position\nprint(f'Mean: {sample_position.mean():.4f}')  # ~0\nprint(f'Std: {sample_position.std():.4f}')    # ~1\n\n# RMSNorm (used in LLaMA, simpler)\nclass RMSNorm(nn.Module):\n    def __init__(self, dim, eps=1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(dim))\n        self.eps = eps\n    \n    def forward(self, x):\n        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n        return x / rms * self.weight"
      },
      "tips": [
        "Pre-LN (norm before attention) is more stable than Post-LN",
        "RMSNorm is faster and works just as well in practice",
        "LayerNorm has learnable Œ≥ and Œ≤ parameters"
      ]
    },
    {
      "id": "learning_rate_scheduling",
      "title": "Learning Rate Scheduling",
      "symbol": "üìâ",
      "level": "intermediate",
      "definition": {
        "text": "Learning rate scheduling adjusts the learning rate during training to improve convergence. Starting high allows fast progress; decreasing over time enables fine-tuning. Common strategies include step decay, cosine annealing, warmup (gradual increase at start), and one-cycle policy. Proper scheduling can significantly improve final performance.",
        "keyTerms": ["Step Decay", "Cosine Annealing", "Warmup", "One-Cycle", "ReduceLROnPlateau", "Polynomial Decay", "Linear Warmup"]
      },
      "keyFormulas": [
        {
          "id": "cosine_schedule",
          "name": "Cosine Annealing",
          "formula": "lr = lr_min + 0.5(lr_max - lr_min)(1 + cos(œÄt/T))",
          "latex": "lr_t = lr_{min} + \\frac{1}{2}(lr_{max} - lr_{min})\\left(1 + \\cos\\frac{\\pi t}{T}\\right)",
          "meaning": "Smooth decay from max to min over T steps"
        }
      ],
      "examples": [
        {
          "id": "lr_ex1",
          "question": "Why use warmup in Transformer training?",
          "steps": [
            {"step": 1, "action": "Initial weights", "result": "Random, high variance", "explanation": "Adam statistics unstable"},
            {"step": 2, "action": "Large LR + bad estimates", "result": "Can cause divergence", "explanation": "Early gradients unreliable"},
            {"step": 3, "action": "Warmup benefit", "result": "Gradual increase stabilizes start", "explanation": "Let optimizer adapt"}
          ],
          "finalAnswer": "Warmup prevents early training instability when optimizer statistics are unreliable",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "LLM Training", "description": "Linear warmup + cosine decay is standard"},
        {"title": "Fine-tuning", "description": "Lower LR to preserve pretrained knowledge"},
        {"title": "Long Training", "description": "Cosine annealing with restarts"}
      ],
      "codeExample": {
        "python": "import torch\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import (\n    StepLR, CosineAnnealingLR, OneCycleLR,\n    LinearLR, SequentialLR\n)\n\nmodel = torch.nn.Linear(10, 2)\noptimizer = AdamW(model.parameters(), lr=1e-3)\n\n# Step decay: multiply LR by 0.1 every 30 epochs\nscheduler_step = StepLR(optimizer, step_size=30, gamma=0.1)\n\n# Cosine annealing\nscheduler_cosine = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n\n# One cycle (warmup + decay in one)\nscheduler_onecycle = OneCycleLR(\n    optimizer, max_lr=1e-3, \n    total_steps=1000,\n    pct_start=0.1  # 10% warmup\n)\n\n# Linear warmup + cosine decay (common for Transformers)\nwarmup_steps = 100\ntotal_steps = 1000\n\nwarmup = LinearLR(optimizer, start_factor=0.1, total_iters=warmup_steps)\ncosine = CosineAnnealingLR(optimizer, T_max=total_steps - warmup_steps)\nscheduler = SequentialLR(optimizer, [warmup, cosine], milestones=[warmup_steps])\n\n# Training loop\nfor step in range(total_steps):\n    # ... training step ...\n    scheduler.step()\n    if step % 100 == 0:\n        print(f'Step {step}: LR = {scheduler.get_last_lr()[0]:.6f}')"
      },
      "tips": [
        "Warmup for 1-10% of total training is common",
        "Cosine annealing often beats step decay",
        "Monitor LR in logs to debug training issues"
      ]
    }
  ]
}

