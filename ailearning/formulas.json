{
  "version": "1.0.0",
  "description": "Essential AI/ML formulas and equations reference",
  "categories": [
    {
      "id": "loss_functions",
      "name": "Loss Functions",
      "icon": "chart.line.downtrend.xyaxis",
      "color": "#EF4444",
      "formulas": [
        {
          "id": "mse",
          "title": "Mean Squared Error",
          "description": "Regression loss, penalizes large errors quadratically",
          "formula": "L = (1/n) Σ(yᵢ - ŷᵢ)²",
          "latex": "L = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2",
          "units": "Squared units of target",
          "level": "beginner",
          "example": "Predicting house prices: MSE = avg((actual - predicted)²)"
        },
        {
          "id": "cross_entropy",
          "title": "Cross-Entropy Loss",
          "description": "Classification loss, measures probability distribution difference",
          "formula": "L = -Σ yᵢ·log(pᵢ)",
          "latex": "L = -\\sum_{i=1}^{C} y_i \\log(p_i)",
          "units": "Nats or bits",
          "level": "beginner",
          "example": "Image classification: penalize confident wrong predictions"
        },
        {
          "id": "bce",
          "title": "Binary Cross-Entropy",
          "description": "Binary classification loss",
          "formula": "L = -[y·log(p) + (1-y)·log(1-p)]",
          "latex": "L = -[y \\log(p) + (1-y)\\log(1-p)]",
          "units": "Nats",
          "level": "beginner",
          "example": "Spam detection: probability of being spam"
        },
        {
          "id": "huber",
          "title": "Huber Loss",
          "description": "Robust regression loss, less sensitive to outliers",
          "formula": "L = ½(y-ŷ)² if |y-ŷ| ≤ δ, else δ|y-ŷ| - ½δ²",
          "latex": "L = \\begin{cases} \\frac{1}{2}(y-\\hat{y})^2 & |y-\\hat{y}| \\leq \\delta \\\\ \\delta|y-\\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise} \\end{cases}",
          "units": "Mixed",
          "level": "intermediate",
          "example": "Regression with outliers in data"
        }
      ]
    },
    {
      "id": "activation_functions",
      "name": "Activation Functions",
      "icon": "waveform.path.ecg",
      "color": "#8B5CF6",
      "formulas": [
        {
          "id": "relu",
          "title": "ReLU",
          "description": "Most common hidden layer activation",
          "formula": "f(x) = max(0, x)",
          "latex": "f(x) = \\max(0, x)",
          "units": "Same as input",
          "level": "beginner",
          "example": "Default for hidden layers in CNNs, MLPs"
        },
        {
          "id": "sigmoid",
          "title": "Sigmoid",
          "description": "Squashes to (0,1), good for probabilities",
          "formula": "σ(x) = 1 / (1 + e⁻ˣ)",
          "latex": "\\sigma(x) = \\frac{1}{1 + e^{-x}}",
          "units": "(0, 1)",
          "level": "beginner",
          "example": "Binary classification output layer"
        },
        {
          "id": "softmax",
          "title": "Softmax",
          "description": "Multi-class probability distribution",
          "formula": "softmax(xᵢ) = eˣⁱ / Σeˣʲ",
          "latex": "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}",
          "units": "(0, 1), sums to 1",
          "level": "beginner",
          "example": "Multi-class classification output"
        },
        {
          "id": "gelu",
          "title": "GELU",
          "description": "Gaussian Error Linear Unit, used in Transformers",
          "formula": "f(x) = x · Φ(x)",
          "latex": "f(x) = x \\cdot \\Phi(x)",
          "units": "Same as input",
          "level": "intermediate",
          "example": "BERT, GPT activation function"
        }
      ]
    },
    {
      "id": "optimization",
      "name": "Optimization",
      "icon": "arrow.down.right",
      "color": "#10B981",
      "formulas": [
        {
          "id": "sgd",
          "title": "SGD Update",
          "description": "Stochastic Gradient Descent",
          "formula": "θ = θ - α·∇L(θ)",
          "latex": "\\theta := \\theta - \\alpha \\nabla L(\\theta)",
          "units": "Parameter units",
          "level": "beginner",
          "example": "α = 0.01, gradient points uphill, go opposite"
        },
        {
          "id": "momentum",
          "title": "Momentum",
          "description": "SGD with velocity term",
          "formula": "v = βv + ∇L, θ = θ - αv",
          "latex": "v := \\beta v + \\nabla L, \\quad \\theta := \\theta - \\alpha v",
          "units": "Parameter units",
          "level": "intermediate",
          "example": "β = 0.9 maintains running average of gradients"
        },
        {
          "id": "adam",
          "title": "Adam",
          "description": "Adaptive Moment Estimation",
          "formula": "θ = θ - α·m̂/(√v̂ + ε)",
          "latex": "\\theta := \\theta - \\alpha \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon}",
          "units": "Parameter units",
          "level": "intermediate",
          "example": "Default optimizer for deep learning"
        }
      ]
    },
    {
      "id": "regularization",
      "name": "Regularization",
      "icon": "scale.3d",
      "color": "#F59E0B",
      "formulas": [
        {
          "id": "l2_reg",
          "title": "L2 Regularization (Ridge)",
          "description": "Penalizes large weights",
          "formula": "L_total = L + λ·Σwᵢ²",
          "latex": "L_{total} = L + \\lambda \\sum_i w_i^2",
          "units": "Loss units",
          "level": "intermediate",
          "example": "Weight decay in Adam: L2 penalty"
        },
        {
          "id": "l1_reg",
          "title": "L1 Regularization (Lasso)",
          "description": "Promotes sparsity",
          "formula": "L_total = L + λ·Σ|wᵢ|",
          "latex": "L_{total} = L + \\lambda \\sum_i |w_i|",
          "units": "Loss units",
          "level": "intermediate",
          "example": "Feature selection - drives weights to exactly 0"
        },
        {
          "id": "dropout",
          "title": "Dropout",
          "description": "Random neuron deactivation",
          "formula": "y = x·mask/(1-p)",
          "latex": "y = \\frac{x \\cdot \\text{mask}}{1-p}",
          "units": "Same as input",
          "level": "intermediate",
          "example": "p=0.5 drops 50% of neurons during training"
        }
      ]
    },
    {
      "id": "attention",
      "name": "Attention Mechanism",
      "icon": "eye",
      "color": "#EC4899",
      "formulas": [
        {
          "id": "scaled_dot_attention",
          "title": "Scaled Dot-Product Attention",
          "description": "Core of Transformers",
          "formula": "Attention(Q,K,V) = softmax(QKᵀ/√d)V",
          "latex": "\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V",
          "units": "Same as V",
          "level": "advanced",
          "example": "Q,K,V from same input in self-attention"
        },
        {
          "id": "multi_head",
          "title": "Multi-Head Attention",
          "description": "Multiple attention heads in parallel",
          "formula": "MultiHead = Concat(head₁,...,headₕ)Wᴼ",
          "latex": "\\text{MultiHead} = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O",
          "units": "Same as input",
          "level": "advanced",
          "example": "GPT-4 uses 96+ attention heads"
        }
      ]
    },
    {
      "id": "metrics",
      "name": "Evaluation Metrics",
      "icon": "chart.bar",
      "color": "#6366F1",
      "formulas": [
        {
          "id": "accuracy",
          "title": "Accuracy",
          "description": "Proportion of correct predictions",
          "formula": "Accuracy = (TP + TN) / (TP + TN + FP + FN)",
          "latex": "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}",
          "units": "[0, 1]",
          "level": "beginner",
          "example": "90% accuracy = 90 correct out of 100"
        },
        {
          "id": "precision",
          "title": "Precision",
          "description": "Of predicted positives, how many are correct",
          "formula": "Precision = TP / (TP + FP)",
          "latex": "\\text{Precision} = \\frac{TP}{TP + FP}",
          "units": "[0, 1]",
          "level": "beginner",
          "example": "Spam filter: don't mark real emails as spam"
        },
        {
          "id": "recall",
          "title": "Recall",
          "description": "Of actual positives, how many did we find",
          "formula": "Recall = TP / (TP + FN)",
          "latex": "\\text{Recall} = \\frac{TP}{TP + FN}",
          "units": "[0, 1]",
          "level": "beginner",
          "example": "Cancer detection: find all actual cases"
        },
        {
          "id": "f1",
          "title": "F1 Score",
          "description": "Harmonic mean of precision and recall",
          "formula": "F1 = 2 × (P × R) / (P + R)",
          "latex": "F_1 = 2 \\times \\frac{P \\times R}{P + R}",
          "units": "[0, 1]",
          "level": "beginner",
          "example": "Balanced metric for imbalanced datasets"
        },
        {
          "id": "auc_roc",
          "title": "AUC-ROC",
          "description": "Area Under ROC Curve",
          "formula": "AUC = P(score(+) > score(-))",
          "latex": "\\text{AUC} = P(\\text{score}(+) > \\text{score}(-))",
          "units": "[0, 1]",
          "level": "intermediate",
          "example": "0.5 = random, 1.0 = perfect separation"
        }
      ]
    }
  ]
}

