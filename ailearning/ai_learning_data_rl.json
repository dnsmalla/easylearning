{
  "category": "Reinforcement Learning",
  "categoryId": "rl",
  "version": "1.0.0",
  "description": "Agents, environments, rewards, and policy optimization",
  "icon": "gamecontroller",
  "color": "#EF4444",
  "topics": [
    {
      "id": "rl_intro",
      "title": "RL Introduction",
      "symbol": "üéØ",
      "level": "beginner",
      "definition": {
        "text": "Reinforcement Learning is a paradigm where an agent learns to make decisions by interacting with an environment. The agent takes actions, receives rewards or penalties, and learns a policy to maximize cumulative reward. Unlike supervised learning, RL learns from trial and error without explicit labels.",
        "keyTerms": ["Agent", "Environment", "State", "Action", "Reward", "Policy", "Episode", "Return"]
      },
      "keyFormulas": [
        {
          "id": "return_formula",
          "name": "Discounted Return",
          "formula": "G_t = Œ£ Œ≥·µè r_{t+k+1}",
          "latex": "G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}",
          "meaning": "Œ≥ is discount factor (0-1)"
        }
      ],
      "examples": [
        {
          "id": "rl_ex1",
          "question": "Why use a discount factor Œ≥?",
          "steps": [
            {"step": 1, "action": "Immediate vs future", "result": "Prefer rewards now", "explanation": "Bird in hand"},
            {"step": 2, "action": "Mathematical", "result": "Ensures finite return", "explanation": "Sum converges if Œ≥ < 1"},
            {"step": 3, "action": "Typical value", "result": "Œ≥ = 0.99 or 0.999", "explanation": "Balance short and long term"}
          ],
          "finalAnswer": "Discount factor bounds return and models preference for sooner rewards",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Game Playing", "description": "AlphaGo, Atari, Chess"},
        {"title": "Robotics", "description": "Learning to walk, grasp objects"},
        {"title": "Recommendation Systems", "description": "Sequential recommendations"}
      ],
      "codeExample": {
        "python": "import gymnasium as gym\n\n# Create environment\nenv = gym.make('CartPole-v1')\n\n# RL loop\nstate, info = env.reset()\ntotal_reward = 0\n\nfor step in range(1000):\n    # Agent selects action (random for now)\n    action = env.action_space.sample()\n    \n    # Environment returns new state, reward, done\n    next_state, reward, terminated, truncated, info = env.step(action)\n    done = terminated or truncated\n    \n    total_reward += reward\n    state = next_state\n    \n    if done:\n        print(f\"Episode finished after {step+1} steps\")\n        print(f\"Total reward: {total_reward}\")\n        break\n\nenv.close()"
      },
      "tips": [
        "RL differs from supervised learning: no labels, just rewards",
        "Exploration vs exploitation: try new things vs use what works",
        "Gymnasium (formerly Gym) is the standard RL environment library"
      ]
    },
    {
      "id": "mdp",
      "title": "Markov Decision Process",
      "symbol": "üîÑ",
      "level": "intermediate",
      "definition": {
        "text": "A Markov Decision Process (MDP) is the mathematical framework for RL. It's defined by states S, actions A, transition probabilities P(s'|s,a), rewards R(s,a,s'), and discount Œ≥. The Markov property states that the future depends only on the current state, not history. Solving an MDP means finding an optimal policy.",
        "keyTerms": ["Markov Property", "State Space", "Action Space", "Transition Probability", "Reward Function", "Bellman Equation"]
      },
      "keyFormulas": [
        {
          "id": "bellman",
          "name": "Bellman Equation (Value)",
          "formula": "V(s) = max_a [R(s,a) + Œ≥ Œ£ P(s'|s,a) V(s')]",
          "latex": "V(s) = \\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V(s') \\right]",
          "meaning": "Optimal value = best action + future value"
        }
      ],
      "examples": [
        {
          "id": "mdp_ex1",
          "question": "Is real-world navigation an MDP?",
          "steps": [
            {"step": 1, "action": "Check Markov property", "result": "Current position sufficient?", "explanation": "Need velocity too"},
            {"step": 2, "action": "Augment state", "result": "State = (position, velocity)", "explanation": "Now Markovian"},
            {"step": 3, "action": "Partial observability", "result": "If can't observe full state ‚Üí POMDP", "explanation": "Different formalism"}
          ],
          "finalAnswer": "Yes, if state includes all relevant information (position, velocity, etc.)",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Inventory Management", "description": "When and how much to reorder"},
        {"title": "Medical Treatment", "description": "Optimal treatment sequences"},
        {"title": "Traffic Control", "description": "Signal timing optimization"}
      ],
      "codeExample": {
        "python": "import numpy as np\n\nclass GridWorldMDP:\n    \"\"\"Simple 4x4 grid world MDP\"\"\"\n    def __init__(self):\n        self.n_states = 16\n        self.n_actions = 4  # up, down, left, right\n        self.goal_state = 15\n        self.gamma = 0.99\n        \n    def step(self, state, action):\n        \"\"\"Returns (next_state, reward, done)\"\"\"\n        # Grid position\n        row, col = state // 4, state % 4\n        \n        # Apply action\n        if action == 0 and row > 0: row -= 1    # up\n        elif action == 1 and row < 3: row += 1  # down\n        elif action == 2 and col > 0: col -= 1  # left\n        elif action == 3 and col < 3: col += 1  # right\n        \n        next_state = row * 4 + col\n        done = (next_state == self.goal_state)\n        reward = 1.0 if done else -0.01\n        \n        return next_state, reward, done\n    \n    def get_transition_prob(self, state, action, next_state):\n        \"\"\"Deterministic: P(s'|s,a) = 1 or 0\"\"\"\n        actual_next, _, _ = self.step(state, action)\n        return 1.0 if next_state == actual_next else 0.0"
      },
      "tips": [
        "POMDPs (Partial Observable MDPs) when state isn't fully visible",
        "Model-based RL learns the transition function P(s'|s,a)",
        "Bellman equation is foundation of value-based methods"
      ]
    },
    {
      "id": "q_learning",
      "title": "Q-Learning",
      "symbol": "üìä",
      "level": "intermediate",
      "definition": {
        "text": "Q-Learning is a model-free, off-policy algorithm that learns the action-value function Q(s,a). Q(s,a) represents the expected return of taking action a in state s, then following the optimal policy. Q-learning updates Q values using temporal difference (TD) learning without needing a model of the environment.",
        "keyTerms": ["Q-value", "Action-Value Function", "Temporal Difference", "Off-policy", "Model-free", "Œµ-greedy", "Q-table"]
      },
      "keyFormulas": [
        {
          "id": "q_update",
          "name": "Q-Learning Update",
          "formula": "Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]",
          "latex": "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\right]",
          "meaning": "Œ±=learning rate, TD error in brackets"
        }
      ],
      "examples": [
        {
          "id": "qlearn_ex1",
          "question": "Why is Q-learning called 'off-policy'?",
          "steps": [
            {"step": 1, "action": "Behavior policy", "result": "Œµ-greedy for exploration", "explanation": "How agent acts"},
            {"step": 2, "action": "Target policy", "result": "max Q(s',a') is greedy", "explanation": "What we're learning"},
            {"step": 3, "action": "Off-policy", "result": "Learning and acting policies differ", "explanation": "Can learn from any data"}
          ],
          "finalAnswer": "Q-learning learns the optimal policy while following an exploratory policy",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Game AI", "description": "Learning to play simple games"},
        {"title": "Robot Navigation", "description": "Grid-based path finding"},
        {"title": "Resource Allocation", "description": "Discrete decision problems"}
      ],
      "codeExample": {
        "python": "import numpy as np\nimport gymnasium as gym\n\ndef q_learning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):\n    # Initialize Q-table\n    Q = np.zeros([env.observation_space.n, env.action_space.n])\n    \n    for episode in range(episodes):\n        state, _ = env.reset()\n        done = False\n        \n        while not done:\n            # Œµ-greedy action selection\n            if np.random.random() < epsilon:\n                action = env.action_space.sample()  # Explore\n            else:\n                action = np.argmax(Q[state])        # Exploit\n            \n            # Take action\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            \n            # Q-learning update\n            td_target = reward + gamma * np.max(Q[next_state]) * (not terminated)\n            td_error = td_target - Q[state, action]\n            Q[state, action] += alpha * td_error\n            \n            state = next_state\n    \n    return Q\n\n# Train on FrozenLake\nenv = gym.make('FrozenLake-v1', is_slippery=False)\nQ = q_learning(env)\nprint(\"Optimal policy:\", np.argmax(Q, axis=1))"
      },
      "tips": [
        "Decay Œµ over time: explore early, exploit later",
        "Q-learning can overestimate Q-values; Double Q-learning fixes this",
        "Doesn't scale to large state spaces (use DQN instead)"
      ]
    },
    {
      "id": "policy_gradient",
      "title": "Policy Gradient",
      "symbol": "üìà",
      "level": "advanced",
      "definition": {
        "text": "Policy gradient methods directly optimize the policy œÄ(a|s) by gradient ascent on expected return. Unlike value-based methods, they parameterize the policy (often as a neural network) and update parameters to increase the probability of actions that led to high rewards. The policy gradient theorem provides the gradient formula.",
        "keyTerms": ["Policy Parameterization", "Policy Gradient Theorem", "REINFORCE", "Baseline", "Advantage", "Actor-Critic"]
      },
      "keyFormulas": [
        {
          "id": "policy_grad",
          "name": "Policy Gradient",
          "formula": "‚àáJ(Œ∏) = E[‚àálog œÄ(a|s) ¬∑ Q(s,a)]",
          "latex": "\\nabla J(\\theta) = \\mathbb{E}\\left[ \\nabla \\log \\pi_\\theta(a|s) \\cdot Q^\\pi(s,a) \\right]",
          "meaning": "Increase prob of high-value actions"
        }
      ],
      "examples": [
        {
          "id": "pg_ex1",
          "question": "Why use a baseline in policy gradient?",
          "steps": [
            {"step": 1, "action": "Vanilla REINFORCE", "result": "High variance in gradients", "explanation": "All rewards could be positive"},
            {"step": 2, "action": "Add baseline b(s)", "result": "Q(s,a) ‚Üí Q(s,a) - b(s)", "explanation": "Doesn't bias gradient"},
            {"step": 3, "action": "Effect", "result": "Reduce variance, faster learning", "explanation": "V(s) is common baseline"}
          ],
          "finalAnswer": "Baseline reduces variance without biasing the gradient estimate",
          "difficulty": "hard"
        }
      ],
      "realWorldApplications": [
        {"title": "Continuous Control", "description": "Robot arm manipulation"},
        {"title": "LLM Fine-tuning", "description": "RLHF for language models"},
        {"title": "Game Playing", "description": "AlphaGo's policy network"}
      ],
      "codeExample": {
        "python": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gymnasium as gym\n\nclass PolicyNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(state_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_dim),\n            nn.Softmax(dim=-1)\n        )\n    \n    def forward(self, x):\n        return self.fc(x)\n\ndef reinforce(env, policy, episodes=1000, gamma=0.99):\n    optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n    \n    for episode in range(episodes):\n        states, actions, rewards = [], [], []\n        state, _ = env.reset()\n        done = False\n        \n        # Collect episode\n        while not done:\n            state_tensor = torch.FloatTensor(state)\n            probs = policy(state_tensor)\n            action = torch.multinomial(probs, 1).item()\n            \n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            \n            states.append(state_tensor)\n            actions.append(action)\n            rewards.append(reward)\n            state = next_state\n        \n        # Compute returns\n        returns = []\n        G = 0\n        for r in reversed(rewards):\n            G = r + gamma * G\n            returns.insert(0, G)\n        returns = torch.FloatTensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + 1e-8)  # Normalize\n        \n        # Policy gradient update\n        loss = 0\n        for state, action, G in zip(states, actions, returns):\n            probs = policy(state)\n            log_prob = torch.log(probs[action])\n            loss -= log_prob * G\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()"
      },
      "tips": [
        "REINFORCE is simplest policy gradient; high variance",
        "Actor-Critic uses value function as baseline",
        "PPO and A2C are practical policy gradient algorithms"
      ]
    },
    {
      "id": "ppo",
      "title": "PPO",
      "symbol": "üî•",
      "level": "advanced",
      "definition": {
        "text": "Proximal Policy Optimization (PPO) is a practical policy gradient algorithm that prevents destructively large policy updates. It uses a clipped surrogate objective that restricts how much the policy can change in one update. PPO achieves TRPO's stability benefits with simpler implementation and is the default RL algorithm for many applications.",
        "keyTerms": ["Clipped Objective", "Surrogate Loss", "Trust Region", "KL Divergence", "Advantage Estimation", "GAE"]
      },
      "keyFormulas": [
        {
          "id": "ppo_clip",
          "name": "PPO Clipped Objective",
          "formula": "L = min(r(Œ∏)A, clip(r(Œ∏), 1-Œµ, 1+Œµ)A)",
          "latex": "L^{CLIP} = \\min\\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t \\right)",
          "meaning": "r(Œ∏) = œÄ_new/œÄ_old, Œµ‚âà0.2"
        }
      ],
      "examples": [
        {
          "id": "ppo_ex1",
          "question": "Why clip the probability ratio?",
          "steps": [
            {"step": 1, "action": "Without clipping", "result": "Large policy changes possible", "explanation": "Can diverge"},
            {"step": 2, "action": "Clipping", "result": "Limits update when r(Œ∏) > 1+Œµ or < 1-Œµ", "explanation": "Conservative updates"},
            {"step": 3, "action": "Effect", "result": "Stable training, similar to TRPO", "explanation": "But simpler to implement"}
          ],
          "finalAnswer": "Clipping prevents destructively large policy updates, ensuring stable training",
          "difficulty": "hard"
        }
      ],
      "realWorldApplications": [
        {"title": "ChatGPT/RLHF", "description": "Fine-tuning LLMs with human feedback"},
        {"title": "Robotics", "description": "Learning complex motor skills"},
        {"title": "Game AI", "description": "OpenAI Five, Dota 2"}
      ],
      "codeExample": {
        "python": "import torch\nimport torch.nn as nn\nfrom torch.distributions import Categorical\n\nclass PPO:\n    def __init__(self, actor, critic, lr=3e-4, clip_eps=0.2, gamma=0.99):\n        self.actor = actor\n        self.critic = critic\n        self.optimizer = torch.optim.Adam(\n            list(actor.parameters()) + list(critic.parameters()), lr=lr\n        )\n        self.clip_eps = clip_eps\n        self.gamma = gamma\n    \n    def compute_ppo_loss(self, states, actions, old_log_probs, returns, advantages):\n        # Get current policy\n        probs = self.actor(states)\n        dist = Categorical(probs)\n        new_log_probs = dist.log_prob(actions)\n        \n        # Probability ratio\n        ratio = torch.exp(new_log_probs - old_log_probs)\n        \n        # Clipped surrogate objective\n        surr1 = ratio * advantages\n        surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantages\n        actor_loss = -torch.min(surr1, surr2).mean()\n        \n        # Value loss\n        values = self.critic(states).squeeze()\n        critic_loss = nn.MSELoss()(values, returns)\n        \n        # Entropy bonus for exploration\n        entropy = dist.entropy().mean()\n        \n        return actor_loss + 0.5 * critic_loss - 0.01 * entropy\n    \n    def update(self, trajectories, epochs=10):\n        \"\"\"Run multiple epochs on collected data\"\"\"\n        for _ in range(epochs):\n            loss = self.compute_ppo_loss(**trajectories)\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()"
      },
      "tips": [
        "PPO-Clip (Œµ=0.2) is most common variant",
        "Run multiple epochs on same data (unlike vanilla PG)",
        "Generalized Advantage Estimation (GAE) improves returns estimation"
      ]
    }
  ]
}

