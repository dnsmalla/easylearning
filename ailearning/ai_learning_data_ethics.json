{
  "category": "AI Ethics & Safety",
  "categoryId": "ethics",
  "version": "1.0.0",
  "description": "Bias, fairness, safety, alignment, and responsible AI",
  "icon": "shield.checkered",
  "color": "#64748B",
  "topics": [
    {
      "id": "bias_in_ml",
      "title": "Bias in ML",
      "symbol": "‚ö†Ô∏è",
      "level": "beginner",
      "definition": {
        "text": "ML bias occurs when models systematically produce unfair outcomes for certain groups. Sources include biased training data (reflecting historical discrimination), biased labels (from biased annotators), selection bias (non-representative samples), and algorithmic bias (model architecture choices). Bias can amplify societal inequalities at scale.",
        "keyTerms": ["Training Data Bias", "Selection Bias", "Confirmation Bias", "Measurement Bias", "Aggregation Bias", "Representation Bias"]
      },
      "keyFormulas": [
        {
          "id": "demographic_parity",
          "name": "Demographic Parity",
          "formula": "P(≈∂=1|A=0) = P(≈∂=1|A=1)",
          "latex": "P(\\hat{Y}=1|A=0) = P(\\hat{Y}=1|A=1)",
          "meaning": "Equal positive rates across groups"
        }
      ],
      "examples": [
        {
          "id": "bias_ex1",
          "question": "How did Amazon's hiring AI become biased?",
          "steps": [
            {"step": 1, "action": "Training data", "result": "10 years of resumes, mostly male hires", "explanation": "Reflected past hiring"},
            {"step": 2, "action": "Pattern learned", "result": "Downgraded 'women's' keywords", "explanation": "'women's chess club' penalized"},
            {"step": 3, "action": "Outcome", "result": "System favored male candidates", "explanation": "Amplified historical bias"}
          ],
          "finalAnswer": "Biased historical data led to discriminatory predictions against women",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Hiring Systems", "description": "Resume screening, interview scheduling"},
        {"title": "Credit Scoring", "description": "Loan approval algorithms"},
        {"title": "Criminal Justice", "description": "Risk assessment tools like COMPAS"}
      ],
      "codeExample": {
        "python": "import pandas as pd\nfrom sklearn.metrics import confusion_matrix\n\ndef check_bias(y_true, y_pred, sensitive_attribute):\n    \"\"\"\n    Check for bias in predictions across groups\n    \"\"\"\n    results = {}\n    \n    for group in sensitive_attribute.unique():\n        mask = sensitive_attribute == group\n        y_true_group = y_true[mask]\n        y_pred_group = y_pred[mask]\n        \n        # Positive rate for this group\n        positive_rate = y_pred_group.mean()\n        \n        # True positive rate (if ground truth available)\n        tn, fp, fn, tp = confusion_matrix(y_true_group, y_pred_group).ravel()\n        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n        \n        results[group] = {\n            'positive_rate': positive_rate,\n            'true_positive_rate': tpr,\n            'false_positive_rate': fpr\n        }\n    \n    # Check demographic parity\n    rates = [r['positive_rate'] for r in results.values()]\n    disparity = max(rates) - min(rates)\n    \n    print(f\"Demographic parity gap: {disparity:.3f}\")\n    if disparity > 0.1:\n        print(\"‚ö†Ô∏è Significant bias detected!\")\n    \n    return results"
      },
      "tips": [
        "Always disaggregate metrics by demographic groups",
        "Bias can exist even with protected attributes removed (proxy variables)",
        "Prevention > detection > mitigation in order of effectiveness"
      ]
    },
    {
      "id": "fairness_metrics",
      "title": "Fairness Metrics",
      "symbol": "‚öñÔ∏è",
      "level": "intermediate",
      "definition": {
        "text": "Fairness metrics quantify how equitably a model treats different groups. Key metrics include demographic parity (equal positive rates), equalized odds (equal TPR and FPR), and calibration (equal accuracy of probability estimates). Different metrics capture different notions of fairness, and some are mathematically incompatible.",
        "keyTerms": ["Demographic Parity", "Equalized Odds", "Calibration", "Individual Fairness", "Group Fairness", "Counterfactual Fairness"]
      },
      "keyFormulas": [
        {
          "id": "equalized_odds",
          "name": "Equalized Odds",
          "formula": "P(≈∂=1|Y=y,A=0) = P(≈∂=1|Y=y,A=1) for y‚àà{0,1}",
          "latex": "P(\\hat{Y}=1|Y=y,A=0) = P(\\hat{Y}=1|Y=y,A=1)",
          "meaning": "Equal TPR and FPR across groups"
        }
      ],
      "examples": [
        {
          "id": "fair_ex1",
          "question": "Why can't we achieve all fairness metrics simultaneously?",
          "steps": [
            {"step": 1, "action": "Base rate differs", "result": "Groups have different Y=1 rates", "explanation": "Real world disparity"},
            {"step": 2, "action": "Calibration", "result": "Requires P(Y=1|≈∂=p) = p for all groups", "explanation": "Accurate probabilities"},
            {"step": 3, "action": "Impossibility", "result": "Perfect calibration + equal FPR ‚Üí unequal TPR", "explanation": "Math constraint"}
          ],
          "finalAnswer": "With different base rates, calibration and equal error rates are mathematically incompatible",
          "difficulty": "hard"
        }
      ],
      "realWorldApplications": [
        {"title": "Loan Decisions", "description": "Equal opportunity for creditworthy applicants"},
        {"title": "Medical Diagnosis", "description": "Equalized odds across demographics"},
        {"title": "Hiring", "description": "Demographic parity in interview rates"}
      ],
      "codeExample": {
        "python": "from fairlearn.metrics import (\n    demographic_parity_difference,\n    equalized_odds_difference,\n    MetricFrame\n)\nfrom sklearn.metrics import accuracy_score, recall_score\n\ndef evaluate_fairness(y_true, y_pred, sensitive_feature):\n    \"\"\"\n    Comprehensive fairness evaluation\n    \"\"\"\n    # Create MetricFrame for disaggregated metrics\n    metrics = {\n        'accuracy': accuracy_score,\n        'recall': recall_score,\n    }\n    \n    metric_frame = MetricFrame(\n        metrics=metrics,\n        y_true=y_true,\n        y_pred=y_pred,\n        sensitive_features=sensitive_feature\n    )\n    \n    print(\"Metrics by group:\")\n    print(metric_frame.by_group)\n    print(f\"\\nMax difference in accuracy: {metric_frame.difference()['accuracy']:.3f}\")\n    \n    # Fairness metrics\n    dp_diff = demographic_parity_difference(\n        y_true, y_pred, sensitive_features=sensitive_feature\n    )\n    eo_diff = equalized_odds_difference(\n        y_true, y_pred, sensitive_features=sensitive_feature\n    )\n    \n    print(f\"\\nDemographic parity difference: {dp_diff:.3f}\")\n    print(f\"Equalized odds difference: {eo_diff:.3f}\")\n    \n    # Thresholds (common guidelines)\n    if abs(dp_diff) > 0.1:\n        print(\"‚ö†Ô∏è Demographic parity violation (>10%)\")\n    if abs(eo_diff) > 0.1:\n        print(\"‚ö†Ô∏è Equalized odds violation (>10%)\")"
      },
      "tips": [
        "Choose fairness metric based on context and stakeholder values",
        "80% rule (disparate impact): positive rate ratio > 0.8",
        "Fairlearn and AIF360 are leading fairness toolkits"
      ]
    },
    {
      "id": "explainability",
      "title": "Explainability",
      "symbol": "üîç",
      "level": "intermediate",
      "definition": {
        "text": "Explainability (XAI) makes ML model decisions understandable to humans. Methods include feature importance (which inputs mattered), local explanations (why this specific prediction), and global explanations (how the model generally behaves). Explainability is crucial for trust, debugging, and regulatory compliance (GDPR's 'right to explanation').",
        "keyTerms": ["SHAP", "LIME", "Feature Importance", "Attention Visualization", "Saliency Maps", "Counterfactual Explanations"]
      },
      "keyFormulas": [
        {
          "id": "shapley",
          "name": "Shapley Value",
          "formula": "œÜ·µ¢ = Œ£ [f(S‚à™{i}) - f(S)] √ó weight",
          "latex": "\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(n-|S|-1)!}{n!} [f(S \\cup \\{i\\}) - f(S)]",
          "meaning": "Fair attribution of feature contribution"
        }
      ],
      "examples": [
        {
          "id": "xai_ex1",
          "question": "What's the difference between SHAP and LIME?",
          "steps": [
            {"step": 1, "action": "SHAP", "result": "Game-theoretic, global consistency", "explanation": "Based on Shapley values"},
            {"step": 2, "action": "LIME", "result": "Local linear approximation", "explanation": "Fits simple model around prediction"},
            {"step": 3, "action": "Tradeoff", "result": "SHAP is slower but theoretically grounded", "explanation": "LIME is faster, less consistent"}
          ],
          "finalAnswer": "SHAP uses Shapley values (consistent); LIME uses local linear models (faster)",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Healthcare", "description": "Why model recommended this treatment"},
        {"title": "Finance", "description": "Explaining credit decisions"},
        {"title": "Debugging", "description": "Finding why model fails on certain inputs"}
      ],
      "codeExample": {
        "python": "import shap\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train model\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n\n# SHAP explanations\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\n# Global feature importance\nshap.summary_plot(shap_values[1], X_test, feature_names=feature_names)\n\n# Local explanation for single prediction\nidx = 0  # Explain first test sample\nshap.force_plot(\n    explainer.expected_value[1],\n    shap_values[1][idx],\n    X_test.iloc[idx],\n    feature_names=feature_names\n)\n\n# Waterfall plot for single prediction\nshap.plots.waterfall(shap.Explanation(\n    values=shap_values[1][idx],\n    base_values=explainer.expected_value[1],\n    data=X_test.iloc[idx],\n    feature_names=feature_names\n))"
      },
      "tips": [
        "SHAP is model-agnostic but has optimized versions for trees/deep learning",
        "Attention weights don't always reflect true feature importance",
        "Counterfactual explanations: 'What would change the decision?'"
      ]
    },
    {
      "id": "alignment",
      "title": "AI Alignment",
      "symbol": "üéØ",
      "level": "advanced",
      "definition": {
        "text": "AI alignment ensures AI systems pursue goals beneficial to humans. The core challenge: specifying human values precisely is hard, and powerful AI might find unexpected ways to achieve poorly-specified objectives. Key concepts include Goodhart's law (optimizing a measure corrupts it), reward hacking, and specification gaming.",
        "keyTerms": ["Value Alignment", "Reward Hacking", "Specification Gaming", "Instrumental Convergence", "Corrigibility", "Inner Alignment", "Outer Alignment"]
      },
      "keyFormulas": [
        {
          "id": "goodhart",
          "name": "Goodhart's Law",
          "formula": "When a measure becomes a target, it ceases to be a good measure",
          "latex": "\\text{Optimize}(\\text{proxy}) \\neq \\text{Optimize}(\\text{true goal})",
          "meaning": "Metrics get gamed when optimized"
        }
      ],
      "examples": [
        {
          "id": "align_ex1",
          "question": "What is reward hacking in RL?",
          "steps": [
            {"step": 1, "action": "Intended task", "result": "Clean a room", "explanation": "Human's goal"},
            {"step": 2, "action": "Specified reward", "result": "+1 for each piece of trash in bin", "explanation": "Proxy measure"},
            {"step": 3, "action": "Hacked solution", "result": "Put trash in bin, take out, repeat", "explanation": "Infinite reward, room not clean"}
          ],
          "finalAnswer": "Agent exploits reward specification to get high reward without achieving intended goal",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "RLHF", "description": "Aligning LLMs with human preferences"},
        {"title": "Autonomous Systems", "description": "Ensuring safe behavior"},
        {"title": "AGI Safety", "description": "Long-term alignment research"}
      ],
      "codeExample": {
        "python": "# Conceptual example: Reward hacking in a simple environment\n\nclass MisalignedEnvironment:\n    \"\"\"Environment where proxy reward ‚â† true objective\"\"\"\n    \n    def __init__(self):\n        self.true_objective = 0  # What we actually want\n        self.proxy_reward = 0    # What agent optimizes\n        self.state = 'start'\n    \n    def step(self, action):\n        if action == 'hack':\n            # Agent found exploit: high proxy reward, low true value\n            self.proxy_reward += 100\n            self.true_objective -= 10\n            return self.proxy_reward, {'hacked': True}\n        \n        elif action == 'intended':\n            # Intended behavior: moderate reward, high true value\n            self.proxy_reward += 10\n            self.true_objective += 50\n            return self.proxy_reward, {'hacked': False}\n    \n    def evaluate_alignment(self):\n        \"\"\"Check if optimizing proxy aligned with true goal\"\"\"\n        alignment_gap = self.proxy_reward - self.true_objective\n        print(f\"Proxy reward: {self.proxy_reward}\")\n        print(f\"True objective: {self.true_objective}\")\n        print(f\"Alignment gap: {alignment_gap}\")\n        \n        if alignment_gap > 50:\n            print(\"‚ö†Ô∏è Severe misalignment detected!\")\n\n# Lesson: Reward specification is hard!\n# Solutions: RLHF, reward modeling, debate, amplification"
      },
      "tips": [
        "RLHF learns rewards from human preferences, not hand-coded",
        "Constitutional AI uses self-critique for alignment",
        "Corrigibility: AI should allow itself to be corrected"
      ]
    },
    {
      "id": "responsible_ai",
      "title": "Responsible AI",
      "symbol": "‚úÖ",
      "level": "beginner",
      "definition": {
        "text": "Responsible AI is the practice of developing and deploying AI systems ethically and sustainably. It encompasses fairness, accountability, transparency, privacy, safety, and environmental impact. Organizations need governance frameworks, documentation practices (model cards, datasheets), and ongoing monitoring to ensure responsible AI.",
        "keyTerms": ["Model Cards", "Datasheets", "AI Governance", "Accountability", "Transparency", "Privacy", "Sustainability"]
      },
      "keyFormulas": [
        {
          "id": "carbon_footprint",
          "name": "Training Carbon Footprint",
          "formula": "CO‚ÇÇ = Power √ó Time √ó Carbon Intensity",
          "latex": "CO_2 = P \\times t \\times CI",
          "meaning": "Environmental impact of training"
        }
      ],
      "examples": [
        {
          "id": "resp_ex1",
          "question": "What should a Model Card include?",
          "steps": [
            {"step": 1, "action": "Model details", "result": "Architecture, training data, intended use", "explanation": "What it is"},
            {"step": 2, "action": "Performance", "result": "Metrics across demographic groups", "explanation": "How well it works"},
            {"step": 3, "action": "Limitations", "result": "Known failures, biases, out-of-scope uses", "explanation": "What it can't do"}
          ],
          "finalAnswer": "Model Cards document model details, performance, limitations, and ethical considerations",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Regulatory Compliance", "description": "EU AI Act, GDPR requirements"},
        {"title": "Enterprise AI", "description": "Internal governance and auditing"},
        {"title": "Open Source", "description": "Hugging Face model cards"}
      ],
      "codeExample": {
        "python": "# Model Card Template (simplified)\n\nMODEL_CARD = \"\"\"\n# Model Card: [Model Name]\n\n## Model Details\n- **Developed by:** [Organization]\n- **Model type:** [e.g., BERT-based text classifier]\n- **Language:** [e.g., English]\n- **License:** [e.g., MIT, Apache 2.0]\n\n## Intended Use\n- **Primary use:** [e.g., Sentiment analysis for product reviews]\n- **Out-of-scope:** [e.g., Medical advice, legal decisions]\n\n## Training Data\n- **Dataset:** [e.g., Amazon reviews, 1M samples]\n- **Data collection:** [How data was gathered]\n- **Known biases:** [e.g., Skews toward US English speakers]\n\n## Evaluation\n| Metric | Overall | Male | Female | Unknown |\n|--------|---------|------|--------|----------|\n| Accuracy | 92% | 93% | 91% | 90% |\n| F1 | 0.91 | 0.92 | 0.90 | 0.89 |\n\n## Limitations\n- May underperform on informal text (slang, emojis)\n- Not tested on non-English text\n- Trained on 2020 data; may not reflect current trends\n\n## Ethical Considerations\n- Could be used for opinion manipulation\n- Privacy: Ensure no PII in input text\n\n## Carbon Footprint\n- Training: ~50 kg CO‚ÇÇ equivalent\n- Hardware: 4x V100 GPUs for 24 hours\n\"\"\"\n\ndef generate_model_card(model_info):\n    \"\"\"Generate model card from structured info\"\"\"\n    # In practice, use Hugging Face's model card toolkit\n    return MODEL_CARD.format(**model_info)"
      },
      "tips": [
        "Document before deploying; it's harder to add later",
        "Hugging Face Hub requires model cards for hosted models",
        "EU AI Act mandates documentation for high-risk AI systems"
      ]
    }
  ]
}

