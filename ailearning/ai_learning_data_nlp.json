{
  "category": "Natural Language Processing",
  "categoryId": "nlp",
  "version": "1.0.0",
  "description": "Text processing, language models, and text generation",
  "icon": "text.bubble",
  "color": "#14B8A6",
  "topics": [
    {
      "id": "tokenization",
      "title": "Tokenization",
      "symbol": "‚úÇÔ∏è",
      "level": "beginner",
      "definition": {
        "text": "Tokenization is the process of splitting text into smaller units called tokens. Tokens can be words, subwords, or characters. Modern LLMs use subword tokenization (BPE, WordPiece) which balances vocabulary size with the ability to handle unknown words by breaking them into known subword pieces.",
        "keyTerms": ["Token", "Subword", "BPE", "WordPiece", "SentencePiece", "Vocabulary", "OOV (Out of Vocabulary)"]
      },
      "keyFormulas": [
        {
          "id": "vocab_size",
          "name": "Vocabulary Impact",
          "formula": "Memory ‚àù vocab_size √ó embedding_dim",
          "latex": "\\text{Memory} \\propto V \\times d",
          "meaning": "Larger vocabulary = more parameters"
        }
      ],
      "examples": [
        {
          "id": "tok_ex1",
          "question": "How does BPE tokenize 'unhappiness'?",
          "steps": [
            {"step": 1, "action": "Start with characters", "result": "u n h a p p i n e s s", "explanation": "Base units"},
            {"step": 2, "action": "Merge frequent pairs", "result": "un happ in ess", "explanation": "Common subwords"},
            {"step": 3, "action": "Final tokens", "result": "['un', 'happiness'] or ['un', 'happy', 'ness']", "explanation": "Depends on training"}
          ],
          "finalAnswer": "BPE breaks unknown words into known subword pieces",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "LLM Input Processing", "description": "Converting text to token IDs for GPT, BERT"},
        {"title": "Multilingual Models", "description": "Subword tokenization handles multiple languages"}
      ],
      "codeExample": {
        "python": "from transformers import AutoTokenizer\n\n# Load GPT-2 tokenizer (BPE-based)\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\ntext = \"Machine learning is fascinating!\"\n\n# Tokenize\ntokens = tokenizer.tokenize(text)\nprint(f\"Tokens: {tokens}\")\n# ['Machine', 'ƒ†learning', 'ƒ†is', 'ƒ†fascinating', '!']\n\n# Convert to IDs\ntoken_ids = tokenizer.encode(text)\nprint(f\"Token IDs: {token_ids}\")\n# [22137, 4673, 318, 21338, 0]\n\n# Decode back\ndecoded = tokenizer.decode(token_ids)\nprint(f\"Decoded: {decoded}\")\n# \"Machine learning is fascinating!\""
      },
      "tips": [
        "ƒ† symbol in GPT tokenizers represents a space before the token",
        "Subword tokenization handles rare/unknown words gracefully",
        "tiktoken is OpenAI's fast tokenizer for GPT models"
      ]
    },
    {
      "id": "word_embeddings",
      "title": "Word Embeddings",
      "symbol": "üî¢",
      "level": "intermediate",
      "definition": {
        "text": "Word embeddings are dense vector representations of words in a continuous vector space. Unlike one-hot encoding, embeddings capture semantic relationships - similar words have similar vectors. The key insight: words appearing in similar contexts have similar meanings. Embeddings are learned from large text corpora.",
        "keyTerms": ["Embedding", "Vector Space", "Semantic Similarity", "Cosine Similarity", "Embedding Dimension", "Pretrained Embeddings"]
      },
      "keyFormulas": [
        {
          "id": "cosine_sim",
          "name": "Cosine Similarity",
          "formula": "cos(Œ∏) = (A¬∑B)/(||A|| ||B||)",
          "latex": "\\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}",
          "meaning": "Measures angle between word vectors"
        }
      ],
      "examples": [
        {
          "id": "embed_ex1",
          "question": "Why does king - man + woman ‚âà queen work?",
          "steps": [
            {"step": 1, "action": "Vector arithmetic", "result": "Embeddings encode relationships", "explanation": "Gender encoded as direction"},
            {"step": 2, "action": "king - man", "result": "Removes 'male' component", "explanation": "Leaves royalty concept"},
            {"step": 3, "action": "+ woman", "result": "Adds 'female' component", "explanation": "Female royalty = queen"}
          ],
          "finalAnswer": "Embeddings encode semantic relationships as vector directions",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Semantic Search", "description": "Find similar documents by embedding similarity"},
        {"title": "Recommendation Systems", "description": "Content-based filtering using embeddings"}
      ],
      "codeExample": {
        "python": "import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Simulated word embeddings (normally from pretrained model)\nembeddings = {\n    'king': np.array([0.9, 0.1, 0.8]),\n    'queen': np.array([0.85, 0.9, 0.8]),\n    'man': np.array([0.9, 0.1, 0.1]),\n    'woman': np.array([0.85, 0.9, 0.1]),\n}\n\n# Vector arithmetic: king - man + woman\nresult = embeddings['king'] - embeddings['man'] + embeddings['woman']\nprint(f\"king - man + woman = {result}\")\n\n# Find closest word\nfor word, vec in embeddings.items():\n    sim = cosine_similarity([result], [vec])[0][0]\n    print(f\"{word}: {sim:.3f}\")\n# queen should have highest similarity"
      },
      "tips": [
        "Typical embedding dimensions: 100-300 (Word2Vec) to 768+ (BERT)",
        "Pretrained embeddings (GloVe, FastText) save training time",
        "Contextual embeddings (BERT) give different vectors based on context"
      ]
    },
    {
      "id": "word2vec",
      "title": "Word2Vec",
      "symbol": "üìù",
      "level": "intermediate",
      "definition": {
        "text": "Word2Vec is a neural network technique for learning word embeddings. It has two architectures: CBOW (predict center word from context) and Skip-gram (predict context words from center). The key insight is that words appearing in similar contexts have similar meanings. Training is efficient using negative sampling.",
        "keyTerms": ["CBOW", "Skip-gram", "Negative Sampling", "Context Window", "Hierarchical Softmax", "Distributed Representations"]
      },
      "keyFormulas": [
        {
          "id": "skipgram",
          "name": "Skip-gram Objective",
          "formula": "maximize Œ£ log P(context | center)",
          "latex": "\\max \\sum_{t} \\sum_{-c \\leq j \\leq c} \\log P(w_{t+j} | w_t)",
          "meaning": "Predict context from center word"
        }
      ],
      "examples": [
        {
          "id": "w2v_ex1",
          "question": "What's the difference between CBOW and Skip-gram?",
          "steps": [
            {"step": 1, "action": "CBOW", "result": "Context ‚Üí Center word", "explanation": "Faster, good for frequent words"},
            {"step": 2, "action": "Skip-gram", "result": "Center ‚Üí Context words", "explanation": "Better for rare words"},
            {"step": 3, "action": "Training data", "result": "Skip-gram creates more examples", "explanation": "One center ‚Üí multiple context"}
          ],
          "finalAnswer": "Skip-gram predicts context from word; CBOW predicts word from context",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Text Classification", "description": "Word embeddings as input features"},
        {"title": "Analogies", "description": "Discovering semantic relationships"}
      ],
      "codeExample": {
        "python": "from gensim.models import Word2Vec\n\n# Sample corpus\nsentences = [\n    ['machine', 'learning', 'is', 'powerful'],\n    ['deep', 'learning', 'uses', 'neural', 'networks'],\n    ['neural', 'networks', 'learn', 'representations'],\n    ['machine', 'learning', 'applications', 'are', 'everywhere'],\n]\n\n# Train Word2Vec model\nmodel = Word2Vec(\n    sentences,\n    vector_size=100,  # Embedding dimension\n    window=5,         # Context window size\n    min_count=1,      # Minimum word frequency\n    sg=1,             # Skip-gram (0 for CBOW)\n    epochs=100\n)\n\n# Get word vector\nvector = model.wv['learning']\nprint(f\"'learning' vector shape: {vector.shape}\")\n\n# Find similar words\nsimilar = model.wv.most_similar('learning', topn=3)\nprint(f\"Similar to 'learning': {similar}\")"
      },
      "tips": [
        "Skip-gram with negative sampling (SGNS) is most common",
        "Window size 5-10 is typical; larger captures broader context",
        "Pre-trained Word2Vec on Google News has 3M words"
      ]
    },
    {
      "id": "tfidf",
      "title": "TF-IDF",
      "symbol": "üìä",
      "level": "beginner",
      "definition": {
        "text": "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic reflecting how important a word is to a document in a corpus. TF measures how often a term appears in a document, while IDF measures how rare it is across all documents. Words that are frequent in a document but rare overall get high scores.",
        "keyTerms": ["Term Frequency", "Inverse Document Frequency", "Document-Term Matrix", "Sparse Representation", "Bag of Words"]
      },
      "keyFormulas": [
        {
          "id": "tfidf_formula",
          "name": "TF-IDF Score",
          "formula": "TF-IDF = TF(t,d) √ó IDF(t)",
          "latex": "\\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\log\\frac{N}{\\text{DF}(t)}",
          "meaning": "N=total docs, DF=docs containing term"
        }
      ],
      "examples": [
        {
          "id": "tfidf_ex1",
          "question": "Calculate TF-IDF for 'neural' in a document",
          "steps": [
            {"step": 1, "action": "Count TF", "result": "TF = 5/100 = 0.05", "explanation": "'neural' appears 5 times in 100-word doc"},
            {"step": 2, "action": "Calculate IDF", "result": "IDF = log(1000/50) = 1.3", "explanation": "50 of 1000 docs contain 'neural'"},
            {"step": 3, "action": "Multiply", "result": "TF-IDF = 0.05 √ó 1.3 = 0.065", "explanation": "Final score"}
          ],
          "finalAnswer": "TF-IDF = 0.065 (moderately important term)",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Search Engines", "description": "Ranking documents by relevance"},
        {"title": "Keyword Extraction", "description": "Finding important terms in documents"}
      ],
      "codeExample": {
        "python": "from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Sample documents\ndocuments = [\n    \"Machine learning is a subset of artificial intelligence\",\n    \"Deep learning uses neural networks\",\n    \"Neural networks are inspired by the brain\",\n    \"Artificial intelligence is transforming industries\"\n]\n\n# Create TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(documents)\n\n# Get feature names (vocabulary)\nfeature_names = vectorizer.get_feature_names_out()\n\n# Show TF-IDF scores for first document\nimport pandas as pd\ndf = pd.DataFrame(\n    tfidf_matrix[0].toarray(),\n    columns=feature_names\n).T\ndf.columns = ['TF-IDF']\nprint(df[df['TF-IDF'] > 0].sort_values('TF-IDF', ascending=False))"
      },
      "tips": [
        "Add 1 to IDF denominator to avoid division by zero (smoothing)",
        "TF-IDF vectors are sparse - most entries are zero",
        "Works best for keyword-based retrieval, less for semantic search"
      ]
    },
    {
      "id": "seq2seq",
      "title": "Seq2Seq Models",
      "symbol": "‚ÜîÔ∏è",
      "level": "intermediate",
      "definition": {
        "text": "Sequence-to-Sequence models map input sequences to output sequences of potentially different lengths. The encoder processes the input into a context vector, and the decoder generates the output autoregressively. Originally used RNNs/LSTMs; modern versions use Transformers. The attention mechanism was introduced to address the bottleneck of fixed-size context vectors.",
        "keyTerms": ["Encoder", "Decoder", "Context Vector", "Autoregressive", "Teacher Forcing", "Beam Search"]
      },
      "keyFormulas": [
        {
          "id": "seq2seq_prob",
          "name": "Conditional Probability",
          "formula": "P(y|x) = Œ† P(y‚Çú|y<‚Çú, x)",
          "latex": "P(\\mathbf{y}|\\mathbf{x}) = \\prod_{t=1}^{T} P(y_t | y_{<t}, \\mathbf{x})",
          "meaning": "Generate one token at a time"
        }
      ],
      "examples": [
        {
          "id": "s2s_ex1",
          "question": "What is the bottleneck problem in vanilla Seq2Seq?",
          "steps": [
            {"step": 1, "action": "Encoder output", "result": "Fixed-size context vector", "explanation": "All input compressed"},
            {"step": 2, "action": "Long sequences", "result": "Information loss", "explanation": "Can't retain all details"},
            {"step": 3, "action": "Solution", "result": "Attention mechanism", "explanation": "Access all encoder states"}
          ],
          "finalAnswer": "Fixed context vector causes information bottleneck; attention solves this",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Machine Translation", "description": "English to French, etc."},
        {"title": "Text Summarization", "description": "Long document ‚Üí short summary"},
        {"title": "Chatbots", "description": "User query ‚Üí response"}
      ],
      "codeExample": {
        "python": "import torch\nimport torch.nn as nn\n\nclass Seq2SeqEncoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        outputs, (hidden, cell) = self.lstm(embedded)\n        return outputs, hidden, cell\n\nclass Seq2SeqDecoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        \n    def forward(self, x, hidden, cell):\n        embedded = self.embedding(x)\n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        prediction = self.fc(output)\n        return prediction, hidden, cell\n\n# Usage: encoder encodes input, decoder generates output token by token"
      },
      "tips": [
        "Teacher forcing: use ground truth as decoder input during training",
        "Beam search at inference for better output quality",
        "Modern Seq2Seq uses Transformer encoder-decoder (T5, BART)"
      ]
    }
  ]
}

