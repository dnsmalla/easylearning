{
  "category": "Large Language Models",
  "categoryId": "llms",
  "version": "1.0.0",
  "description": "GPT, Claude, prompting, fine-tuning, and RLHF",
  "icon": "sparkles",
  "color": "#10B981",
  "topics": [
    {
      "id": "what_are_llms",
      "title": "What are LLMs?",
      "symbol": "ü§ñ",
      "level": "beginner",
      "definition": {
        "text": "Large Language Models (LLMs) are neural networks with billions of parameters trained on vast amounts of text data. They predict the next token in a sequence, learning language patterns, facts, and reasoning abilities. Examples include GPT-4, Claude, Llama, and Gemini. LLMs can perform translation, summarization, coding, and general-purpose reasoning.",
        "keyTerms": ["Transformer", "Token", "Parameters", "Pre-training", "Prompt", "Context Window", "Autoregressive", "Foundation Model"]
      },
      "keyFormulas": [
        {
          "id": "next_token",
          "name": "Next Token Prediction",
          "formula": "P(x‚Çú | x‚ÇÅ, x‚ÇÇ, ..., x‚Çú‚Çã‚ÇÅ)",
          "latex": "P(x_t | x_1, x_2, \\ldots, x_{t-1})",
          "meaning": "Probability of next token given previous tokens"
        }
      ],
      "examples": [
        {
          "id": "llm_ex1",
          "question": "Why do LLMs seem to 'understand' despite just predicting next tokens?",
          "steps": [
            {"step": 1, "action": "Compression hypothesis", "result": "To predict well, must understand", "explanation": "Predicting requires knowledge"},
            {"step": 2, "action": "Scale matters", "result": "Billions of parameters + trillions of tokens", "explanation": "Emergent abilities at scale"},
            {"step": 3, "action": "Generalization", "result": "Patterns learned transfer to new tasks", "explanation": "In-context learning emerges"}
          ],
          "finalAnswer": "Next token prediction at scale learns representations useful for reasoning",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "ChatGPT", "description": "Conversational AI assistant"},
        {"title": "GitHub Copilot", "description": "AI pair programmer"},
        {"title": "Content Generation", "description": "Writing, marketing, creative work"}
      ],
      "codeExample": {
        "python": "# Using OpenAI API\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n    ],\n    temperature=0.7,\n    max_tokens=150\n)\n\nprint(response.choices[0].message.content)\n\n# Using Hugging Face Transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"meta-llama/Llama-2-7b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\nprompt = \"Machine learning is\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0]))"
      },
      "tips": [
        "LLMs are trained on next token prediction but exhibit emergent abilities",
        "Parameter count: GPT-3 = 175B, GPT-4 = ~1.7T (rumored)",
        "Context window determines how much text the model can 'see'"
      ]
    },
    {
      "id": "prompt_engineering",
      "title": "Prompt Engineering",
      "symbol": "‚úèÔ∏è",
      "level": "beginner",
      "definition": {
        "text": "Prompt Engineering is the art of crafting effective prompts to get desired outputs from LLMs. Techniques include clear instructions, providing examples (few-shot), role-playing, chain-of-thought reasoning, and structured output formats. Good prompts dramatically improve model performance without any fine-tuning.",
        "keyTerms": ["Zero-shot", "Few-shot", "System Prompt", "User Prompt", "Chain of Thought", "Role-playing", "Temperature", "Output Formatting"]
      },
      "keyFormulas": [
        {
          "id": "prompt_structure",
          "name": "Effective Prompt Structure",
          "formula": "Context + Task + Format + Examples",
          "latex": "\\text{Context} + \\text{Task} + \\text{Format} + \\text{Examples}",
          "meaning": "Key components of a good prompt"
        }
      ],
      "examples": [
        {
          "id": "prompt_ex1",
          "question": "Convert a vague request into an effective prompt",
          "steps": [
            {"step": 1, "action": "Vague: 'Write about AI'", "result": "Too broad, unclear format", "explanation": "Needs specification"},
            {"step": 2, "action": "Add context", "result": "You are a tech journalist", "explanation": "Sets the role/perspective"},
            {"step": 3, "action": "Add specifics", "result": "Write a 200-word article about AI in healthcare for a general audience", "explanation": "Clear task with constraints"},
            {"step": 4, "action": "Add format", "result": "Include a catchy headline and 3 key takeaways at the end", "explanation": "Structured output"}
          ],
          "finalAnswer": "Role + specific task + constraints + output format = effective prompt",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Customer Service", "description": "Crafting prompts for support bots"},
        {"title": "Code Generation", "description": "Getting accurate code from Copilot"},
        {"title": "Content Creation", "description": "Consistent brand voice in generated text"}
      ],
      "codeExample": {
        "python": "# Prompt Engineering Examples\n\n# 1. Zero-shot (no examples)\nzero_shot = \"\"\"Classify the sentiment of this review as positive, negative, or neutral:\n\nReview: \"The product arrived quickly but the quality was disappointing.\"\nSentiment:\"\"\"\n\n# 2. Few-shot (with examples)\nfew_shot = \"\"\"Classify the sentiment:\n\nReview: \"Amazing product, exceeded expectations!\" ‚Üí Positive\nReview: \"Terrible, broke after one day\" ‚Üí Negative\nReview: \"It's okay, nothing special\" ‚Üí Neutral\n\nReview: \"The product arrived quickly but the quality was disappointing.\"\nSentiment:\"\"\"\n\n# 3. Chain-of-Thought\nchain_of_thought = \"\"\"Solve this step by step:\n\nIf a store has 50 apples and sells 23, then receives a shipment of 15 more,\nhow many apples do they have?\n\nLet me think through this step by step:\n1. Start with: 50 apples\n2. After selling 23: 50 - 23 = 27 apples\n3. After receiving 15: 27 + 15 = 42 apples\n\nFinal answer: 42 apples\n\nNow solve this:\nIf a bookstore has 120 books...\"\"\"\n\n# 4. Structured Output\nstructured = \"\"\"Extract information from this text in JSON format:\n\nText: \"John Smith, age 32, works at Google as a software engineer.\"\n\nOutput format:\n{\"name\": \"...\", \"age\": ..., \"company\": \"...\", \"role\": \"...\"}\"\"\"\n\nprint(\"Prompts ready for API calls\")"
      },
      "tips": [
        "Be specific: 'Write a 3-paragraph essay' > 'Write about X'",
        "Show don't tell: Examples often work better than instructions",
        "Iterate: Refine prompts based on outputs"
      ]
    },
    {
      "id": "chain_of_thought",
      "title": "Chain of Thought",
      "symbol": "üí≠",
      "level": "intermediate",
      "definition": {
        "text": "Chain of Thought (CoT) prompting encourages LLMs to 'show their work' by generating intermediate reasoning steps before giving a final answer. This dramatically improves performance on complex reasoning tasks like math and logic problems. Variants include zero-shot CoT ('Let's think step by step') and self-consistency.",
        "keyTerms": ["Reasoning", "Step-by-step", "Self-consistency", "Zero-shot CoT", "Few-shot CoT", "Decomposition", "Verification"]
      },
      "keyFormulas": [
        {
          "id": "cot_structure",
          "name": "CoT Pattern",
          "formula": "Question ‚Üí Reasoning Steps ‚Üí Answer",
          "latex": "Q \\rightarrow R_1, R_2, \\ldots, R_n \\rightarrow A",
          "meaning": "Explicit reasoning before answering"
        }
      ],
      "examples": [
        {
          "id": "cot_ex1",
          "question": "Without CoT vs With CoT on a math problem",
          "steps": [
            {"step": 1, "action": "Without CoT", "result": "Q: If 3 apples cost $1.50, how much for 7? A: $3.50 (often wrong)", "explanation": "Direct answer may be wrong"},
            {"step": 2, "action": "With CoT", "result": "Step 1: Cost per apple = $1.50/3 = $0.50", "explanation": "Break down the problem"},
            {"step": 3, "action": "Continue CoT", "result": "Step 2: 7 apples = 7 √ó $0.50 = $3.50", "explanation": "Apply to original question"},
            {"step": 4, "action": "Final answer", "result": "Therefore, 7 apples cost $3.50", "explanation": "Correct with reasoning"}
          ],
          "finalAnswer": "CoT improves accuracy on reasoning tasks",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Math Problem Solving", "description": "Step-by-step calculations"},
        {"title": "Code Debugging", "description": "Reasoning through bugs"},
        {"title": "Complex Analysis", "description": "Breaking down multi-step problems"}
      ],
      "codeExample": {
        "python": "from openai import OpenAI\n\nclient = OpenAI()\n\n# Standard prompt (often fails on reasoning)\nstandard_prompt = \"\"\"\nRoger has 5 tennis balls. He buys 2 more cans of tennis balls.\nEach can has 3 tennis balls. How many tennis balls does he have now?\n\nAnswer:\"\"\"\n\n# Chain of Thought prompt\ncot_prompt = \"\"\"\nRoger has 5 tennis balls. He buys 2 more cans of tennis balls.\nEach can has 3 tennis balls. How many tennis balls does he have now?\n\nLet's think step by step:\"\"\"\n\n# Zero-shot CoT (just add magic words)\nzero_shot_cot = \"\"\"\nRoger has 5 tennis balls. He buys 2 more cans of tennis balls.\nEach can has 3 tennis balls. How many tennis balls does he have now?\n\nLet's think step by step.\"\"\"\n\n# Self-consistency: Generate multiple CoT paths and vote\ndef self_consistency(prompt, n_samples=5):\n    answers = []\n    for _ in range(n_samples):\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0.7  # Higher temp for diversity\n        )\n        # Extract final answer from each\n        answers.append(response.choices[0].message.content)\n    # Vote on most common answer\n    return max(set(answers), key=answers.count)"
      },
      "tips": [
        "\"Let's think step by step\" is surprisingly effective (zero-shot CoT)",
        "Self-consistency: Sample multiple CoT paths and vote",
        "CoT works best on GPT-3.5+, less effective on smaller models"
      ]
    },
    {
      "id": "rlhf",
      "title": "RLHF",
      "symbol": "üëç",
      "level": "advanced",
      "definition": {
        "text": "Reinforcement Learning from Human Feedback (RLHF) is the technique that makes LLMs helpful, harmless, and honest. First, a reward model is trained on human preferences (which response is better). Then, the LLM is fine-tuned using PPO to maximize this reward while staying close to the original model. This alignment process is what makes ChatGPT different from raw GPT-3.",
        "keyTerms": ["Reward Model", "PPO", "Alignment", "Human Preferences", "Constitutional AI", "DPO", "KL Divergence", "Preference Learning"]
      },
      "keyFormulas": [
        {
          "id": "rlhf_objective",
          "name": "RLHF Objective",
          "formula": "J = E[R(x,y)] - Œ≤ √ó KL(œÄ||œÄ_ref)",
          "latex": "J = \\mathbb{E}[R(x,y)] - \\beta \\cdot \\text{KL}(\\pi || \\pi_{\\text{ref}})",
          "meaning": "Maximize reward while staying close to reference policy"
        },
        {
          "id": "reward_model",
          "name": "Reward Model Training",
          "formula": "L = -log(œÉ(r·µ©(x,y·µÇ) - r·µ©(x,y·¥∏)))",
          "latex": "L = -\\log\\sigma(r_\\theta(x,y^w) - r_\\theta(x,y^l))",
          "meaning": "Preferred response should score higher"
        }
      ],
      "examples": [
        {
          "id": "rlhf_ex1",
          "question": "Why can't we just use supervised fine-tuning for alignment?",
          "steps": [
            {"step": 1, "action": "SFT limitations", "result": "Needs 'correct' answers", "explanation": "Hard to define for open-ended tasks"},
            {"step": 2, "action": "Preference data", "result": "Easier to compare than create", "explanation": "Which response is better?"},
            {"step": 3, "action": "RLHF advantage", "result": "Learns nuanced preferences", "explanation": "Captures subtle quality differences"},
            {"step": 4, "action": "Result", "result": "More helpful, less harmful outputs", "explanation": "Better alignment with intent"}
          ],
          "finalAnswer": "RLHF captures nuanced human preferences that SFT cannot",
          "difficulty": "hard"
        }
      ],
      "realWorldApplications": [
        {"title": "ChatGPT", "description": "RLHF makes GPT helpful and safe"},
        {"title": "Claude", "description": "Uses Constitutional AI (RLHF variant)"},
        {"title": "Safety", "description": "Reduces harmful outputs"}
      ],
      "codeExample": {
        "python": "# RLHF Pipeline Overview (conceptual)\nimport torch\nfrom transformers import AutoModelForCausalLM\nfrom trl import PPOTrainer, PPOConfig\n\n# Step 1: Train Reward Model on human preferences\nclass RewardModel(torch.nn.Module):\n    def __init__(self, base_model):\n        super().__init__()\n        self.model = base_model\n        self.value_head = torch.nn.Linear(768, 1)\n    \n    def forward(self, input_ids):\n        outputs = self.model(input_ids)\n        hidden_states = outputs.last_hidden_state[:, -1, :]\n        return self.value_head(hidden_states)\n\n# Step 2: PPO Training Loop (simplified)\nppo_config = PPOConfig(\n    learning_rate=1e-5,\n    batch_size=64,\n    ppo_epochs=4,\n    init_kl_coef=0.2,  # KL penalty\n)\n\n# ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer)\n\n# Step 3: DPO Alternative (Direct Preference Optimization)\n# Simpler than PPO, no separate reward model\n# Loss: -log(œÉ(Œ≤ √ó log(œÄ/œÄ_ref)(y_w) - Œ≤ √ó log(œÄ/œÄ_ref)(y_l)))\n\nprint(\"RLHF/DPO: Key to making LLMs helpful and safe\")"
      },
      "tips": [
        "DPO is simpler than RLHF (no reward model, no PPO)",
        "KL penalty prevents model from drifting too far from base",
        "Quality of human preference data is crucial"
      ]
    },
    {
      "id": "rag",
      "title": "RAG",
      "symbol": "üìñ",
      "level": "intermediate",
      "definition": {
        "text": "Retrieval Augmented Generation (RAG) enhances LLMs by retrieving relevant documents and including them in the context before generation. This allows the model to access up-to-date information, cite sources, and reduce hallucinations. RAG combines the strengths of retrieval systems (accuracy) with LLMs (fluency).",
        "keyTerms": ["Retrieval", "Vector Database", "Embeddings", "Chunking", "Context Window", "Semantic Search", "Hybrid Search", "Reranking"]
      },
      "keyFormulas": [
        {
          "id": "rag_pipeline",
          "name": "RAG Pipeline",
          "formula": "Query ‚Üí Retrieve(docs) ‚Üí LLM(query + docs) ‚Üí Answer",
          "latex": "q \\rightarrow \\text{Retrieve}(D) \\rightarrow \\text{LLM}(q \\oplus d_i) \\rightarrow a",
          "meaning": "Retrieve relevant docs, then generate with context"
        }
      ],
      "examples": [
        {
          "id": "rag_ex1",
          "question": "When should you use RAG vs fine-tuning?",
          "steps": [
            {"step": 1, "action": "RAG advantages", "result": "No training, easy to update", "explanation": "Just update documents"},
            {"step": 2, "action": "Fine-tuning advantages", "result": "Better for style/format changes", "explanation": "Deeper integration"},
            {"step": 3, "action": "RAG use case", "result": "QA over docs, customer support", "explanation": "Factual, source-based"},
            {"step": 4, "action": "Fine-tuning use case", "result": "Domain-specific language, tone", "explanation": "Behavior modification"}
          ],
          "finalAnswer": "RAG for facts/knowledge, fine-tuning for behavior/style",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Enterprise Search", "description": "Answer questions from company docs"},
        {"title": "Customer Support", "description": "Accurate answers from knowledge base"},
        {"title": "Research Assistants", "description": "Cite sources for claims"}
      ],
      "codeExample": {
        "python": "from langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Step 1: Load and chunk documents\nloader = TextLoader(\"documents/knowledge_base.txt\")\ndocuments = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\nchunks = text_splitter.split_documents(documents)\n\n# Step 2: Create embeddings and store in vector DB\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(\n    documents=chunks,\n    embedding=embeddings,\n    persist_directory=\"./chroma_db\"\n)\n\n# Step 3: Create RAG chain\nllm = ChatOpenAI(model=\"gpt-4\")\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3})\n)\n\n# Step 4: Query\nquery = \"What is our refund policy?\"\nresult = qa_chain.run(query)\nprint(result)"
      },
      "tips": [
        "Chunk size matters: too small loses context, too big wastes tokens",
        "Hybrid search (keyword + semantic) often works best",
        "Reranking improves retrieval quality"
      ]
    },
    {
      "id": "quantization",
      "title": "Model Quantization",
      "symbol": "üóúÔ∏è",
      "level": "advanced",
      "definition": {
        "text": "Quantization reduces model size and speeds up inference by using lower-precision numbers (e.g., INT8 or INT4 instead of FP32). This is crucial for deploying large LLMs on consumer hardware. Techniques include post-training quantization (PTQ), quantization-aware training (QAT), and GPTQ/AWQ for LLMs.",
        "keyTerms": ["INT8", "INT4", "FP16", "BF16", "GPTQ", "AWQ", "GGUF", "Bitsandbytes", "QLoRA"]
      },
      "keyFormulas": [
        {
          "id": "quant_formula",
          "name": "Linear Quantization",
          "formula": "Q(x) = round(x/scale) + zero_point",
          "latex": "Q(x) = \\text{round}\\left(\\frac{x}{s}\\right) + z",
          "meaning": "Map float to integer range"
        }
      ],
      "examples": [
        {
          "id": "quant_ex1",
          "question": "How much memory does a 7B parameter model need at different precisions?",
          "steps": [
            {"step": 1, "action": "FP32 (4 bytes)", "result": "7B √ó 4 = 28 GB", "explanation": "Full precision"},
            {"step": 2, "action": "FP16 (2 bytes)", "result": "7B √ó 2 = 14 GB", "explanation": "Half precision"},
            {"step": 3, "action": "INT4 (0.5 bytes)", "result": "7B √ó 0.5 = 3.5 GB", "explanation": "4-bit quantization"}
          ],
          "finalAnswer": "FP32: 28GB, FP16: 14GB, INT4: 3.5GB",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Local LLMs", "description": "Run 70B models on consumer GPUs"},
        {"title": "Mobile Deployment", "description": "On-device inference"},
        {"title": "Cost Reduction", "description": "Faster inference, lower cloud costs"}
      ],
      "codeExample": {
        "python": "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\n# 4-bit quantization with bitsandbytes\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",  # Normal Float 4-bit\n    bnb_4bit_use_double_quant=True  # Quantize the quantization constants\n)\n\n# Load quantized model\nmodel_name = \"meta-llama/Llama-2-7b-hf\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Check memory usage\nprint(f\"Model memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n\n# Inference works normally\ninputs = tokenizer(\"The future of AI is\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      },
      "tips": [
        "INT4 is sweet spot for LLMs (minimal quality loss)",
        "GGUF format is standard for llama.cpp (CPU inference)",
        "QLoRA combines quantization with efficient fine-tuning"
      ]
    },
    {
      "id": "agents",
      "title": "LLM Agents",
      "symbol": "ü§ñ",
      "level": "advanced",
      "definition": {
        "text": "LLM Agents are systems that use language models to reason, plan, and take actions to accomplish tasks. They combine LLMs with tools (code execution, web search, APIs), memory, and planning capabilities. The agent decides which tool to use, executes it, observes results, and iterates until the task is complete.",
        "keyTerms": ["ReAct", "Tool Use", "Function Calling", "Planning", "Memory", "Chain of Thought", "Self-Reflection", "Multi-Agent"]
      },
      "keyFormulas": [
        {
          "id": "react_loop",
          "name": "ReAct Loop",
          "formula": "Thought ‚Üí Action ‚Üí Observation ‚Üí repeat",
          "latex": "\\text{Thought} \\rightarrow \\text{Action} \\rightarrow \\text{Observation} \\rightarrow \\cdots",
          "meaning": "Reasoning and Acting in interleaved manner"
        }
      ],
      "examples": [
        {
          "id": "agent_ex1",
          "question": "How does an agent answer 'What's the weather in Tokyo and convert to Fahrenheit'?",
          "steps": [
            {"step": 1, "action": "Thought", "result": "Need weather API for Tokyo", "explanation": "Identify required tool"},
            {"step": 2, "action": "Action", "result": "weather_api(location='Tokyo')", "explanation": "Call weather tool"},
            {"step": 3, "action": "Observation", "result": "Temperature: 20¬∞C", "explanation": "Tool returns data"},
            {"step": 4, "action": "Thought", "result": "Need to convert C to F", "explanation": "Identify next step"},
            {"step": 5, "action": "Action", "result": "calculator(20 √ó 9/5 + 32)", "explanation": "Use calculator tool"},
            {"step": 6, "action": "Final Answer", "result": "68¬∞F in Tokyo", "explanation": "Combine results"}
          ],
          "finalAnswer": "Agent chains multiple tools (weather + calculator) to answer complex queries",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Coding Assistants", "description": "Write, run, debug code iteratively"},
        {"title": "Research Agents", "description": "Search, read, synthesize information"},
        {"title": "Automation", "description": "Email, calendar, data entry tasks"}
      ],
      "codeExample": {
        "python": "from langchain.agents import initialize_agent, Tool\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.tools import DuckDuckGoSearchRun, PythonREPLTool\n\n# Define tools\nsearch = DuckDuckGoSearchRun()\npython_repl = PythonREPLTool()\n\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"Search the web for current information\"\n    ),\n    Tool(\n        name=\"Python\",\n        func=python_repl.run,\n        description=\"Execute Python code for calculations\"\n    )\n]\n\n# Initialize agent\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0)\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=\"zero-shot-react-description\",  # ReAct agent\n    verbose=True  # See agent's reasoning\n)\n\n# Run agent\nresult = agent.run(\n    \"What is the population of France? Calculate the square root of that number.\"\n)\nprint(result)\n\n# Agent will:\n# 1. Search for France population\n# 2. Use Python to calculate sqrt\n# 3. Return combined answer"
      },
      "tips": [
        "Use GPT-4 or Claude for complex agent tasks",
        "Limit tools to prevent confusion",
        "Add error handling for tool failures"
      ]
    },
    {
      "id": "constitutional_ai",
      "title": "Constitutional AI",
      "symbol": "üìú",
      "level": "advanced",
      "definition": {
        "text": "Constitutional AI (CAI) is Anthropic's approach to training helpful, harmless, and honest AI. Instead of relying solely on human feedback, the model critiques and revises its own outputs based on a set of principles (constitution). This self-improvement process reduces the need for human labeling while producing safer, more aligned models.",
        "keyTerms": ["Self-Critique", "Principles", "Red-Teaming", "RLAIF", "Harmlessness", "Helpfulness", "Honesty"]
      },
      "keyFormulas": [
        {
          "id": "cai_process",
          "name": "CAI Process",
          "formula": "Response ‚Üí Critique ‚Üí Revision ‚Üí RLHF",
          "latex": "R \\xrightarrow{\\text{critique}} C \\xrightarrow{\\text{revise}} R' \\xrightarrow{\\text{RL}} \\pi",
          "meaning": "Self-improvement loop followed by RL"
        }
      ],
      "examples": [
        {
          "id": "cai_ex1",
          "question": "How does Constitutional AI handle a harmful request?",
          "steps": [
            {"step": 1, "action": "Initial response", "result": "Model might give harmful info", "explanation": "Base model vulnerability"},
            {"step": 2, "action": "Self-critique", "result": "Check against principles", "explanation": "Is this harmful?"},
            {"step": 3, "action": "Revision", "result": "Rewrite to be helpful but safe", "explanation": "Explain why can't help"},
            {"step": 4, "action": "Training", "result": "RL on revised responses", "explanation": "Learn to be safe directly"}
          ],
          "finalAnswer": "Model learns to self-correct based on principles, reducing harmful outputs",
          "difficulty": "hard"
        }
      ],
      "realWorldApplications": [
        {"title": "Claude", "description": "Anthropic's AI assistant uses CAI"},
        {"title": "Safe AI Systems", "description": "Reduced need for human moderation"},
        {"title": "Scalable Alignment", "description": "Self-supervision scales better"}
      ],
      "codeExample": {
        "python": "# Conceptual example of Constitutional AI self-critique\n\nCONSTITUTION = [\n    \"Be helpful to the user while avoiding harmful content.\",\n    \"If asked for dangerous information, explain why you can't help.\",\n    \"Be honest about uncertainty and limitations.\",\n    \"Respect privacy and don't assist with illegal activities.\"\n]\n\ndef constitutional_self_critique(initial_response, query):\n    \"\"\"\n    Self-critique process for Constitutional AI\n    \"\"\"\n    prompt = f\"\"\"\n    Query: {query}\n    Response: {initial_response}\n    \n    Constitution principles:\n    {chr(10).join(f'- {p}' for p in CONSTITUTION)}\n    \n    Critique: Does this response violate any principles?\n    If so, provide a revised response that maintains helpfulness\n    while adhering to the constitution.\n    \"\"\"\n    \n    # In practice, this would call the LLM\n    # critique = llm.generate(prompt)\n    # return critique.revised_response\n    \n    return \"Revised response following constitutional principles\"\n\n# Training loop collects (query, revised_response) pairs\n# Then trains via RLHF on the self-improved data"
      },
      "tips": [
        "Constitution should be clear and specific",
        "Balance helpfulness with safety constraints",
        "RLAIF (RL from AI Feedback) scales better than human labeling"
      ]
    }
  ]
}

