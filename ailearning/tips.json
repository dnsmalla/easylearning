{
  "version": "1.0.0",
  "description": "Study tips and best practices for AI/ML learning",
  "categories": [
    {"id": "learning", "name": "Learning Strategies", "icon": "brain", "color": "#6366F1"},
    {"id": "coding", "name": "Coding Tips", "icon": "chevron.left.forwardslash.chevron.right", "color": "#10B981"},
    {"id": "concepts", "name": "Concept Tips", "icon": "lightbulb", "color": "#F59E0B"},
    {"id": "career", "name": "Career Advice", "icon": "briefcase", "color": "#EC4899"}
  ],
  "tips": [
    {
      "id": "tip_001",
      "title": "Learn by Doing",
      "emoji": "üõ†Ô∏è",
      "shortTip": "Build projects, don't just watch tutorials",
      "fullExplanation": "The fastest way to learn ML is by building real projects. After learning a concept, immediately apply it. Build a spam classifier after learning classification, create an image recognizer after learning CNNs. Projects reveal gaps in understanding that passive learning misses.",
      "category": "learning",
      "memoryTrick": "Tutorial ‚Üí Try it ‚Üí Teach it (3 T's)",
      "example": "After learning neural networks, build a digit classifier from scratch"
    },
    {
      "id": "tip_002",
      "title": "Start with Scikit-learn",
      "emoji": "üî¨",
      "shortTip": "Master sklearn before deep learning",
      "fullExplanation": "Scikit-learn teaches ML fundamentals without the complexity of neural networks. You'll understand data preprocessing, model evaluation, cross-validation, and hyperparameter tuning. These skills transfer directly to deep learning and are often overlooked by beginners.",
      "category": "learning",
      "memoryTrick": "sklearn = ML ABCs",
      "example": "Build 5 different classifiers on the same dataset to understand tradeoffs"
    },
    {
      "id": "tip_003",
      "title": "Print Everything",
      "emoji": "üìÉ",
      "shortTip": "Print shapes, types, and sample values constantly",
      "fullExplanation": "Shape mismatches are the #1 cause of ML bugs. Get in the habit of printing tensor shapes at every step. print(x.shape) should be in your muscle memory. Also print sample values to catch NaN/Inf issues early.",
      "category": "coding",
      "memoryTrick": "When in doubt, print it out",
      "example": "print(f'After conv: {x.shape}')  # Debug CNNs"
    },
    {
      "id": "tip_004",
      "title": "Overfit First",
      "emoji": "üìà",
      "shortTip": "If your model can't overfit one batch, something is broken",
      "fullExplanation": "Before training on full data, train on a single batch until loss reaches near zero. If this fails, you have a bug (wrong loss function, architecture issue, learning rate too low). This technique saves hours of debugging.",
      "category": "coding",
      "memoryTrick": "1 batch ‚Üí 0 loss ‚Üí then scale",
      "example": "Train on 10 samples until 99% accuracy, then add data"
    },
    {
      "id": "tip_005",
      "title": "Understand the Math Eventually",
      "emoji": "üî¢",
      "shortTip": "Use APIs first, then learn the math behind them",
      "fullExplanation": "Don't let math intimidate you into never starting. Use PyTorch/TensorFlow first to get intuition, then go back and understand backpropagation, optimization, etc. Practical understanding often makes the math clearer.",
      "category": "concepts",
      "memoryTrick": "Build first, understand later",
      "example": "Use Adam optimizer for months, then study why it works"
    },
    {
      "id": "tip_006",
      "title": "Baseline First",
      "emoji": "üìä",
      "shortTip": "Always start with a simple baseline model",
      "fullExplanation": "Before training a complex model, establish a baseline. For classification, what's random chance? What does a simple logistic regression achieve? Complex models should beat these baselines meaningfully.",
      "category": "coding",
      "memoryTrick": "Simple model ‚Üí Complex model ‚Üí Compare",
      "example": "Logistic regression baseline before trying BERT for text classification"
    },
    {
      "id": "tip_007",
      "title": "Version Control Everything",
      "emoji": "üìÅ",
      "shortTip": "Use Git for code, MLflow/W&B for experiments",
      "fullExplanation": "ML experiments are notoriously hard to reproduce. Track every experiment: hyperparameters, data versions, model checkpoints, and metrics. Tools like Weights & Biases or MLflow make this easy. You'll thank yourself later.",
      "category": "coding",
      "memoryTrick": "If it's not logged, it didn't happen",
      "example": "wandb.log({'loss': loss, 'lr': lr, 'epoch': epoch})"
    },
    {
      "id": "tip_008",
      "title": "Read Papers with Code",
      "emoji": "üìñ",
      "shortTip": "Papers with GitHub implementations are 10x easier to understand",
      "fullExplanation": "When reading ML papers, find the official implementation or a clean reproduction on GitHub. Reading code alongside the paper dramatically improves understanding. PapersWithCode.com is your friend.",
      "category": "learning",
      "memoryTrick": "Paper + Code = Understanding",
      "example": "Read 'Attention Is All You Need' with the annotated Transformer code"
    },
    {
      "id": "tip_009",
      "title": "Use Pretrained Models",
      "emoji": "üöÄ",
      "shortTip": "Don't train from scratch unless you need to",
      "fullExplanation": "Transfer learning is incredibly powerful. For vision, use ImageNet pretrained models. For NLP, use BERT or GPT. Fine-tuning is faster, cheaper, and often better than training from scratch. Only train from scratch if you have a compelling reason.",
      "category": "concepts",
      "memoryTrick": "Pretrained + Fine-tune = Fast results",
      "example": "Fine-tune BERT for sentiment instead of training word embeddings"
    },
    {
      "id": "tip_010",
      "title": "Learning Rate is King",
      "emoji": "üëë",
      "shortTip": "If training fails, check learning rate first",
      "fullExplanation": "Learning rate is the most important hyperparameter. Too high ‚Üí loss explodes or oscillates. Too low ‚Üí training is glacial. Use learning rate finders, start with 1e-3 for Adam, and use learning rate schedulers.",
      "category": "concepts",
      "memoryTrick": "Loss exploding? LR too high. Loss stuck? LR too low.",
      "example": "Start with lr=0.001, reduce by 10x if unstable"
    },
    {
      "id": "tip_011",
      "title": "Kaggle is the Gym",
      "emoji": "üèãÔ∏è",
      "shortTip": "Compete on Kaggle to level up fast",
      "fullExplanation": "Kaggle competitions provide real datasets, clear metrics, and leaderboards. Study winning solutions to learn state-of-the-art techniques. You'll learn data preprocessing tricks, ensemble methods, and practical feature engineering.",
      "category": "career",
      "memoryTrick": "Kaggle = Practical ML education",
      "example": "Enter Titanic competition, then move to active competitions"
    },
    {
      "id": "tip_012",
      "title": "Build a Portfolio",
      "emoji": "üíº",
      "shortTip": "GitHub repos > Certificates",
      "fullExplanation": "Hiring managers want to see what you've built. A well-documented GitHub portfolio with 3-5 solid ML projects is worth more than any course certificate. Include README files, notebooks with explanations, and deployed demos.",
      "category": "career",
      "memoryTrick": "Show, don't tell",
      "example": "End-to-end project: data collection ‚Üí model ‚Üí deployed API"
    },
    {
      "id": "tip_013",
      "title": "Data Quality > Model Complexity",
      "emoji": "üíé",
      "shortTip": "Clean data beats complex models",
      "fullExplanation": "Most ML improvements come from better data, not fancier models. Spend time understanding your data, cleaning it, and engineering meaningful features. A simple model on great data often beats a complex model on mediocre data.",
      "category": "concepts",
      "memoryTrick": "Garbage in, garbage out (GIGO)",
      "example": "Fix 100 mislabeled examples vs tuning hyperparameters for hours"
    },
    {
      "id": "tip_014",
      "title": "Embrace Failure",
      "emoji": "üîÑ",
      "shortTip": "Failed experiments teach more than successes",
      "fullExplanation": "Most ML experiments fail. That's normal. The key is to learn from failures systematically. Why did this architecture underperform? Why did training diverge? Failed experiments build intuition faster than lucky successes.",
      "category": "learning",
      "memoryTrick": "Fail fast, learn faster",
      "example": "Document why approach X didn't work for future reference"
    },
    {
      "id": "tip_015",
      "title": "Join the Community",
      "emoji": "ü§ù",
      "shortTip": "Twitter/X, Discord, and conferences accelerate learning",
      "fullExplanation": "Follow ML researchers on Twitter/X, join Discord communities like MLOps Community or HuggingFace, attend virtual conferences. The ML field moves fast, and staying connected helps you discover new techniques and opportunities.",
      "category": "career",
      "memoryTrick": "Network = Net worth (in knowledge)",
      "example": "Follow @karpathy, @GaryMarcus, @fchollet on Twitter"
    }
  ]
}

