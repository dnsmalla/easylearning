{
  "category": "ML Fundamentals",
  "categoryId": "fundamentals",
  "version": "1.0.0",
  "description": "Core machine learning concepts, algorithms, and mathematical foundations",
  "icon": "brain",
  "color": "#6366F1",
  "topics": [
    {
      "id": "what_is_ml",
      "title": "What is Machine Learning?",
      "symbol": "ðŸ¤–",
      "level": "beginner",
      "definition": {
        "text": "Machine Learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. Instead of writing rules, we provide data and let algorithms discover patterns. ML is divided into three main types: supervised learning (labeled data), unsupervised learning (unlabeled data), and reinforcement learning (learning from rewards).",
        "keyTerms": ["Supervised Learning", "Unsupervised Learning", "Reinforcement Learning", "Training Data", "Model", "Features", "Labels", "Inference"]
      },
      "keyFormulas": [
        {
          "id": "ml_pipeline",
          "name": "ML Pipeline",
          "formula": "Data â†’ Features â†’ Model â†’ Prediction",
          "latex": "\\text{Data} \\rightarrow \\text{Features} \\rightarrow \\text{Model} \\rightarrow \\hat{y}",
          "meaning": "The standard machine learning workflow"
        }
      ],
      "examples": [
        {
          "id": "ml_ex1",
          "question": "Given a dataset of house prices with features (size, bedrooms, location), what type of ML problem is predicting the price?",
          "steps": [
            {"step": 1, "action": "Identify the target variable", "result": "Price (continuous value)", "explanation": "We want to predict a number"},
            {"step": 2, "action": "Check if we have labels", "result": "Yes, we have actual prices", "explanation": "This is supervised learning"},
            {"step": 3, "action": "Determine problem type", "result": "Regression (predicting continuous values)", "explanation": "Not classification since output is numeric"}
          ],
          "finalAnswer": "Supervised Learning - Regression Problem",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Spam Detection", "description": "Classifying emails as spam or not spam"},
        {"title": "Recommendation Systems", "description": "Netflix, Spotify suggesting content you'll like"},
        {"title": "Medical Diagnosis", "description": "Detecting diseases from medical images"}
      ],
      "codeExample": {
        "python": "# Simple ML example with scikit-learn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Sample data: house size -> price\nX = [[1000], [1500], [2000], [2500], [3000]]  # features\ny = [200000, 280000, 350000, 420000, 500000]  # labels\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict\nnew_house = [[1800]]\npredicted_price = model.predict(new_house)\nprint(f'Predicted price: ${predicted_price[0]:,.0f}')"
      },
      "tips": [
        "ML is about finding patterns in data, not memorizing it",
        "More data usually leads to better models (but quality > quantity)",
        "Always split your data into train/test sets to evaluate properly"
      ]
    },
    {
      "id": "linear_regression",
      "title": "Linear Regression",
      "symbol": "ðŸ“ˆ",
      "level": "beginner",
      "definition": {
        "text": "Linear Regression is a supervised learning algorithm that models the relationship between input features and a continuous output by fitting a linear equation. The goal is to find the best-fitting line (or hyperplane) that minimizes the difference between predicted and actual values. It's the foundation for understanding more complex models.",
        "keyTerms": ["Slope", "Intercept", "Coefficients", "Least Squares", "Residuals", "R-squared", "Ordinary Least Squares"]
      },
      "keyFormulas": [
        {
          "id": "linear_eq",
          "name": "Linear Equation",
          "formula": "y = wx + b",
          "latex": "\\hat{y} = wx + b",
          "meaning": "w = weight/slope, b = bias/intercept"
        },
        {
          "id": "mse",
          "name": "Mean Squared Error",
          "formula": "MSE = (1/n) Ã— Î£(yáµ¢ - Å·áµ¢)Â²",
          "latex": "\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2",
          "meaning": "Average squared difference between actual and predicted"
        },
        {
          "id": "r_squared",
          "name": "R-Squared",
          "formula": "RÂ² = 1 - (SS_res / SS_tot)",
          "latex": "R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}",
          "meaning": "Proportion of variance explained (0 to 1)"
        }
      ],
      "examples": [
        {
          "id": "lr_ex1",
          "question": "Calculate the predicted value for x=5 given y = 2x + 3",
          "steps": [
            {"step": 1, "action": "Identify parameters", "result": "w = 2, b = 3", "explanation": "Slope is 2, intercept is 3"},
            {"step": 2, "action": "Apply linear equation", "result": "y = 2(5) + 3", "explanation": "Substitute x = 5"},
            {"step": 3, "action": "Calculate", "result": "y = 10 + 3 = 13", "explanation": "Final prediction"}
          ],
          "finalAnswer": "Å· = 13",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "House Price Prediction", "description": "Predicting prices based on size, location, features"},
        {"title": "Sales Forecasting", "description": "Predicting future sales from historical data"},
        {"title": "Stock Trends", "description": "Simple trend analysis in financial data"}
      ],
      "codeExample": {
        "python": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Create sample data\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([2.1, 4.0, 5.9, 8.1, 10.0])\n\n# Fit model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Get parameters\nprint(f'Weight (slope): {model.coef_[0]:.2f}')\nprint(f'Bias (intercept): {model.intercept_:.2f}')\n\n# Predict\ny_pred = model.predict(X)\n\n# Evaluate\nmse = mean_squared_error(y, y_pred)\nr2 = r2_score(y, y_pred)\nprint(f'MSE: {mse:.4f}')\nprint(f'RÂ²: {r2:.4f}')"
      },
      "tips": [
        "Check for linear relationship before using linear regression",
        "Feature scaling improves convergence in gradient descent",
        "Residual plots help diagnose model problems"
      ]
    },
    {
      "id": "gradient_descent",
      "title": "Gradient Descent",
      "symbol": "â¬‡ï¸",
      "level": "beginner",
      "definition": {
        "text": "Gradient Descent is an optimization algorithm used to minimize a loss function by iteratively moving in the direction of steepest descent (negative gradient). It's the backbone of training neural networks and many ML algorithms. The learning rate controls how big each step is - too small and training is slow, too large and you might overshoot.",
        "keyTerms": ["Learning Rate", "Gradient", "Optimization", "Convergence", "Local Minimum", "Global Minimum", "Batch Size", "Epoch"]
      },
      "keyFormulas": [
        {
          "id": "gd_update",
          "name": "Parameter Update",
          "formula": "Î¸ = Î¸ - Î± Ã— âˆ‡L(Î¸)",
          "latex": "\\theta := \\theta - \\alpha \\nabla_{\\theta} L(\\theta)",
          "meaning": "Î± = learning rate, âˆ‡L = gradient of loss"
        },
        {
          "id": "gradient_simple",
          "name": "Gradient (Simple)",
          "formula": "âˆ‚L/âˆ‚w = (2/n) Ã— Î£(Å· - y) Ã— x",
          "latex": "\\frac{\\partial L}{\\partial w} = \\frac{2}{n}\\sum(\\hat{y}_i - y_i) \\cdot x_i",
          "meaning": "For MSE loss in linear regression"
        }
      ],
      "examples": [
        {
          "id": "gd_ex1",
          "question": "If current weight w=5, gradient=2, and learning rate=0.1, what is the new weight?",
          "steps": [
            {"step": 1, "action": "Apply update rule", "result": "w_new = w - Î± Ã— gradient", "explanation": "Standard GD update"},
            {"step": 2, "action": "Substitute values", "result": "w_new = 5 - 0.1 Ã— 2", "explanation": "Plug in given values"},
            {"step": 3, "action": "Calculate", "result": "w_new = 5 - 0.2 = 4.8", "explanation": "Weight decreases"}
          ],
          "finalAnswer": "w = 4.8",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Neural Network Training", "description": "Backpropagation uses gradient descent"},
        {"title": "Deep Learning", "description": "All modern deep learning optimizers are GD variants"},
        {"title": "Recommendation Engines", "description": "Matrix factorization optimization"}
      ],
      "codeExample": {
        "python": "import numpy as np\n\n# Simple gradient descent for linear regression\ndef gradient_descent(X, y, lr=0.01, epochs=1000):\n    n = len(y)\n    w, b = 0, 0  # Initialize parameters\n    \n    for epoch in range(epochs):\n        # Predictions\n        y_pred = w * X + b\n        \n        # Compute gradients\n        dw = (2/n) * np.sum((y_pred - y) * X)\n        db = (2/n) * np.sum(y_pred - y)\n        \n        # Update parameters\n        w = w - lr * dw\n        b = b - lr * db\n        \n        if epoch % 200 == 0:\n            loss = np.mean((y_pred - y) ** 2)\n            print(f'Epoch {epoch}: Loss = {loss:.4f}')\n    \n    return w, b\n\n# Example usage\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 6, 8, 10])  # y = 2x\n\nw, b = gradient_descent(X, y, lr=0.1, epochs=1000)\nprint(f'\\nLearned: y = {w:.2f}x + {b:.2f}')"
      },
      "tips": [
        "Start with a small learning rate (0.001 or 0.01) and adjust",
        "Use learning rate schedulers to decrease LR over time",
        "Batch gradient descent vs SGD: tradeoff between stability and speed"
      ]
    },
    {
      "id": "loss_functions",
      "title": "Loss Functions",
      "symbol": "ðŸ“‰",
      "level": "beginner",
      "definition": {
        "text": "Loss functions (also called cost functions or objective functions) measure how wrong a model's predictions are compared to actual values. The goal of training is to minimize this loss. Different problems require different loss functions: MSE for regression, Cross-Entropy for classification. The choice of loss function directly impacts what the model learns.",
        "keyTerms": ["MSE", "MAE", "Cross-Entropy", "Binary Cross-Entropy", "Categorical Cross-Entropy", "Hinge Loss", "Huber Loss"]
      },
      "keyFormulas": [
        {
          "id": "mse_loss",
          "name": "Mean Squared Error",
          "formula": "L = (1/n) Ã— Î£(yáµ¢ - Å·áµ¢)Â²",
          "latex": "L = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2",
          "meaning": "For regression, penalizes large errors more"
        },
        {
          "id": "bce_loss",
          "name": "Binary Cross-Entropy",
          "formula": "L = -[yÂ·log(p) + (1-y)Â·log(1-p)]",
          "latex": "L = -[y \\log(p) + (1-y)\\log(1-p)]",
          "meaning": "For binary classification"
        },
        {
          "id": "ce_loss",
          "name": "Categorical Cross-Entropy",
          "formula": "L = -Î£ yáµ¢Â·log(páµ¢)",
          "latex": "L = -\\sum_{i=1}^{C} y_i \\log(p_i)",
          "meaning": "For multi-class classification"
        }
      ],
      "examples": [
        {
          "id": "loss_ex1",
          "question": "Calculate MSE for predictions [2, 4, 6] and actual values [1, 5, 6]",
          "steps": [
            {"step": 1, "action": "Calculate errors", "result": "(2-1)=1, (4-5)=-1, (6-6)=0", "explanation": "Predicted - Actual"},
            {"step": 2, "action": "Square errors", "result": "1Â² + (-1)Â² + 0Â² = 1 + 1 + 0 = 2", "explanation": "Sum of squared errors"},
            {"step": 3, "action": "Calculate mean", "result": "MSE = 2/3 = 0.667", "explanation": "Divide by n=3"}
          ],
          "finalAnswer": "MSE = 0.667",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Regression Tasks", "description": "MSE/MAE for price prediction, forecasting"},
        {"title": "Classification", "description": "Cross-entropy for image classification, sentiment analysis"},
        {"title": "Ranking", "description": "Hinge loss for search result ranking"}
      ],
      "codeExample": {
        "python": "import numpy as np\n\n# Common loss functions\ndef mse_loss(y_true, y_pred):\n    \"\"\"Mean Squared Error - Regression\"\"\"\n    return np.mean((y_true - y_pred) ** 2)\n\ndef mae_loss(y_true, y_pred):\n    \"\"\"Mean Absolute Error - Regression\"\"\"\n    return np.mean(np.abs(y_true - y_pred))\n\ndef binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n    \"\"\"Binary Cross-Entropy - Binary Classification\"\"\"\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return -np.mean(y_true * np.log(y_pred) + \n                    (1 - y_true) * np.log(1 - y_pred))\n\ndef categorical_cross_entropy(y_true, y_pred, epsilon=1e-15):\n    \"\"\"Categorical Cross-Entropy - Multi-class\"\"\"\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return -np.sum(y_true * np.log(y_pred)) / len(y_true)\n\n# Examples\ny_true_reg = np.array([1, 5, 6])\ny_pred_reg = np.array([2, 4, 6])\nprint(f'MSE: {mse_loss(y_true_reg, y_pred_reg):.3f}')\nprint(f'MAE: {mae_loss(y_true_reg, y_pred_reg):.3f}')"
      },
      "tips": [
        "MSE penalizes outliers more heavily than MAE",
        "Use BCE/CE for classification, not MSE",
        "Loss should decrease during training - if not, check learning rate"
      ]
    },
    {
      "id": "regularization",
      "title": "Regularization",
      "symbol": "âš–ï¸",
      "level": "intermediate",
      "definition": {
        "text": "Regularization is a technique to prevent overfitting by adding a penalty term to the loss function that discourages complex models. L1 (Lasso) adds absolute weight values, promoting sparsity. L2 (Ridge) adds squared weights, keeping weights small. Elastic Net combines both. These techniques help models generalize better to unseen data.",
        "keyTerms": ["L1 Regularization", "L2 Regularization", "Lasso", "Ridge", "Elastic Net", "Lambda", "Weight Decay", "Sparsity"]
      },
      "keyFormulas": [
        {
          "id": "l2_reg",
          "name": "L2 Regularization (Ridge)",
          "formula": "L_total = L + Î» Ã— Î£wáµ¢Â²",
          "latex": "L_{total} = L + \\lambda \\sum_{i} w_i^2",
          "meaning": "Penalizes large weights"
        },
        {
          "id": "l1_reg",
          "name": "L1 Regularization (Lasso)",
          "formula": "L_total = L + Î» Ã— Î£|wáµ¢|",
          "latex": "L_{total} = L + \\lambda \\sum_{i} |w_i|",
          "meaning": "Promotes sparse solutions"
        },
        {
          "id": "elastic_net",
          "name": "Elastic Net",
          "formula": "L = L + Î»â‚Î£|wáµ¢| + Î»â‚‚Î£wáµ¢Â²",
          "latex": "L = L + \\lambda_1 \\sum |w_i| + \\lambda_2 \\sum w_i^2",
          "meaning": "Combines L1 and L2"
        }
      ],
      "examples": [
        {
          "id": "reg_ex1",
          "question": "If loss=5, weights=[1, -2, 0.5], and Î»=0.1, calculate L2 regularized loss",
          "steps": [
            {"step": 1, "action": "Calculate sum of squared weights", "result": "1Â² + (-2)Â² + 0.5Â² = 1 + 4 + 0.25 = 5.25", "explanation": "L2 penalty term"},
            {"step": 2, "action": "Apply regularization coefficient", "result": "0.1 Ã— 5.25 = 0.525", "explanation": "Î» times penalty"},
            {"step": 3, "action": "Add to original loss", "result": "5 + 0.525 = 5.525", "explanation": "Total regularized loss"}
          ],
          "finalAnswer": "L_total = 5.525",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Feature Selection", "description": "L1 sets unimportant weights to zero"},
        {"title": "Preventing Overfitting", "description": "All regularization reduces variance"},
        {"title": "Neural Networks", "description": "Weight decay in Adam optimizer"}
      ],
      "codeExample": {
        "python": "from sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.datasets import make_regression\n\n# Create sample data\nX, y = make_regression(n_samples=100, n_features=10, noise=10)\n\n# L2 Regularization (Ridge)\nridge = Ridge(alpha=1.0)\nridge.fit(X, y)\nprint(f'Ridge coefficients: {ridge.coef_[:5].round(2)}...')\n\n# L1 Regularization (Lasso)\nlasso = Lasso(alpha=1.0)\nlasso.fit(X, y)\nprint(f'Lasso coefficients: {lasso.coef_[:5].round(2)}...')\nprint(f'Lasso zero weights: {(lasso.coef_ == 0).sum()}')\n\n# Elastic Net (L1 + L2)\nelastic = ElasticNet(alpha=1.0, l1_ratio=0.5)\nelastic.fit(X, y)\nprint(f'Elastic coefficients: {elastic.coef_[:5].round(2)}...')"
      },
      "tips": [
        "Higher Î» = more regularization = simpler model",
        "Use L1 when you suspect many irrelevant features",
        "L2 is more stable and commonly used (weight decay)"
      ]
    },
    {
      "id": "bias_variance",
      "title": "Bias-Variance Tradeoff",
      "symbol": "ðŸŽ¯",
      "level": "intermediate",
      "definition": {
        "text": "The bias-variance tradeoff is a fundamental concept describing the tension between model complexity and generalization. Bias is error from oversimplified assumptions (underfitting). Variance is error from sensitivity to training data (overfitting). Total error = BiasÂ² + Variance + Irreducible Error. The goal is finding the sweet spot.",
        "keyTerms": ["Bias", "Variance", "Overfitting", "Underfitting", "Model Complexity", "Generalization", "Training Error", "Test Error"]
      },
      "keyFormulas": [
        {
          "id": "total_error",
          "name": "Total Error",
          "formula": "Error = BiasÂ² + Variance + Noise",
          "latex": "\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\sigma^2",
          "meaning": "Decomposition of expected error"
        }
      ],
      "examples": [
        {
          "id": "bv_ex1",
          "question": "A model has 95% training accuracy but only 60% test accuracy. What's the problem?",
          "steps": [
            {"step": 1, "action": "Compare train/test performance", "result": "Large gap (35%)", "explanation": "Train >> Test indicates overfitting"},
            {"step": 2, "action": "Diagnose", "result": "High variance", "explanation": "Model memorized training data"},
            {"step": 3, "action": "Solutions", "result": "More data, regularization, simpler model", "explanation": "Reduce variance"}
          ],
          "finalAnswer": "Overfitting (High Variance) - Model doesn't generalize",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Model Selection", "description": "Choosing between simple and complex models"},
        {"title": "Hyperparameter Tuning", "description": "Finding optimal regularization strength"},
        {"title": "Deep Learning", "description": "Deciding network depth and width"}
      ],
      "codeExample": {
        "python": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n# Generate data\nnp.random.seed(42)\nX = np.sort(np.random.rand(50, 1) * 10, axis=0)\ny = np.sin(X).ravel() + np.random.randn(50) * 0.2\n\n# Test different model complexities\nfor degree in [1, 3, 10, 20]:\n    model = make_pipeline(\n        PolynomialFeatures(degree),\n        LinearRegression()\n    )\n    \n    # Cross-validation scores\n    scores = cross_val_score(model, X, y, cv=5)\n    \n    # Train score\n    model.fit(X, y)\n    train_score = model.score(X, y)\n    \n    print(f'Degree {degree:2d}: Train={train_score:.3f}, '\n          f'CV={scores.mean():.3f}Â±{scores.std():.3f}')"
      },
      "tips": [
        "High train error + High test error = Underfitting (high bias)",
        "Low train error + High test error = Overfitting (high variance)",
        "Use validation curves to visualize the tradeoff"
      ]
    },
    {
      "id": "cross_validation",
      "title": "Cross-Validation",
      "symbol": "ðŸ”„",
      "level": "intermediate",
      "definition": {
        "text": "Cross-validation is a resampling technique to evaluate model performance on limited data. K-Fold CV splits data into K parts, trains on K-1, tests on 1, and repeats K times. It provides a more reliable estimate of generalization than a single train-test split, and helps in hyperparameter tuning and model selection.",
        "keyTerms": ["K-Fold", "Stratified K-Fold", "Leave-One-Out", "Hold-Out", "Validation Set", "Train-Test Split", "Nested CV"]
      },
      "keyFormulas": [
        {
          "id": "cv_score",
          "name": "K-Fold CV Score",
          "formula": "Score = (1/K) Ã— Î£ Scoreáµ¢",
          "latex": "\\text{Score} = \\frac{1}{K}\\sum_{i=1}^{K} \\text{Score}_i",
          "meaning": "Average score across K folds"
        }
      ],
      "examples": [
        {
          "id": "cv_ex1",
          "question": "With 5-fold CV and fold accuracies [0.85, 0.87, 0.83, 0.86, 0.84], what's the CV score?",
          "steps": [
            {"step": 1, "action": "Sum all fold scores", "result": "0.85 + 0.87 + 0.83 + 0.86 + 0.84 = 4.25", "explanation": "Add up accuracies"},
            {"step": 2, "action": "Calculate mean", "result": "4.25 / 5 = 0.85", "explanation": "Average across K=5 folds"},
            {"step": 3, "action": "Calculate std", "result": "Ïƒ â‰ˆ 0.014", "explanation": "Measure of variance"}
          ],
          "finalAnswer": "CV Score = 0.85 Â± 0.014",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Model Comparison", "description": "Fairly compare different algorithms"},
        {"title": "Hyperparameter Tuning", "description": "GridSearchCV uses cross-validation"},
        {"title": "Small Datasets", "description": "Make the most of limited data"}
      ],
      "codeExample": {
        "python": "from sklearn.model_selection import (\n    cross_val_score, KFold, StratifiedKFold, \n    LeaveOneOut, GridSearchCV\n)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\n\n# Load data\nX, y = load_iris(return_X_y=True)\n\n# K-Fold Cross-Validation\nmodel = RandomForestClassifier(random_state=42)\nscores = cross_val_score(model, X, y, cv=5)\nprint(f'5-Fold CV: {scores.mean():.3f} Â± {scores.std():.3f}')\n\n# Stratified K-Fold (maintains class balance)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X, y, cv=skf)\nprint(f'Stratified 5-Fold: {scores.mean():.3f} Â± {scores.std():.3f}')\n\n# GridSearchCV with Cross-Validation\nparam_grid = {'n_estimators': [10, 50, 100], 'max_depth': [3, 5, 10]}\ngrid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X, y)\nprint(f'Best params: {grid_search.best_params_}')\nprint(f'Best CV score: {grid_search.best_score_:.3f}')"
      },
      "tips": [
        "Use stratified K-fold for imbalanced classification",
        "5 or 10 folds is typically a good choice",
        "Leave-one-out is expensive but useful for tiny datasets"
      ]
    },
    {
      "id": "decision_trees",
      "title": "Decision Trees",
      "symbol": "ðŸŒ³",
      "level": "beginner",
      "definition": {
        "text": "Decision Trees are supervised learning algorithms that make predictions by learning decision rules from features. They split data recursively based on feature thresholds that maximize information gain or reduce impurity. Trees are interpretable, handle non-linear relationships, and form the basis for powerful ensemble methods like Random Forests and Gradient Boosting.",
        "keyTerms": ["Node", "Leaf", "Gini Impurity", "Information Gain", "Entropy", "Pruning", "Max Depth", "CART"]
      },
      "keyFormulas": [
        {
          "id": "gini",
          "name": "Gini Impurity",
          "formula": "Gini = 1 - Î£páµ¢Â²",
          "latex": "\\text{Gini} = 1 - \\sum_{i=1}^{C} p_i^2",
          "meaning": "páµ¢ = probability of class i at node"
        },
        {
          "id": "entropy",
          "name": "Entropy",
          "formula": "H = -Î£ páµ¢ Ã— logâ‚‚(páµ¢)",
          "latex": "H = -\\sum_{i=1}^{C} p_i \\log_2(p_i)",
          "meaning": "Measure of disorder/uncertainty"
        },
        {
          "id": "info_gain",
          "name": "Information Gain",
          "formula": "IG = H(parent) - Î£(náµ¢/n)H(childáµ¢)",
          "latex": "IG = H(parent) - \\sum_{i} \\frac{n_i}{n} H(child_i)",
          "meaning": "Reduction in entropy from split"
        }
      ],
      "examples": [
        {
          "id": "dt_ex1",
          "question": "Calculate Gini impurity for a node with 60% class A, 40% class B",
          "steps": [
            {"step": 1, "action": "Identify probabilities", "result": "p_A = 0.6, p_B = 0.4", "explanation": "Class proportions"},
            {"step": 2, "action": "Apply Gini formula", "result": "1 - (0.6Â² + 0.4Â²)", "explanation": "Sum of squared probabilities"},
            {"step": 3, "action": "Calculate", "result": "1 - (0.36 + 0.16) = 1 - 0.52 = 0.48", "explanation": "Gini = 0.48"}
          ],
          "finalAnswer": "Gini = 0.48 (impure node)",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Credit Scoring", "description": "Rule-based loan approval decisions"},
        {"title": "Medical Diagnosis", "description": "Interpretable diagnostic trees"},
        {"title": "Customer Churn", "description": "Identify factors leading to churn"}
      ],
      "codeExample": {
        "python": "from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n# Load data\nX, y = load_iris(return_X_y=True)\nfeature_names = load_iris().feature_names\nclass_names = load_iris().target_names\n\n# Train decision tree\ndt = DecisionTreeClassifier(\n    max_depth=3,\n    criterion='gini',  # or 'entropy'\n    random_state=42\n)\ndt.fit(X, y)\n\nprint(f'Accuracy: {dt.score(X, y):.3f}')\nprint(f'Number of leaves: {dt.get_n_leaves()}')\nprint(f'Tree depth: {dt.get_depth()}')\n\n# Visualize tree\nplt.figure(figsize=(15, 10))\nplot_tree(dt, feature_names=feature_names, \n          class_names=class_names, filled=True)\nplt.title('Decision Tree Visualization')\nplt.show()"
      },
      "tips": [
        "Set max_depth to prevent overfitting",
        "Decision trees are greedy - they find locally optimal splits",
        "Use feature_importances_ to understand which features matter"
      ]
    }
  ]
}

