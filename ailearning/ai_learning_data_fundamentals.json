{
  "category": "ML Fundamentals",
  "categoryId": "fundamentals",
  "version": "1.0.0",
  "description": "Core machine learning concepts, algorithms, and mathematical foundations",
  "icon": "brain",
  "color": "#6366F1",
  "topics": [
    {
      "id": "what_is_ml",
      "title": "What is Machine Learning?",
      "symbol": "ü§ñ",
      "level": "beginner",
      "definition": {
        "text": "Machine Learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. Instead of writing rules, we provide data and let algorithms discover patterns. ML is divided into three main types: supervised learning (labeled data), unsupervised learning (unlabeled data), and reinforcement learning (learning from rewards).",
        "keyTerms": ["Supervised Learning", "Unsupervised Learning", "Reinforcement Learning", "Training Data", "Model", "Features", "Labels", "Inference"]
      },
      "keyFormulas": [
        {
          "id": "ml_pipeline",
          "name": "ML Pipeline",
          "formula": "Data ‚Üí Features ‚Üí Model ‚Üí Prediction",
          "latex": "\\text{Data} \\rightarrow \\text{Features} \\rightarrow \\text{Model} \\rightarrow \\hat{y}",
          "meaning": "The standard machine learning workflow"
        }
      ],
      "examples": [
        {
          "id": "ml_ex1",
          "question": "Given a dataset of house prices with features (size, bedrooms, location), what type of ML problem is predicting the price?",
          "steps": [
            {"step": 1, "action": "Identify the target variable", "result": "Price (continuous value)", "explanation": "We want to predict a number"},
            {"step": 2, "action": "Check if we have labels", "result": "Yes, we have actual prices", "explanation": "This is supervised learning"},
            {"step": 3, "action": "Determine problem type", "result": "Regression (predicting continuous values)", "explanation": "Not classification since output is numeric"}
          ],
          "finalAnswer": "Supervised Learning - Regression Problem",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Spam Detection", "description": "Classifying emails as spam or not spam"},
        {"title": "Recommendation Systems", "description": "Netflix, Spotify suggesting content you'll like"},
        {"title": "Medical Diagnosis", "description": "Detecting diseases from medical images"}
      ],
      "codeExample": {
        "python": "# Simple ML example with scikit-learn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Sample data: house size -> price\nX = [[1000], [1500], [2000], [2500], [3000]]  # features\ny = [200000, 280000, 350000, 420000, 500000]  # labels\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict\nnew_house = [[1800]]\npredicted_price = model.predict(new_house)\nprint(f'Predicted price: ${predicted_price[0]:,.0f}')"
      },
      "tips": [
        "ML is about finding patterns in data, not memorizing it",
        "More data usually leads to better models (but quality > quantity)",
        "Always split your data into train/test sets to evaluate properly"
      ]
    },
    {
      "id": "linear_regression",
      "title": "Linear Regression",
      "symbol": "üìà",
      "level": "beginner",
      "definition": {
        "text": "Linear Regression is a supervised learning algorithm that models the relationship between input features and a continuous output by fitting a linear equation. The goal is to find the best-fitting line (or hyperplane) that minimizes the difference between predicted and actual values. It's the foundation for understanding more complex models.",
        "keyTerms": ["Slope", "Intercept", "Coefficients", "Least Squares", "Residuals", "R-squared", "Ordinary Least Squares"]
      },
      "keyFormulas": [
        {
          "id": "linear_eq",
          "name": "Linear Equation",
          "formula": "y = wx + b",
          "latex": "\\hat{y} = wx + b",
          "meaning": "w = weight/slope, b = bias/intercept"
        },
        {
          "id": "mse",
          "name": "Mean Squared Error",
          "formula": "MSE = (1/n) √ó Œ£(y·µ¢ - ≈∑·µ¢)¬≤",
          "latex": "\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2",
          "meaning": "Average squared difference between actual and predicted"
        },
        {
          "id": "r_squared",
          "name": "R-Squared",
          "formula": "R¬≤ = 1 - (SS_res / SS_tot)",
          "latex": "R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}",
          "meaning": "Proportion of variance explained (0 to 1)"
        }
      ],
      "examples": [
        {
          "id": "lr_ex1",
          "question": "Calculate the predicted value for x=5 given y = 2x + 3",
          "steps": [
            {"step": 1, "action": "Identify parameters", "result": "w = 2, b = 3", "explanation": "Slope is 2, intercept is 3"},
            {"step": 2, "action": "Apply linear equation", "result": "y = 2(5) + 3", "explanation": "Substitute x = 5"},
            {"step": 3, "action": "Calculate", "result": "y = 10 + 3 = 13", "explanation": "Final prediction"}
          ],
          "finalAnswer": "≈∑ = 13",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "House Price Prediction", "description": "Predicting prices based on size, location, features"},
        {"title": "Sales Forecasting", "description": "Predicting future sales from historical data"},
        {"title": "Stock Trends", "description": "Simple trend analysis in financial data"}
      ],
      "codeExample": {
        "python": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Create sample data\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([2.1, 4.0, 5.9, 8.1, 10.0])\n\n# Fit model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Get parameters\nprint(f'Weight (slope): {model.coef_[0]:.2f}')\nprint(f'Bias (intercept): {model.intercept_:.2f}')\n\n# Predict\ny_pred = model.predict(X)\n\n# Evaluate\nmse = mean_squared_error(y, y_pred)\nr2 = r2_score(y, y_pred)\nprint(f'MSE: {mse:.4f}')\nprint(f'R¬≤: {r2:.4f}')"
      },
      "tips": [
        "Check for linear relationship before using linear regression",
        "Feature scaling improves convergence in gradient descent",
        "Residual plots help diagnose model problems"
      ]
    },
    {
      "id": "gradient_descent",
      "title": "Gradient Descent",
      "symbol": "‚¨áÔ∏è",
      "level": "beginner",
      "definition": {
        "text": "Gradient Descent is an optimization algorithm used to minimize a loss function by iteratively moving in the direction of steepest descent (negative gradient). It's the backbone of training neural networks and many ML algorithms. The learning rate controls how big each step is - too small and training is slow, too large and you might overshoot.",
        "keyTerms": ["Learning Rate", "Gradient", "Optimization", "Convergence", "Local Minimum", "Global Minimum", "Batch Size", "Epoch"]
      },
      "keyFormulas": [
        {
          "id": "gd_update",
          "name": "Parameter Update",
          "formula": "Œ∏ = Œ∏ - Œ± √ó ‚àáL(Œ∏)",
          "latex": "\\theta := \\theta - \\alpha \\nabla_{\\theta} L(\\theta)",
          "meaning": "Œ± = learning rate, ‚àáL = gradient of loss"
        },
        {
          "id": "gradient_simple",
          "name": "Gradient (Simple)",
          "formula": "‚àÇL/‚àÇw = (2/n) √ó Œ£(≈∑ - y) √ó x",
          "latex": "\\frac{\\partial L}{\\partial w} = \\frac{2}{n}\\sum(\\hat{y}_i - y_i) \\cdot x_i",
          "meaning": "For MSE loss in linear regression"
        }
      ],
      "examples": [
        {
          "id": "gd_ex1",
          "question": "If current weight w=5, gradient=2, and learning rate=0.1, what is the new weight?",
          "steps": [
            {"step": 1, "action": "Apply update rule", "result": "w_new = w - Œ± √ó gradient", "explanation": "Standard GD update"},
            {"step": 2, "action": "Substitute values", "result": "w_new = 5 - 0.1 √ó 2", "explanation": "Plug in given values"},
            {"step": 3, "action": "Calculate", "result": "w_new = 5 - 0.2 = 4.8", "explanation": "Weight decreases"}
          ],
          "finalAnswer": "w = 4.8",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Neural Network Training", "description": "Backpropagation uses gradient descent"},
        {"title": "Deep Learning", "description": "All modern deep learning optimizers are GD variants"},
        {"title": "Recommendation Engines", "description": "Matrix factorization optimization"}
      ],
      "codeExample": {
        "python": "import numpy as np\n\n# Simple gradient descent for linear regression\ndef gradient_descent(X, y, lr=0.01, epochs=1000):\n    n = len(y)\n    w, b = 0, 0  # Initialize parameters\n    \n    for epoch in range(epochs):\n        # Predictions\n        y_pred = w * X + b\n        \n        # Compute gradients\n        dw = (2/n) * np.sum((y_pred - y) * X)\n        db = (2/n) * np.sum(y_pred - y)\n        \n        # Update parameters\n        w = w - lr * dw\n        b = b - lr * db\n        \n        if epoch % 200 == 0:\n            loss = np.mean((y_pred - y) ** 2)\n            print(f'Epoch {epoch}: Loss = {loss:.4f}')\n    \n    return w, b\n\n# Example usage\nX = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 6, 8, 10])  # y = 2x\n\nw, b = gradient_descent(X, y, lr=0.1, epochs=1000)\nprint(f'\\nLearned: y = {w:.2f}x + {b:.2f}')"
      },
      "tips": [
        "Start with a small learning rate (0.001 or 0.01) and adjust",
        "Use learning rate schedulers to decrease LR over time",
        "Batch gradient descent vs SGD: tradeoff between stability and speed"
      ]
    },
    {
      "id": "loss_functions",
      "title": "Loss Functions",
      "symbol": "üìâ",
      "level": "beginner",
      "definition": {
        "text": "Loss functions (also called cost functions or objective functions) measure how wrong a model's predictions are compared to actual values. The goal of training is to minimize this loss. Different problems require different loss functions: MSE for regression, Cross-Entropy for classification. The choice of loss function directly impacts what the model learns.",
        "keyTerms": ["MSE", "MAE", "Cross-Entropy", "Binary Cross-Entropy", "Categorical Cross-Entropy", "Hinge Loss", "Huber Loss"]
      },
      "keyFormulas": [
        {
          "id": "mse_loss",
          "name": "Mean Squared Error",
          "formula": "L = (1/n) √ó Œ£(y·µ¢ - ≈∑·µ¢)¬≤",
          "latex": "L = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2",
          "meaning": "For regression, penalizes large errors more"
        },
        {
          "id": "bce_loss",
          "name": "Binary Cross-Entropy",
          "formula": "L = -[y¬∑log(p) + (1-y)¬∑log(1-p)]",
          "latex": "L = -[y \\log(p) + (1-y)\\log(1-p)]",
          "meaning": "For binary classification"
        },
        {
          "id": "ce_loss",
          "name": "Categorical Cross-Entropy",
          "formula": "L = -Œ£ y·µ¢¬∑log(p·µ¢)",
          "latex": "L = -\\sum_{i=1}^{C} y_i \\log(p_i)",
          "meaning": "For multi-class classification"
        }
      ],
      "examples": [
        {
          "id": "loss_ex1",
          "question": "Calculate MSE for predictions [2, 4, 6] and actual values [1, 5, 6]",
          "steps": [
            {"step": 1, "action": "Calculate errors", "result": "(2-1)=1, (4-5)=-1, (6-6)=0", "explanation": "Predicted - Actual"},
            {"step": 2, "action": "Square errors", "result": "1¬≤ + (-1)¬≤ + 0¬≤ = 1 + 1 + 0 = 2", "explanation": "Sum of squared errors"},
            {"step": 3, "action": "Calculate mean", "result": "MSE = 2/3 = 0.667", "explanation": "Divide by n=3"}
          ],
          "finalAnswer": "MSE = 0.667",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Regression Tasks", "description": "MSE/MAE for price prediction, forecasting"},
        {"title": "Classification", "description": "Cross-entropy for image classification, sentiment analysis"},
        {"title": "Ranking", "description": "Hinge loss for search result ranking"}
      ],
      "codeExample": {
        "python": "import numpy as np\n\n# Common loss functions\ndef mse_loss(y_true, y_pred):\n    \"\"\"Mean Squared Error - Regression\"\"\"\n    return np.mean((y_true - y_pred) ** 2)\n\ndef mae_loss(y_true, y_pred):\n    \"\"\"Mean Absolute Error - Regression\"\"\"\n    return np.mean(np.abs(y_true - y_pred))\n\ndef binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n    \"\"\"Binary Cross-Entropy - Binary Classification\"\"\"\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return -np.mean(y_true * np.log(y_pred) + \n                    (1 - y_true) * np.log(1 - y_pred))\n\ndef categorical_cross_entropy(y_true, y_pred, epsilon=1e-15):\n    \"\"\"Categorical Cross-Entropy - Multi-class\"\"\"\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return -np.sum(y_true * np.log(y_pred)) / len(y_true)\n\n# Examples\ny_true_reg = np.array([1, 5, 6])\ny_pred_reg = np.array([2, 4, 6])\nprint(f'MSE: {mse_loss(y_true_reg, y_pred_reg):.3f}')\nprint(f'MAE: {mae_loss(y_true_reg, y_pred_reg):.3f}')"
      },
      "tips": [
        "MSE penalizes outliers more heavily than MAE",
        "Use BCE/CE for classification, not MSE",
        "Loss should decrease during training - if not, check learning rate"
      ]
    },
    {
      "id": "regularization",
      "title": "Regularization",
      "symbol": "‚öñÔ∏è",
      "level": "intermediate",
      "definition": {
        "text": "Regularization is a technique to prevent overfitting by adding a penalty term to the loss function that discourages complex models. L1 (Lasso) adds absolute weight values, promoting sparsity. L2 (Ridge) adds squared weights, keeping weights small. Elastic Net combines both. These techniques help models generalize better to unseen data.",
        "keyTerms": ["L1 Regularization", "L2 Regularization", "Lasso", "Ridge", "Elastic Net", "Lambda", "Weight Decay", "Sparsity"]
      },
      "keyFormulas": [
        {
          "id": "l2_reg",
          "name": "L2 Regularization (Ridge)",
          "formula": "L_total = L + Œª √ó Œ£w·µ¢¬≤",
          "latex": "L_{total} = L + \\lambda \\sum_{i} w_i^2",
          "meaning": "Penalizes large weights"
        },
        {
          "id": "l1_reg",
          "name": "L1 Regularization (Lasso)",
          "formula": "L_total = L + Œª √ó Œ£|w·µ¢|",
          "latex": "L_{total} = L + \\lambda \\sum_{i} |w_i|",
          "meaning": "Promotes sparse solutions"
        },
        {
          "id": "elastic_net",
          "name": "Elastic Net",
          "formula": "L = L + Œª‚ÇÅŒ£|w·µ¢| + Œª‚ÇÇŒ£w·µ¢¬≤",
          "latex": "L = L + \\lambda_1 \\sum |w_i| + \\lambda_2 \\sum w_i^2",
          "meaning": "Combines L1 and L2"
        }
      ],
      "examples": [
        {
          "id": "reg_ex1",
          "question": "If loss=5, weights=[1, -2, 0.5], and Œª=0.1, calculate L2 regularized loss",
          "steps": [
            {"step": 1, "action": "Calculate sum of squared weights", "result": "1¬≤ + (-2)¬≤ + 0.5¬≤ = 1 + 4 + 0.25 = 5.25", "explanation": "L2 penalty term"},
            {"step": 2, "action": "Apply regularization coefficient", "result": "0.1 √ó 5.25 = 0.525", "explanation": "Œª times penalty"},
            {"step": 3, "action": "Add to original loss", "result": "5 + 0.525 = 5.525", "explanation": "Total regularized loss"}
          ],
          "finalAnswer": "L_total = 5.525",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Feature Selection", "description": "L1 sets unimportant weights to zero"},
        {"title": "Preventing Overfitting", "description": "All regularization reduces variance"},
        {"title": "Neural Networks", "description": "Weight decay in Adam optimizer"}
      ],
      "codeExample": {
        "python": "from sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.datasets import make_regression\n\n# Create sample data\nX, y = make_regression(n_samples=100, n_features=10, noise=10)\n\n# L2 Regularization (Ridge)\nridge = Ridge(alpha=1.0)\nridge.fit(X, y)\nprint(f'Ridge coefficients: {ridge.coef_[:5].round(2)}...')\n\n# L1 Regularization (Lasso)\nlasso = Lasso(alpha=1.0)\nlasso.fit(X, y)\nprint(f'Lasso coefficients: {lasso.coef_[:5].round(2)}...')\nprint(f'Lasso zero weights: {(lasso.coef_ == 0).sum()}')\n\n# Elastic Net (L1 + L2)\nelastic = ElasticNet(alpha=1.0, l1_ratio=0.5)\nelastic.fit(X, y)\nprint(f'Elastic coefficients: {elastic.coef_[:5].round(2)}...')"
      },
      "tips": [
        "Higher Œª = more regularization = simpler model",
        "Use L1 when you suspect many irrelevant features",
        "L2 is more stable and commonly used (weight decay)"
      ]
    },
    {
      "id": "bias_variance",
      "title": "Bias-Variance Tradeoff",
      "symbol": "üéØ",
      "level": "intermediate",
      "definition": {
        "text": "The bias-variance tradeoff is a fundamental concept describing the tension between model complexity and generalization. Bias is error from oversimplified assumptions (underfitting). Variance is error from sensitivity to training data (overfitting). Total error = Bias¬≤ + Variance + Irreducible Error. The goal is finding the sweet spot.",
        "keyTerms": ["Bias", "Variance", "Overfitting", "Underfitting", "Model Complexity", "Generalization", "Training Error", "Test Error"]
      },
      "keyFormulas": [
        {
          "id": "total_error",
          "name": "Total Error",
          "formula": "Error = Bias¬≤ + Variance + Noise",
          "latex": "\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\sigma^2",
          "meaning": "Decomposition of expected error"
        }
      ],
      "examples": [
        {
          "id": "bv_ex1",
          "question": "A model has 95% training accuracy but only 60% test accuracy. What's the problem?",
          "steps": [
            {"step": 1, "action": "Compare train/test performance", "result": "Large gap (35%)", "explanation": "Train >> Test indicates overfitting"},
            {"step": 2, "action": "Diagnose", "result": "High variance", "explanation": "Model memorized training data"},
            {"step": 3, "action": "Solutions", "result": "More data, regularization, simpler model", "explanation": "Reduce variance"}
          ],
          "finalAnswer": "Overfitting (High Variance) - Model doesn't generalize",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Model Selection", "description": "Choosing between simple and complex models"},
        {"title": "Hyperparameter Tuning", "description": "Finding optimal regularization strength"},
        {"title": "Deep Learning", "description": "Deciding network depth and width"}
      ],
      "codeExample": {
        "python": "import numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n# Generate data\nnp.random.seed(42)\nX = np.sort(np.random.rand(50, 1) * 10, axis=0)\ny = np.sin(X).ravel() + np.random.randn(50) * 0.2\n\n# Test different model complexities\nfor degree in [1, 3, 10, 20]:\n    model = make_pipeline(\n        PolynomialFeatures(degree),\n        LinearRegression()\n    )\n    \n    # Cross-validation scores\n    scores = cross_val_score(model, X, y, cv=5)\n    \n    # Train score\n    model.fit(X, y)\n    train_score = model.score(X, y)\n    \n    print(f'Degree {degree:2d}: Train={train_score:.3f}, '\n          f'CV={scores.mean():.3f}¬±{scores.std():.3f}')"
      },
      "tips": [
        "High train error + High test error = Underfitting (high bias)",
        "Low train error + High test error = Overfitting (high variance)",
        "Use validation curves to visualize the tradeoff"
      ]
    },
    {
      "id": "cross_validation",
      "title": "Cross-Validation",
      "symbol": "üîÑ",
      "level": "intermediate",
      "definition": {
        "text": "Cross-validation is a resampling technique to evaluate model performance on limited data. K-Fold CV splits data into K parts, trains on K-1, tests on 1, and repeats K times. It provides a more reliable estimate of generalization than a single train-test split, and helps in hyperparameter tuning and model selection.",
        "keyTerms": ["K-Fold", "Stratified K-Fold", "Leave-One-Out", "Hold-Out", "Validation Set", "Train-Test Split", "Nested CV"]
      },
      "keyFormulas": [
        {
          "id": "cv_score",
          "name": "K-Fold CV Score",
          "formula": "Score = (1/K) √ó Œ£ Score·µ¢",
          "latex": "\\text{Score} = \\frac{1}{K}\\sum_{i=1}^{K} \\text{Score}_i",
          "meaning": "Average score across K folds"
        }
      ],
      "examples": [
        {
          "id": "cv_ex1",
          "question": "With 5-fold CV and fold accuracies [0.85, 0.87, 0.83, 0.86, 0.84], what's the CV score?",
          "steps": [
            {"step": 1, "action": "Sum all fold scores", "result": "0.85 + 0.87 + 0.83 + 0.86 + 0.84 = 4.25", "explanation": "Add up accuracies"},
            {"step": 2, "action": "Calculate mean", "result": "4.25 / 5 = 0.85", "explanation": "Average across K=5 folds"},
            {"step": 3, "action": "Calculate std", "result": "œÉ ‚âà 0.014", "explanation": "Measure of variance"}
          ],
          "finalAnswer": "CV Score = 0.85 ¬± 0.014",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Model Comparison", "description": "Fairly compare different algorithms"},
        {"title": "Hyperparameter Tuning", "description": "GridSearchCV uses cross-validation"},
        {"title": "Small Datasets", "description": "Make the most of limited data"}
      ],
      "codeExample": {
        "python": "from sklearn.model_selection import (\n    cross_val_score, KFold, StratifiedKFold, \n    LeaveOneOut, GridSearchCV\n)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\n\n# Load data\nX, y = load_iris(return_X_y=True)\n\n# K-Fold Cross-Validation\nmodel = RandomForestClassifier(random_state=42)\nscores = cross_val_score(model, X, y, cv=5)\nprint(f'5-Fold CV: {scores.mean():.3f} ¬± {scores.std():.3f}')\n\n# Stratified K-Fold (maintains class balance)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X, y, cv=skf)\nprint(f'Stratified 5-Fold: {scores.mean():.3f} ¬± {scores.std():.3f}')\n\n# GridSearchCV with Cross-Validation\nparam_grid = {'n_estimators': [10, 50, 100], 'max_depth': [3, 5, 10]}\ngrid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X, y)\nprint(f'Best params: {grid_search.best_params_}')\nprint(f'Best CV score: {grid_search.best_score_:.3f}')"
      },
      "tips": [
        "Use stratified K-fold for imbalanced classification",
        "5 or 10 folds is typically a good choice",
        "Leave-one-out is expensive but useful for tiny datasets"
      ]
    },
    {
      "id": "decision_trees",
      "title": "Decision Trees",
      "symbol": "üå≥",
      "level": "beginner",
      "definition": {
        "text": "Decision Trees are supervised learning algorithms that make predictions by learning decision rules from features. They split data recursively based on feature thresholds that maximize information gain or reduce impurity. Trees are interpretable, handle non-linear relationships, and form the basis for powerful ensemble methods like Random Forests and Gradient Boosting.",
        "keyTerms": ["Node", "Leaf", "Gini Impurity", "Information Gain", "Entropy", "Pruning", "Max Depth", "CART"]
      },
      "keyFormulas": [
        {
          "id": "gini",
          "name": "Gini Impurity",
          "formula": "Gini = 1 - Œ£p·µ¢¬≤",
          "latex": "\\text{Gini} = 1 - \\sum_{i=1}^{C} p_i^2",
          "meaning": "p·µ¢ = probability of class i at node"
        },
        {
          "id": "entropy",
          "name": "Entropy",
          "formula": "H = -Œ£ p·µ¢ √ó log‚ÇÇ(p·µ¢)",
          "latex": "H = -\\sum_{i=1}^{C} p_i \\log_2(p_i)",
          "meaning": "Measure of disorder/uncertainty"
        },
        {
          "id": "info_gain",
          "name": "Information Gain",
          "formula": "IG = H(parent) - Œ£(n·µ¢/n)H(child·µ¢)",
          "latex": "IG = H(parent) - \\sum_{i} \\frac{n_i}{n} H(child_i)",
          "meaning": "Reduction in entropy from split"
        }
      ],
      "examples": [
        {
          "id": "dt_ex1",
          "question": "Calculate Gini impurity for a node with 60% class A, 40% class B",
          "steps": [
            {"step": 1, "action": "Identify probabilities", "result": "p_A = 0.6, p_B = 0.4", "explanation": "Class proportions"},
            {"step": 2, "action": "Apply Gini formula", "result": "1 - (0.6¬≤ + 0.4¬≤)", "explanation": "Sum of squared probabilities"},
            {"step": 3, "action": "Calculate", "result": "1 - (0.36 + 0.16) = 1 - 0.52 = 0.48", "explanation": "Gini = 0.48"}
          ],
          "finalAnswer": "Gini = 0.48 (impure node)",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Credit Scoring", "description": "Rule-based loan approval decisions"},
        {"title": "Medical Diagnosis", "description": "Interpretable diagnostic trees"},
        {"title": "Customer Churn", "description": "Identify factors leading to churn"}
      ],
      "codeExample": {
        "python": "from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n# Load data\nX, y = load_iris(return_X_y=True)\nfeature_names = load_iris().feature_names\nclass_names = load_iris().target_names\n\n# Train decision tree\ndt = DecisionTreeClassifier(\n    max_depth=3,\n    criterion='gini',  # or 'entropy'\n    random_state=42\n)\ndt.fit(X, y)\n\nprint(f'Accuracy: {dt.score(X, y):.3f}')\nprint(f'Number of leaves: {dt.get_n_leaves()}')\nprint(f'Tree depth: {dt.get_depth()}')\n\n# Visualize tree\nplt.figure(figsize=(15, 10))\nplot_tree(dt, feature_names=feature_names, \n          class_names=class_names, filled=True)\nplt.title('Decision Tree Visualization')\nplt.show()"
      },
      "tips": [
        "Set max_depth to prevent overfitting",
        "Decision trees are greedy - they find locally optimal splits",
        "Use feature_importances_ to understand which features matter"
      ]
    },
    {
      "id": "random_forests",
      "title": "Random Forests",
      "symbol": "üå≤",
      "level": "intermediate",
      "definition": {
        "text": "Random Forest is an ensemble method that builds multiple decision trees and combines their predictions. Each tree is trained on a random subset of data (bagging) and features (feature randomness). This reduces overfitting and variance while maintaining the interpretability benefits of trees. It's robust, handles missing data well, and provides feature importance.",
        "keyTerms": ["Ensemble", "Bagging", "Bootstrap", "Feature Randomness", "Out-of-Bag Error", "Feature Importance", "Voting"]
      },
      "keyFormulas": [
        {
          "id": "rf_prediction",
          "name": "Random Forest Prediction",
          "formula": "≈∑ = mode(tree‚ÇÅ(x), tree‚ÇÇ(x), ..., tree‚Çô(x))",
          "latex": "\\hat{y} = \\text{mode}(h_1(x), h_2(x), \\ldots, h_n(x))",
          "meaning": "Majority vote for classification, average for regression"
        }
      ],
      "examples": [
        {
          "id": "rf_ex1",
          "question": "Why does Random Forest reduce overfitting compared to a single tree?",
          "steps": [
            {"step": 1, "action": "Single tree problem", "result": "High variance, overfits training data", "explanation": "Memorizes noise"},
            {"step": 2, "action": "Bagging effect", "result": "Different trees see different data", "explanation": "Reduces variance"},
            {"step": 3, "action": "Feature randomness", "result": "Trees are decorrelated", "explanation": "Different features at each split"},
            {"step": 4, "action": "Averaging", "result": "Errors cancel out", "explanation": "Wisdom of crowds"}
          ],
          "finalAnswer": "Bagging + feature randomness creates diverse trees whose errors average out",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Credit Scoring", "description": "Robust predictions with feature importance"},
        {"title": "Medical Diagnosis", "description": "Handles many features, missing data"},
        {"title": "Fraud Detection", "description": "Excellent for imbalanced data with class weights"}
      ],
      "codeExample": {
        "python": "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Load data\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Train Random Forest\nrf = RandomForestClassifier(\n    n_estimators=100,      # Number of trees\n    max_depth=5,           # Limit tree depth\n    min_samples_split=5,   # Min samples to split\n    max_features='sqrt',   # Features per split\n    oob_score=True,        # Out-of-bag evaluation\n    random_state=42\n)\nrf.fit(X_train, y_train)\n\nprint(f'Test Accuracy: {rf.score(X_test, y_test):.3f}')\nprint(f'OOB Score: {rf.oob_score_:.3f}')\n\n# Feature importance\nfor name, importance in zip(load_iris().feature_names, rf.feature_importances_):\n    print(f'{name}: {importance:.3f}')"
      },
      "tips": [
        "100-500 trees is usually sufficient",
        "OOB score is a free validation estimate",
        "Use class_weight='balanced' for imbalanced data"
      ]
    },
    {
      "id": "gradient_boosting",
      "title": "Gradient Boosting",
      "symbol": "üöÄ",
      "level": "intermediate",
      "definition": {
        "text": "Gradient Boosting builds trees sequentially, where each new tree corrects the errors of the previous ensemble. It fits trees to the negative gradient (residuals) of the loss function. XGBoost, LightGBM, and CatBoost are optimized implementations that dominate tabular data competitions. Slower to train than Random Forest but often more accurate.",
        "keyTerms": ["Boosting", "Residuals", "Learning Rate", "Shrinkage", "XGBoost", "LightGBM", "CatBoost", "Early Stopping"]
      },
      "keyFormulas": [
        {
          "id": "gb_update",
          "name": "Gradient Boosting Update",
          "formula": "F_m(x) = F_{m-1}(x) + Œ∑ √ó h_m(x)",
          "latex": "F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)",
          "meaning": "Add new tree h_m scaled by learning rate Œ∑"
        }
      ],
      "examples": [
        {
          "id": "gb_ex1",
          "question": "What's the difference between bagging (RF) and boosting (GBM)?",
          "steps": [
            {"step": 1, "action": "Bagging", "result": "Parallel trees, independent", "explanation": "Average reduces variance"},
            {"step": 2, "action": "Boosting", "result": "Sequential trees, dependent", "explanation": "Each fixes previous errors"},
            {"step": 3, "action": "Bias-Variance", "result": "Bagging: ‚Üìvariance, Boosting: ‚Üìbias", "explanation": "Different goals"},
            {"step": 4, "action": "Speed", "result": "Bagging parallelizable, Boosting sequential", "explanation": "RF trains faster"}
          ],
          "finalAnswer": "Bagging averages independent trees; Boosting chains dependent trees",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Kaggle Competitions", "description": "XGBoost/LightGBM win most tabular contests"},
        {"title": "Click-Through Rate", "description": "Ad ranking systems"},
        {"title": "Insurance Pricing", "description": "Risk prediction with interpretability"}
      ],
      "codeExample": {
        "python": "import xgboost as xgb\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load data (regression example)\nfrom sklearn.datasets import fetch_california_housing\ndata = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, test_size=0.2\n)\n\n# XGBoost model\nmodel = xgb.XGBRegressor(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,      # Shrinkage\n    subsample=0.8,          # Row sampling\n    colsample_bytree=0.8,   # Column sampling\n    early_stopping_rounds=10,\n    random_state=42\n)\n\n# Train with early stopping\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    verbose=False\n)\n\n# Evaluate\ny_pred = model.predict(X_test)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(f'RMSE: {rmse:.3f}')\nprint(f'Best iteration: {model.best_iteration}')"
      },
      "tips": [
        "Lower learning rate + more trees = better but slower",
        "Always use early stopping to prevent overfitting",
        "LightGBM is fastest, CatBoost handles categoricals natively"
      ]
    },
    {
      "id": "svm",
      "title": "Support Vector Machines",
      "symbol": "üìê",
      "level": "intermediate",
      "definition": {
        "text": "Support Vector Machines find the optimal hyperplane that separates classes with maximum margin. Only the 'support vectors' (points closest to the boundary) matter. The kernel trick allows SVMs to handle non-linear boundaries by mapping data to higher dimensions. SVMs are effective in high-dimensional spaces but scale poorly to large datasets.",
        "keyTerms": ["Hyperplane", "Margin", "Support Vectors", "Kernel Trick", "RBF Kernel", "Soft Margin", "C Parameter", "Gamma"]
      },
      "keyFormulas": [
        {
          "id": "svm_decision",
          "name": "SVM Decision Function",
          "formula": "f(x) = sign(w¬∑x + b)",
          "latex": "f(x) = \\text{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b)",
          "meaning": "Classify based on which side of hyperplane"
        },
        {
          "id": "rbf_kernel",
          "name": "RBF Kernel",
          "formula": "K(x, x') = exp(-Œ≥||x - x'||¬≤)",
          "latex": "K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)",
          "meaning": "Œ≥ controls influence radius"
        }
      ],
      "examples": [
        {
          "id": "svm_ex1",
          "question": "When should you use SVM vs Random Forest?",
          "steps": [
            {"step": 1, "action": "Dataset size", "result": "SVM for small/medium, RF for large", "explanation": "SVM O(n¬≤) complexity"},
            {"step": 2, "action": "Feature scaling", "result": "SVM requires scaling, RF doesn't", "explanation": "Distance-based vs tree-based"},
            {"step": 3, "action": "Interpretability", "result": "RF gives feature importance", "explanation": "SVM is a black box"},
            {"step": 4, "action": "Non-linearity", "result": "Both handle it, different ways", "explanation": "Kernels vs tree splits"}
          ],
          "finalAnswer": "SVM for small/medium data with clear margins; RF for large data or when interpretability needed",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Text Classification", "description": "High-dimensional, sparse data"},
        {"title": "Image Classification", "description": "Before deep learning era"},
        {"title": "Bioinformatics", "description": "Gene expression analysis"}
      ],
      "codeExample": {
        "python": "from sklearn.svm import SVC, SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# Load data\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# SVM requires scaling!\nsvm_pipeline = make_pipeline(\n    StandardScaler(),\n    SVC(kernel='rbf', C=1.0, gamma='scale')\n)\nsvm_pipeline.fit(X_train, y_train)\nprint(f'Accuracy: {svm_pipeline.score(X_test, y_test):.3f}')\n\n# Hyperparameter tuning\nparam_grid = {\n    'svc__C': [0.1, 1, 10],\n    'svc__gamma': ['scale', 'auto', 0.1, 1]\n}\ngrid = GridSearchCV(svm_pipeline, param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(f'Best params: {grid.best_params_}')\nprint(f'Best CV score: {grid.best_score_:.3f}')"
      },
      "tips": [
        "ALWAYS scale features before SVM",
        "Start with RBF kernel, try linear for high-dimensional",
        "Use GridSearchCV to tune C and gamma"
      ]
    },
    {
      "id": "knn",
      "title": "K-Nearest Neighbors",
      "symbol": "üë•",
      "level": "beginner",
      "definition": {
        "text": "K-Nearest Neighbors is a simple, non-parametric algorithm that classifies based on the majority vote of the K closest training examples. It's 'lazy learning' - no training phase, just stores data and computes distances at prediction time. Distance metric (Euclidean, Manhattan) and K value are key hyperparameters.",
        "keyTerms": ["Lazy Learning", "Distance Metric", "Euclidean Distance", "Manhattan Distance", "K Value", "Curse of Dimensionality", "Weighted KNN"]
      },
      "keyFormulas": [
        {
          "id": "euclidean",
          "name": "Euclidean Distance",
          "formula": "d(x, y) = ‚àöŒ£(x·µ¢ - y·µ¢)¬≤",
          "latex": "d(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}",
          "meaning": "Straight-line distance in n-dimensions"
        }
      ],
      "examples": [
        {
          "id": "knn_ex1",
          "question": "How do you choose the optimal K value?",
          "steps": [
            {"step": 1, "action": "K too small", "result": "Overfitting, sensitive to noise", "explanation": "K=1 memorizes data"},
            {"step": 2, "action": "K too large", "result": "Underfitting, loses locality", "explanation": "Averages too much"},
            {"step": 3, "action": "Odd K for binary", "result": "Avoids ties in voting", "explanation": "K=3, 5, 7..."},
            {"step": 4, "action": "Cross-validation", "result": "Try K=1 to ‚àön, pick best", "explanation": "Empirical tuning"}
          ],
          "finalAnswer": "Use cross-validation to find K that balances bias and variance (typically 3-10)",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Recommendation Systems", "description": "Find similar users/items"},
        {"title": "Anomaly Detection", "description": "Points far from neighbors are anomalies"},
        {"title": "Image Recognition", "description": "Simple baseline for image classification"}
      ],
      "codeExample": {
        "python": "from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nfrom sklearn.datasets import load_iris\nX, y = load_iris(return_X_y=True)\n\n# KNN requires scaling (distance-based)\nknn_pipeline = make_pipeline(\n    StandardScaler(),\n    KNeighborsClassifier(n_neighbors=5)\n)\n\n# Find optimal K\nk_values = range(1, 21)\nscores = []\nfor k in k_values:\n    knn = make_pipeline(\n        StandardScaler(),\n        KNeighborsClassifier(n_neighbors=k)\n    )\n    cv_scores = cross_val_score(knn, X, y, cv=5)\n    scores.append(cv_scores.mean())\n\nbest_k = k_values[np.argmax(scores)]\nprint(f'Best K: {best_k}, Score: {max(scores):.3f}')\n\n# Different distance metrics\nknn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\nknn_weighted = KNeighborsClassifier(n_neighbors=5, weights='distance')"
      },
      "tips": [
        "Always scale features (KNN is distance-based)",
        "Use odd K for binary classification to avoid ties",
        "KNN struggles with high dimensions (curse of dimensionality)"
      ]
    },
    {
      "id": "kmeans",
      "title": "K-Means Clustering",
      "symbol": "üéØ",
      "level": "beginner",
      "definition": {
        "text": "K-Means is an unsupervised algorithm that partitions data into K clusters by minimizing within-cluster variance. It iteratively assigns points to the nearest centroid and updates centroids to cluster means. Simple and fast, but sensitive to initialization, assumes spherical clusters, and requires specifying K. The elbow method helps choose K.",
        "keyTerms": ["Unsupervised Learning", "Centroid", "Inertia", "Elbow Method", "Silhouette Score", "K-Means++", "Lloyd's Algorithm"]
      },
      "keyFormulas": [
        {
          "id": "kmeans_objective",
          "name": "K-Means Objective",
          "formula": "J = Œ£·µ¢ Œ£‚±º ||x‚±º - Œº·µ¢||¬≤",
          "latex": "J = \\sum_{i=1}^{K} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2",
          "meaning": "Minimize sum of squared distances to centroids"
        }
      ],
      "examples": [
        {
          "id": "kmeans_ex1",
          "question": "How do you choose the optimal number of clusters K?",
          "steps": [
            {"step": 1, "action": "Elbow method", "result": "Plot inertia vs K", "explanation": "Look for 'elbow' point"},
            {"step": 2, "action": "Silhouette score", "result": "Measure cluster separation", "explanation": "Higher is better"},
            {"step": 3, "action": "Domain knowledge", "result": "Business requirements", "explanation": "K might be known"},
            {"step": 4, "action": "Gap statistic", "result": "Compare to random data", "explanation": "Formal statistical test"}
          ],
          "finalAnswer": "Use elbow method + silhouette score, combined with domain knowledge",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Customer Segmentation", "description": "Group customers by behavior"},
        {"title": "Image Compression", "description": "Reduce colors by clustering pixels"},
        {"title": "Anomaly Detection", "description": "Points far from all centroids"}
      ],
      "codeExample": {
        "python": "from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\nfrom sklearn.datasets import make_blobs\nX, _ = make_blobs(n_samples=300, centers=4, random_state=42)\nX = StandardScaler().fit_transform(X)\n\n# Elbow method\ninertias = []\nsilhouettes = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X)\n    inertias.append(kmeans.inertia_)\n    silhouettes.append(silhouette_score(X, kmeans.labels_))\n\n# Plot elbow\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(K_range, inertias, 'bo-')\nplt.xlabel('K'); plt.ylabel('Inertia'); plt.title('Elbow Method')\n\nplt.subplot(1, 2, 2)\nplt.plot(K_range, silhouettes, 'ro-')\nplt.xlabel('K'); plt.ylabel('Silhouette Score'); plt.title('Silhouette')\nplt.show()\n\n# Final clustering\nkmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(X)\nprint(f'Cluster sizes: {np.bincount(labels)}')"
      },
      "tips": [
        "Use K-Means++ initialization (default in sklearn)",
        "Scale features before clustering",
        "K-Means assumes spherical clusters; use DBSCAN for arbitrary shapes"
      ]
    },
    {
      "id": "pca",
      "title": "PCA",
      "symbol": "üìâ",
      "level": "intermediate",
      "definition": {
        "text": "Principal Component Analysis is a dimensionality reduction technique that finds orthogonal directions of maximum variance. It projects high-dimensional data onto a lower-dimensional subspace while preserving as much variance as possible. PCA is used for visualization, noise reduction, feature extraction, and speeding up other algorithms.",
        "keyTerms": ["Dimensionality Reduction", "Principal Components", "Eigenvalues", "Eigenvectors", "Explained Variance", "Covariance Matrix", "Singular Value Decomposition"]
      },
      "keyFormulas": [
        {
          "id": "pca_transform",
          "name": "PCA Transformation",
          "formula": "Z = XW",
          "latex": "Z = XW",
          "meaning": "W contains eigenvectors (principal components)"
        },
        {
          "id": "explained_variance",
          "name": "Explained Variance Ratio",
          "formula": "EVR = Œª·µ¢ / Œ£Œª‚±º",
          "latex": "\\text{EVR}_i = \\frac{\\lambda_i}{\\sum_j \\lambda_j}",
          "meaning": "Proportion of variance captured by each PC"
        }
      ],
      "examples": [
        {
          "id": "pca_ex1",
          "question": "How many components should you keep?",
          "steps": [
            {"step": 1, "action": "Cumulative variance", "result": "Keep components until 95% variance", "explanation": "Common threshold"},
            {"step": 2, "action": "Scree plot", "result": "Look for 'elbow' in eigenvalues", "explanation": "Sharp drop indicates cutoff"},
            {"step": 3, "action": "Task-specific", "result": "Visualization: 2-3, ML: preserve 95%", "explanation": "Depends on goal"},
            {"step": 4, "action": "Cross-validation", "result": "Use downstream task performance", "explanation": "Empirical validation"}
          ],
          "finalAnswer": "Keep components explaining 95% variance, or use scree plot elbow",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Visualization", "description": "Project high-dim data to 2D/3D"},
        {"title": "Face Recognition", "description": "Eigenfaces for compression"},
        {"title": "Preprocessing", "description": "Reduce features before ML model"}
      ],
      "codeExample": {
        "python": "from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nX, y = load_digits(return_X_y=True)  # 64 features (8x8 images)\n\n# Always scale before PCA\nX_scaled = StandardScaler().fit_transform(X)\n\n# Fit PCA\npca = PCA()  # Keep all components initially\npca.fit(X_scaled)\n\n# Cumulative explained variance\ncumulative_var = np.cumsum(pca.explained_variance_ratio_)\nn_95 = np.argmax(cumulative_var >= 0.95) + 1\nprint(f'Components for 95% variance: {n_95}')\n\n# Apply PCA\npca = PCA(n_components=2)  # For visualization\nX_pca = pca.fit_transform(X_scaled)\n\n# Visualize\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.6)\nplt.colorbar(scatter, label='Digit')\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\nplt.title('Digits Dataset - PCA Visualization')\nplt.show()"
      },
      "tips": [
        "Always standardize data before PCA (center and scale)",
        "PCA is linear; use t-SNE/UMAP for non-linear visualization",
        "Whitening (variance=1) can help some downstream models"
      ]
    },
    {
      "id": "time_series",
      "title": "Time Series Forecasting",
      "symbol": "üìà",
      "level": "intermediate",
      "definition": {
        "text": "Time series forecasting predicts future values based on historical patterns. Key components are trend (long-term direction), seasonality (repeating patterns), and noise. Classical methods include ARIMA (autoregressive integrated moving average) and exponential smoothing. Modern approaches use LSTMs, Transformers, and libraries like Prophet.",
        "keyTerms": ["Trend", "Seasonality", "Stationarity", "ARIMA", "Exponential Smoothing", "Autocorrelation", "Lag Features", "Prophet"]
      },
      "keyFormulas": [
        {
          "id": "arima",
          "name": "ARIMA Model",
          "formula": "ARIMA(p, d, q): AR(p) + differencing(d) + MA(q)",
          "latex": "y_t = c + \\sum_{i=1}^{p}\\phi_i y_{t-i} + \\sum_{j=1}^{q}\\theta_j \\epsilon_{t-j} + \\epsilon_t",
          "meaning": "p=AR order, d=differencing, q=MA order"
        },
        {
          "id": "mape",
          "name": "MAPE (Mean Absolute % Error)",
          "formula": "MAPE = (100/n) √ó Œ£|y‚Çú - ≈∑‚Çú|/|y‚Çú|",
          "latex": "\\text{MAPE} = \\frac{100}{n}\\sum_{t=1}^{n}\\left|\\frac{y_t - \\hat{y}_t}{y_t}\\right|",
          "meaning": "Percentage error, interpretable across scales"
        }
      ],
      "examples": [
        {
          "id": "ts_ex1",
          "question": "How do you handle non-stationary time series?",
          "steps": [
            {"step": 1, "action": "Check stationarity", "result": "ADF test (Augmented Dickey-Fuller)", "explanation": "Null: non-stationary"},
            {"step": 2, "action": "Remove trend", "result": "Differencing: y'‚Çú = y‚Çú - y‚Çú‚Çã‚ÇÅ", "explanation": "First difference often works"},
            {"step": 3, "action": "Remove seasonality", "result": "Seasonal differencing or decomposition", "explanation": "y‚Çú - y‚Çú‚Çã‚Çò for period m"},
            {"step": 4, "action": "Transform variance", "result": "Log or Box-Cox transformation", "explanation": "Stabilize variance"}
          ],
          "finalAnswer": "Differencing removes trend, seasonal differencing removes seasonality, transforms stabilize variance",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Demand Forecasting", "description": "Predict sales, inventory needs"},
        {"title": "Stock Prices", "description": "Financial market prediction"},
        {"title": "Weather Forecasting", "description": "Temperature, precipitation prediction"},
        {"title": "Energy Load", "description": "Electricity demand forecasting"}
      ],
      "codeExample": {
        "python": "import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom prophet import Prophet\nimport matplotlib.pyplot as plt\n\n# Create sample time series\nnp.random.seed(42)\ndates = pd.date_range('2020-01-01', periods=365, freq='D')\ntrend = np.linspace(0, 50, 365)\nseasonality = 10 * np.sin(2 * np.pi * np.arange(365) / 7)  # Weekly\nnoise = np.random.randn(365) * 5\ny = trend + seasonality + noise + 100\n\ndf = pd.DataFrame({'ds': dates, 'y': y})\n\n# Decomposition\nresult = seasonal_decompose(y, period=7)\nresult.plot()\nplt.show()\n\n# ARIMA\nmodel = ARIMA(y, order=(1, 1, 1))\narima_fit = model.fit()\nprint(arima_fit.summary())\n\n# Prophet (easy-to-use)\nprophet = Prophet(yearly_seasonality=False, weekly_seasonality=True)\nprophet.fit(df)\n\nfuture = prophet.make_future_dataframe(periods=30)\nforecast = prophet.predict(future)\nprophet.plot(forecast)\nplt.title('Prophet Forecast')\nplt.show()"
      },
      "tips": [
        "Always visualize your data first (trend, seasonality, outliers)",
        "Use train/test split with temporal order (no shuffling!)",
        "Prophet is great for business time series with holidays"
      ]
    },
    {
      "id": "model_evaluation",
      "title": "Model Evaluation Metrics",
      "symbol": "üìä",
      "level": "beginner",
      "definition": {
        "text": "Model evaluation metrics quantify how well a model performs. For classification: accuracy, precision, recall, F1-score, AUC-ROC. For regression: MSE, RMSE, MAE, R¬≤. Choosing the right metric depends on the problem: class imbalance, cost of errors, and business objectives. Always evaluate on held-out test data.",
        "keyTerms": ["Accuracy", "Precision", "Recall", "F1-Score", "AUC-ROC", "Confusion Matrix", "RMSE", "MAE", "R-squared"]
      },
      "keyFormulas": [
        {
          "id": "precision",
          "name": "Precision",
          "formula": "Precision = TP / (TP + FP)",
          "latex": "\\text{Precision} = \\frac{TP}{TP + FP}",
          "meaning": "Of predicted positives, how many are correct"
        },
        {
          "id": "recall",
          "name": "Recall",
          "formula": "Recall = TP / (TP + FN)",
          "latex": "\\text{Recall} = \\frac{TP}{TP + FN}",
          "meaning": "Of actual positives, how many were found"
        },
        {
          "id": "f1",
          "name": "F1 Score",
          "formula": "F1 = 2 √ó (P √ó R) / (P + R)",
          "latex": "F1 = 2 \\times \\frac{P \\times R}{P + R}",
          "meaning": "Harmonic mean of precision and recall"
        }
      ],
      "examples": [
        {
          "id": "eval_ex1",
          "question": "When should you use precision vs recall?",
          "steps": [
            {"step": 1, "action": "Cost of false positives", "result": "High ‚Üí optimize precision", "explanation": "Spam filter, criminal conviction"},
            {"step": 2, "action": "Cost of false negatives", "result": "High ‚Üí optimize recall", "explanation": "Cancer detection, fraud"},
            {"step": 3, "action": "Balance both", "result": "Use F1 score", "explanation": "Harmonic mean"},
            {"step": 4, "action": "Threshold tuning", "result": "Use precision-recall curve", "explanation": "Find optimal tradeoff"}
          ],
          "finalAnswer": "Precision when FP costly (spam), Recall when FN costly (disease), F1 for balance",
          "difficulty": "easy"
        }
      ],
      "realWorldApplications": [
        {"title": "Medical Diagnosis", "description": "High recall (don't miss diseases)"},
        {"title": "Spam Detection", "description": "High precision (don't block good email)"},
        {"title": "Search Ranking", "description": "Precision@K, NDCG"}
      ],
      "codeExample": {
        "python": "from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, roc_auc_score,\n    mean_squared_error, mean_absolute_error, r2_score\n)\nimport numpy as np\n\n# Classification metrics\ny_true = [1, 1, 0, 1, 0, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1, 1, 1, 0, 0]\ny_prob = [0.9, 0.3, 0.2, 0.8, 0.1, 0.6, 0.95, 0.85, 0.15, 0.4]\n\nprint('=== Classification Metrics ===')\nprint(f'Accuracy:  {accuracy_score(y_true, y_pred):.3f}')\nprint(f'Precision: {precision_score(y_true, y_pred):.3f}')\nprint(f'Recall:    {recall_score(y_true, y_pred):.3f}')\nprint(f'F1 Score:  {f1_score(y_true, y_pred):.3f}')\nprint(f'AUC-ROC:   {roc_auc_score(y_true, y_prob):.3f}')\nprint(f'\\nConfusion Matrix:\\n{confusion_matrix(y_true, y_pred)}')\nprint(f'\\n{classification_report(y_true, y_pred)}')\n\n# Regression metrics\ny_true_reg = [3.1, 4.2, 5.5, 2.1, 6.3]\ny_pred_reg = [2.9, 4.5, 5.2, 2.0, 6.0]\n\nprint('\\n=== Regression Metrics ===')\nprint(f'MSE:  {mean_squared_error(y_true_reg, y_pred_reg):.3f}')\nprint(f'RMSE: {np.sqrt(mean_squared_error(y_true_reg, y_pred_reg)):.3f}')\nprint(f'MAE:  {mean_absolute_error(y_true_reg, y_pred_reg):.3f}')\nprint(f'R¬≤:   {r2_score(y_true_reg, y_pred_reg):.3f}')"
      },
      "tips": [
        "Accuracy is misleading for imbalanced classes",
        "Use classification_report for a complete picture",
        "AUC-ROC measures ranking ability, independent of threshold"
      ]
    },
    {
      "id": "feature_engineering",
      "title": "Feature Engineering",
      "symbol": "üîß",
      "level": "intermediate",
      "definition": {
        "text": "Feature engineering transforms raw data into features that better represent the underlying problem. It includes creating new features (interactions, polynomials), encoding categoricals (one-hot, target encoding), handling missing values, scaling, and feature selection. Good features often matter more than model choice.",
        "keyTerms": ["Feature Extraction", "One-Hot Encoding", "Target Encoding", "Feature Scaling", "Imputation", "Feature Selection", "Polynomial Features", "Binning"]
      },
      "keyFormulas": [
        {
          "id": "zscore",
          "name": "Z-Score Normalization",
          "formula": "z = (x - Œº) / œÉ",
          "latex": "z = \\frac{x - \\mu}{\\sigma}",
          "meaning": "Standardize to mean=0, std=1"
        },
        {
          "id": "minmax",
          "name": "Min-Max Scaling",
          "formula": "x' = (x - min) / (max - min)",
          "latex": "x' = \\frac{x - x_{min}}{x_{max} - x_{min}}",
          "meaning": "Scale to [0, 1] range"
        }
      ],
      "examples": [
        {
          "id": "fe_ex1",
          "question": "How do you encode high-cardinality categorical features?",
          "steps": [
            {"step": 1, "action": "One-hot encoding", "result": "Creates too many features", "explanation": "1000 categories = 1000 columns"},
            {"step": 2, "action": "Target encoding", "result": "Mean target per category", "explanation": "Risk of overfitting"},
            {"step": 3, "action": "Frequency encoding", "result": "Count of each category", "explanation": "Simple, no leakage"},
            {"step": 4, "action": "Embedding", "result": "Learned dense vectors", "explanation": "For deep learning"}
          ],
          "finalAnswer": "Target encoding (with CV), frequency encoding, or embeddings for high-cardinality",
          "difficulty": "medium"
        }
      ],
      "realWorldApplications": [
        {"title": "Kaggle Competitions", "description": "Feature engineering wins competitions"},
        {"title": "Fraud Detection", "description": "Aggregate features over time windows"},
        {"title": "Recommendation", "description": "User/item interaction features"}
      ],
      "codeExample": {
        "python": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import (\n    StandardScaler, MinMaxScaler, OneHotEncoder,\n    PolynomialFeatures, LabelEncoder\n)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Sample data\ndf = pd.DataFrame({\n    'age': [25, 30, np.nan, 45, 35],\n    'salary': [50000, 60000, 70000, 80000, 65000],\n    'city': ['NYC', 'LA', 'NYC', 'Chicago', 'LA'],\n    'experience': [2, 5, 8, 15, 7]\n})\n\n# 1. Handle missing values\ndf['age'].fillna(df['age'].median(), inplace=True)\n\n# 2. Create new features\ndf['salary_per_experience'] = df['salary'] / (df['experience'] + 1)\ndf['age_squared'] = df['age'] ** 2\n\n# 3. Encode categoricals\ndf_encoded = pd.get_dummies(df, columns=['city'], drop_first=True)\n\n# 4. Scale numerical features\nscaler = StandardScaler()\nnumerical_cols = ['age', 'salary', 'experience', 'salary_per_experience']\ndf_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])\n\nprint(df_encoded.head())\n\n# Full preprocessing pipeline\nnumerical_features = ['age', 'salary', 'experience']\ncategorical_features = ['city']\n\npreprocessor = ColumnTransformer([\n    ('num', Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())\n    ]), numerical_features),\n    ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n])"
      },
      "tips": [
        "Domain knowledge is the best source of features",
        "Always scale for distance-based algorithms (KNN, SVM, NN)",
        "Use pipelines to prevent data leakage"
      ]
    }
  ]
}

